{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rapport : Résolution de Problèmes / Machine Learning\n",
        "\n",
        "#### IMT Mines Alès Octobre 2022\n",
        "##### INFRES 13 \n",
        "- **Quentin BIALOTA**\n",
        "- **Alexandre BOMPUIS**\n",
        "- **Julia LOCATELLI**\n",
        "- **Tom L'HERMENIER**\n",
        "\n",
        "![Logo IMT](https://www.fondation-mines-telecom.org/wp-content/uploads/2018/02/imt_mines_ales-300x158.jpg)\n",
        "\n",
        "---\n",
        "\n",
        "Dans le cadre de du cours de **ANN & Deep learning** à l'IMT Mines Alès, nous avons réalisé un projet de notre choix, visant à résoudre un problème de machine learning, le tout en évaluant et comparant quelques modèles pour la résolution du problème. Dans ce projet, nous avons choisi de traiter une problématique visant à prédire les vêtements qu'on peut observer dans une image donné. Cette prédiction se fera à partir de différentes caractéristiques, notamment le sexe de la personne sur l'image, les catégories du vêtement (vêtement, accessoire, etc), et d'autres.\n",
        "\n",
        "---\n",
        "\n",
        "Afin de traiter ce problème, nous avons décidé d'utiliser un dataset déjà existant et disponible sur le site Kaggle. Ce dataset nommé \"Fashion Product Images\" est accessible au lien suivant : https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Imports globaux\n",
        "\n",
        "Avant toutes choses, veuillez lancer la cellule suivante pour importer les librairies et les outils utilisés dans ce rapport"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOEi0jIRmXgw",
        "outputId": "bdfdf452-26ac-48d6-fad6-6dddb60288c8"
      },
      "outputs": [],
      "source": [
        "## Import everything needed\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Import dataset\n",
        "\n",
        "Via la cellule suivante, vous pouvez importer le dataset en local (le dataset small est disponible [ici](https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small?resource=download))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import dataset\n",
        "df = pd.read_csv(\"fashion-dataset-small/styles.csv\", on_bad_lines='skip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Information sur le dataset\n",
        "\n",
        "44000+ produits avec des étiquettes de catégorie et des images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "voLg4planmU-",
        "outputId": "359f403c-80c5-4630-a0f8-762f50db595f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Dataset size'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "44424"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "'First ten rows of the dataset'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>masterCategory</th>\n",
              "      <th>subCategory</th>\n",
              "      <th>articleType</th>\n",
              "      <th>baseColour</th>\n",
              "      <th>season</th>\n",
              "      <th>year</th>\n",
              "      <th>usage</th>\n",
              "      <th>productDisplayName</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15970</td>\n",
              "      <td>Men</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>Topwear</td>\n",
              "      <td>Shirts</td>\n",
              "      <td>Navy Blue</td>\n",
              "      <td>Fall</td>\n",
              "      <td>2011.0</td>\n",
              "      <td>Casual</td>\n",
              "      <td>Turtle Check Men Navy Blue Shirt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>39386</td>\n",
              "      <td>Men</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>Bottomwear</td>\n",
              "      <td>Jeans</td>\n",
              "      <td>Blue</td>\n",
              "      <td>Summer</td>\n",
              "      <td>2012.0</td>\n",
              "      <td>Casual</td>\n",
              "      <td>Peter England Men Party Blue Jeans</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>59263</td>\n",
              "      <td>Women</td>\n",
              "      <td>Accessories</td>\n",
              "      <td>Watches</td>\n",
              "      <td>Watches</td>\n",
              "      <td>Silver</td>\n",
              "      <td>Winter</td>\n",
              "      <td>2016.0</td>\n",
              "      <td>Casual</td>\n",
              "      <td>Titan Women Silver Watch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>21379</td>\n",
              "      <td>Men</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>Bottomwear</td>\n",
              "      <td>Track Pants</td>\n",
              "      <td>Black</td>\n",
              "      <td>Fall</td>\n",
              "      <td>2011.0</td>\n",
              "      <td>Casual</td>\n",
              "      <td>Manchester United Men Solid Black Track Pants</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>53759</td>\n",
              "      <td>Men</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>Topwear</td>\n",
              "      <td>Tshirts</td>\n",
              "      <td>Grey</td>\n",
              "      <td>Summer</td>\n",
              "      <td>2012.0</td>\n",
              "      <td>Casual</td>\n",
              "      <td>Puma Men Grey T-shirt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1855</td>\n",
              "      <td>Men</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>Topwear</td>\n",
              "      <td>Tshirts</td>\n",
              "      <td>Grey</td>\n",
              "      <td>Summer</td>\n",
              "      <td>2011.0</td>\n",
              "      <td>Casual</td>\n",
              "      <td>Inkfruit Mens Chain Reaction T-shirt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>30805</td>\n",
              "      <td>Men</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>Topwear</td>\n",
              "      <td>Shirts</td>\n",
              "      <td>Green</td>\n",
              "      <td>Summer</td>\n",
              "      <td>2012.0</td>\n",
              "      <td>Ethnic</td>\n",
              "      <td>Fabindia Men Striped Green Shirt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>26960</td>\n",
              "      <td>Women</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>Topwear</td>\n",
              "      <td>Shirts</td>\n",
              "      <td>Purple</td>\n",
              "      <td>Summer</td>\n",
              "      <td>2012.0</td>\n",
              "      <td>Casual</td>\n",
              "      <td>Jealous 21 Women Purple Shirt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>29114</td>\n",
              "      <td>Men</td>\n",
              "      <td>Accessories</td>\n",
              "      <td>Socks</td>\n",
              "      <td>Socks</td>\n",
              "      <td>Navy Blue</td>\n",
              "      <td>Summer</td>\n",
              "      <td>2012.0</td>\n",
              "      <td>Casual</td>\n",
              "      <td>Puma Men Pack of 3 Socks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>30039</td>\n",
              "      <td>Men</td>\n",
              "      <td>Accessories</td>\n",
              "      <td>Watches</td>\n",
              "      <td>Watches</td>\n",
              "      <td>Black</td>\n",
              "      <td>Winter</td>\n",
              "      <td>2016.0</td>\n",
              "      <td>Casual</td>\n",
              "      <td>Skagen Men Black Watch</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id gender masterCategory subCategory  articleType baseColour  season  \\\n",
              "0  15970    Men        Apparel     Topwear       Shirts  Navy Blue    Fall   \n",
              "1  39386    Men        Apparel  Bottomwear        Jeans       Blue  Summer   \n",
              "2  59263  Women    Accessories     Watches      Watches     Silver  Winter   \n",
              "3  21379    Men        Apparel  Bottomwear  Track Pants      Black    Fall   \n",
              "4  53759    Men        Apparel     Topwear      Tshirts       Grey  Summer   \n",
              "5   1855    Men        Apparel     Topwear      Tshirts       Grey  Summer   \n",
              "6  30805    Men        Apparel     Topwear       Shirts      Green  Summer   \n",
              "7  26960  Women        Apparel     Topwear       Shirts     Purple  Summer   \n",
              "8  29114    Men    Accessories       Socks        Socks  Navy Blue  Summer   \n",
              "9  30039    Men    Accessories     Watches      Watches      Black  Winter   \n",
              "\n",
              "     year   usage                             productDisplayName  \n",
              "0  2011.0  Casual               Turtle Check Men Navy Blue Shirt  \n",
              "1  2012.0  Casual             Peter England Men Party Blue Jeans  \n",
              "2  2016.0  Casual                       Titan Women Silver Watch  \n",
              "3  2011.0  Casual  Manchester United Men Solid Black Track Pants  \n",
              "4  2012.0  Casual                          Puma Men Grey T-shirt  \n",
              "5  2011.0  Casual           Inkfruit Mens Chain Reaction T-shirt  \n",
              "6  2012.0  Ethnic               Fabindia Men Striped Green Shirt  \n",
              "7  2012.0  Casual                  Jealous 21 Women Purple Shirt  \n",
              "8  2012.0  Casual                       Puma Men Pack of 3 Socks  \n",
              "9  2016.0  Casual                         Skagen Men Black Watch  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Topwear                     15402\n",
              "Shoes                        7343\n",
              "Bags                         3055\n",
              "Bottomwear                   2694\n",
              "Watches                      2542\n",
              "Innerwear                    1808\n",
              "Jewellery                    1079\n",
              "Eyewear                      1073\n",
              "Fragrance                    1011\n",
              "Sandal                        963\n",
              "Wallets                       933\n",
              "Flip Flops                    913\n",
              "Belts                         811\n",
              "Socks                         698\n",
              "Lips                          527\n",
              "Dress                         478\n",
              "Loungewear and Nightwear      470\n",
              "Saree                         427\n",
              "Nails                         329\n",
              "Makeup                        307\n",
              "Headwear                      293\n",
              "Ties                          258\n",
              "Accessories                   129\n",
              "Scarves                       118\n",
              "Cufflinks                     108\n",
              "Apparel Set                   106\n",
              "Free Gifts                    104\n",
              "Stoles                         90\n",
              "Skin Care                      77\n",
              "Skin                           69\n",
              "Eyes                           43\n",
              "Mufflers                       38\n",
              "Shoe Accessories               24\n",
              "Sports Equipment               21\n",
              "Gloves                         20\n",
              "Hair                           19\n",
              "Bath and Body                  12\n",
              "Water Bottle                    7\n",
              "Perfumes                        6\n",
              "Umbrellas                       6\n",
              "Beauty Accessories              4\n",
              "Wristbands                      4\n",
              "Sports Accessories              3\n",
              "Home Furnishing                 1\n",
              "Vouchers                        1\n",
              "Name: subCategory, dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "'Number of articleType in subcategory Topwear'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Tshirts          7066\n",
              "Shirts           3217\n",
              "Kurtas           1844\n",
              "Tops             1762\n",
              "Sweatshirts       285\n",
              "Sweaters          277\n",
              "Jackets           258\n",
              "Kurtis            234\n",
              "Tunics            229\n",
              "Dupatta           116\n",
              "Suspenders         40\n",
              "Rain Jacket        18\n",
              "Waistcoat          15\n",
              "Rompers            12\n",
              "Blazers             8\n",
              "Shrug               6\n",
              "Nehru Jackets       5\n",
              "Lehenga Choli       4\n",
              "Belts               3\n",
              "Dresses             2\n",
              "Suits               1\n",
              "Name: articleType, dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## Dataset visualization\n",
        "\n",
        "# Show dataset size\n",
        "display(\"Dataset size\", df.shape[0])\n",
        "# Show the first ten rows of the dataset\n",
        "display(\"First ten rows of the dataset\", df.head(10))\n",
        "# Show number of subcategory\n",
        "display(df.subCategory.value_counts())\n",
        "# Show number of articleType in subcategory Topwear\n",
        "display(\"Number of articleType in subcategory Topwear\", df[df.subCategory == \"Topwear\"].articleType.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Exemple d'une image contenu dans le dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "bsa5lTnZr_Hm",
        "outputId": "07c2c986-90b1-4dae-c2b0-66a04c138172"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMUAAAD7CAYAAADaSFAtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABt/0lEQVR4nO39WawtTZaYh30rIqe9z3iHf6j6q6r/anXTVIsWq6kCTYGCIZOiQVMEWw9Cm7RACAaB9oNkUJAAkfKD9eIH6kUSARsGGqTsFkCLTbdEmCAEyQ2yKUEvDXazKbHniVWuqq5/vsMZ9t6ZGbH8EBGZkbn3Ge5/h/+ce8+62PfsnUNkZOZasea1RFW5gzu4gxHMFz2BO7iDmwZ3RHEHdzCDO6K4gzuYwR1R3MEdzOCOKO7gDmZwRxR3cAczeC6iEJE/KSK/ISK/LSJ/+UVN6g7u4IsE+bx+ChGxwG8CfwL4LvAPgT+nqr/64qZ3B3fw6qF4jnP/MPDbqvq7ACLyN4EfAy4kiocPH+r777//HJf8vKDZNwlf+jZ8VFHXgu9BBLEWEYOqot6DKt571HnA432HqsOUC4rlfUxRgXThg0f9CliHS6bLiiDGDN8hfV7sve2GeJ3Z4qdxm2xtZDxeCPOVNIYBBO8U1ynqFZyC8wiCsSXGFCAgzK+neA9OFTEW2+xhijpOT7LjJrN+afCtb32LTz75ZOdlnoco3gO+k/3+LvC/uOyE999/n1/4hV94jktOQYdHqJMXKrOjwMf/hR6Lqkc+/R7m0+9Ct6J//B382SeYqqI8uodUFW7T0a82+N7Rnp7Rnp3h3Jr1+vt0/ROW7/5zPPxD/1ua+z8A9iMov4dyhm9/Ge1/C9RDD6IgRYltFmAtSAGmmt1JIBLJvs/vYPpNMyTXrfsPw6QxJB4SztHsgyrG+3A1r+DjKV5HoqgKKA1oAW4BvmT1pOPJBxvc2qGPN+jjNdZUHBx9icXyPmIUazsQj6qgHrwqp63jtHPY5RFHP/yHad7+OiIGMTbciYZLp+mLpCehw9/JfQ4EJbN7vhq++c1vXrjveYjiWiAiPwH8BMDXvva1l325S0CZIJt61DvwDkEREURMWOVEEDy4DvEukJIoGLBFiafBFHV4mWnl92F1NAIa/oPCgIaX7hXEKYgH3zEQQfpPzDiW7FL1pkSgAysasSh8k/gvDS6T2xaRgUAAMCZ8tyNGaiQKRfHS453HsMBygMgeYhxF0UHp6OVT+n6NiqNtW4xdYQyUlWBMuh2DKBjxWMBELqzOgdH43C9DaMne3xZVvHB4HqL4HvDV7PdX4rYJqOpPAj8J8M1vfvMFB1rlD2vryheeAYD3qOvAdQAYYzBGAtIAqEfcBvoe63tUHGKgKGuwSlEtMLYIYpEQkSyuaCYio61BbeBSXvCA4BB14RKSrhfEtrA0GjA6XemH+0kfP3xPOqEXg4pBSNaTjPsoEzFoyos00kcaF1Q93iuqns6t6PsNVioauYflLYxRysoj6uhxdN2n9OqwqxVeLUVhQSqstZjCUNgCAxjjsITFQ53D9V18hsX43C+EXQRx84jiHwI/LCJfJxDDnwX+dy9kVs8ElxOEZl9GdhzX2qgzEDlFWqlzqV8BJLFvQYxFKMPf7Jxh9VUNotPsJWpceRNeS+IQGsfQiM4Sz81ofRAdshHnKJLfr+bnTA7ajURiRsaRJK1B6kJQNSgWlRpkgRhFjMeYHqQInAXw3uOdwxuYGHDivQ5krgROrSNRX6wa5YuCxJPz+92+p+cllc9NFKrai8i/Dfy3gAX+M1X9leeczzPBRYixBZnkYSS8MOd6fLeBvsWoBhYvMkow1mLrBrEO5xXfORSDchAQtzwAW4GxaK9o26G6gf4p2n+GiGDsHiLVyAEAMZIRZ0Z+vozYKaAZ15BdCvrICxIGGBlXelEdsNv7QIwjqTPca45vokGcEQ2ruPaO8FofILqPyDFivgrmARQdpt6gbKCoUUn6So9qFx630XB6+miYn/E94nvUObxzcUHwEy4W/spzI/fnhefSKVT1vwb+6xc0l1cCCTWcd7i+RSJRDBaieICxBVIFoqDr8bYNKGebgKjVPtgSjEG94toOXItuTtHuUVAgS0VsHY5J+ocVpMhEIxFETZDnByKISCI2fgSwGRWY7dWXnoFPeIeqR73ivYursSCRmExhB+V2JAyH9H04tuvRrkepEHsPMV9BOALzZTD3kGKFVCcIa8RWUb/xeFwgDATNiSLRNYr4HvEO9Q7vHeJNMHxgJjrWM+jMLxxeuqL9siFnruz4Pt8mSUVVjYp2kM9lS7lNamt8mYMybpEoB0/fXL6KR+Q2FuJxg1JuhKTMj0p2LiDFb4OcnynTOuca+eX95EYFCRKZmHB/g44xVcPDpaLlyROtUBqNAoLYGisHGPYQykikJtxHuk0jiArWSLhlIxgT7lUmcx05WLKETa1o2/clu17wDlAyyeo5CerWE8V1YCp3KqIeXItvV4jvwCrG2mCJQQOhqENwgMNKTykdXgooK0yxpCibjJAMIgUqJdgG0WXwd9QHSLkAEayJFhbRzLi0W3EMMrqioihutnrmCrhmPwOyCgKSfAVgTT7+7JsqOB+m0Su0gTikB9OFcWzxDmp/CDE1YvYDoVsDhSIKtjJUdYGop24sdW2xlaWoLba0SGIVPtCt8T4832j5U7WT+7hQaEpWsx1BGHOyel54I4giQHxcUbZV74L1yfeBxSeTbFQC03GCx8QP4rG2QIoKY8uZvB9EHTEF2AqxBRQ1FE1YMZNoNln9NZsTk1VTo2VpYBiMRiSyv8M5UoGYzKIVuZHZRiJUg97h/cgdHNCH79IHwsAYRPdB3hpFOaKuY8Oxxgq2sBiFsjCUhWALwViDsVFH8nGxGbiEj5wisSfdqR9eaZESuaZS+WzwhhBF9sg1W6lcD9pHxTYhj44rTu7NxUcRyoAtghiVRBIx0RNeoFqhVGCLIDol2T0zg6m4bNsopo3LXK5mjiaocfeU8w3bBstV5CqRwEda1GHb4KBz6XmkRUCD2GXjBWUDnAAlsCChjFAgFBhTBJOrCtZYrDHh2SQiUhM/YZXP1WnJpp/fvm6Jk9n7eAXwBhBFJrurC5zBdWjfov0G1KFVDVKgA6eI8rTYIDcLGByeArEFpmgQWw0vSUyBLZugZNo91B0EzlA0qKmi+dGFvzhU+tEiJBLl/0zWl+D1GuTxue4y+FLStsj9CMQmOSNKyO899D4jCj/cq8RbHujLMIh6mKfAB6ALVB+CLgEwUoFRymIB1QLRnrIoKWx4RiJxccCgaiNXSFavLZJ/3jf8wkQneC2I4gr+mUlNMmHfQYQS9dvnaL765mKXjmEJUUcYVjBjAsdRC5QjUQ1+jFxoSv9LfKGJQNKEI0FIcglvK/Sa3Vv44ZnEG6V79T5M3XlwbkIU85V6soAMhoAOdB3mom44IViLQpyYMRbxijEmPB8JxgbBBsFI42euSOvOr8PzTzaz4bZ3wRW7Pw+8BkRxNQySuobQjfQR3wfxQpM5dFy/FAGxqBi82OAxNhbKGqmXmLKO5xB0vyg/CwVIGbePSCYmHChSopI99lz+h4w4wq9MK2agrMmN5TYrS3YmQVSLA9gg1kxOno+VDFg+OdVsUC70LJiNhXgPkRgkiUNBL9DhYRiC+TcZAxyJk2k0XozLw241+UUrz88Ct5wodPYXLnqEAX889B10IUJWXBtXVx1X9GEVNqgpUCxeChwGpECqBdIchL955KuNCCEF4hJRmNFZjQkjS/IRzOYp+b3k3wdMZQjvSLJOJvOolITXmXhPQrg4RyOjQcFkCniiDa+oC2N6F+OSKIAW/CkiBQYNf0XRFFYiMNpzDRCe2egsVEJkZNDklT6IrBo525ZZNhDXxCH+iqnilhNFBpKtKbmsrQntJXp6o9UjWZhk9vSj5DDgWyZJBcOLDUp19DuMjN5nLzdb8ZLGqzIdKCeKfO5xvHFWuZHgkvvPTVP5HAbmJ5E+BI0e7UEES7Qz3LOAz+cpTOW1/KIMAbWjqJffXy40Bg6U3sgkFGQqP+24FtP3Or3L8R5fALwmRJEe/K6HMq6WqoqPeRTqgmdVJK1U6bVGXcFr8FA7h9UeFQ8GTF1hlkukqglKJKjrwZ9HMaMdxkv4hZqA5qoYVQw98WKZBHQRxieCSOLIuE0yepFhMIaxA+JnxJHWDNGBRhkQWgd1woiADQoyukQ5RmQ/iIYEMVTp8NrhfI/zHo+iGJCS4JtQIPp6pIPo7fbRTyGaC3tbd3u9Hdtr0AuB14Ao5uKGbO0dvnuPdz3ad4Eo1I2yN0mej4quenA94npsUjCNYMsSqRsogvVJvaK+h34NrCEhPIqXbHaaxo0ytZDT4mzCsmPH9jZNCnlSqtPdJutyiuViPHXKAKduzURkg/6jFtUF6D4iS8AmWSuGc3R4dfSqWCUQBcm8rVHkC4Sh4lB1OPUhzmrg2kwnOOgml8BlHPMFwGtAFNeDgYF7Hz6aizuQI90Q/RotQF6h9xq4B4JJ3uNkoREFSSKZD0gAwRciIMnyMohQgIawke1s4F0EwXRbroirzA6PlCazgxM3EkaLjsrwDEYzrgT9ImX7RMWapKTLqAaM7g5JoYiBoHKTsQcd/BQyXoOcJKdiX8YAZ6Bb/+fnj9a352MdbwxRACFwL3EK3xOsILm8LBlBhNglVWi9sGoVsVBqgZUKIyWlxEQa9Yj0CB34DfhV1D96VEOog5GKgQMN6WWMXttcHp44DXPYJWdH0ZDRkiOZOCYTi/NU/c0RFB8iatMCnyzVpigwdgFSxTwRj+JxLn2E3hssGnSVIlipNOZj4AV8gToDvoDBKpX+7tCD2HX7U2V8ui0f5/llqVtPFDt0r0uODZzCpzyKUUMMIIxvY/ATKF6h84p4gmkyigiJAYgoIkFUgOAlFwAvYR8g+DFWKhebkoKs2fW3MGL7RQtTX0UMWRwehOTPJGNHMpydJK6kLEtc1XUkCoEUvsJgMQvPzXsdOIXXzF8T7yGIR4GTqZf4uA2jv2L3LUr+I7+/yYu+SMF4McrFrSeKtEoMOsGMraZXYDQkwQyebO9IvglNIpAGsUFEgrOr71Dn6L3Qao2hojIl1hbBUQWgiusduulA+5DBV1SBWAob4p6SrI2B6HUOyJVWbJ0Sxo6AwZFOZNyjo/iVL/ojYilefZSasqxCif8Jk3zsICEJ6jxeHYLHq4AW4G0k/nCssQZiNmHnPF4U5/vguQfEFsHy5jSyriEMN3C05PDM8zqGB3AJgqu8QPTfDbeaKNLK6AcsioFuwGgaDASBgut7fHuOb88DUZgCMYEoNOoBJok2fQ/tBu8cbW84Z0nBHnu2pijLSBQK6nBtz+ZsA9pS7RvKehkQyKTknzIgFlG21qCMez+aJXUIf1DEJ8RPiJTwJ6FCFJm84vtwvjeCmpQkFWK6VT3OOzR6m4vkoExh7Vlwngyh3uBbRV0f8yMM3oV8dGPAaLDY2SIo4h5l1fdY8XSuR1PUsakRU+DpUdfH0BaHJzlLJYbW51a1cNcjTNl4SpcayF52nfP8cKuJIkAuh1ywP7fOqGMIV5jH+g/mz4is3oUMMQWPxUcF26QsvXis6x2bTRCbtPL4KhQwKIqQWwAM7pDpCje3AWkQY9D4wjM7Udq+tZLqeFgmYqejhjTYuXo6yWkIspJ6gwJ972jb4H22hWKLkdsOTzVG4yqxbA1Bh1D10e0iwzxGjWdgjcMY2wKTbv2azHsGL8Ox9xoQxTVY6U55Na02EpNislgmFNf3bNYrnHN0KviyQEuLFBZjQxhDqg/14Ufn/OYvf5/N5ilUH0P5iEVT8KX3Djk8rCkKQ1O3WCtUhaEqp6ZSI1HEGNSZlG4q6PCKkkKd3YWAsQHlbeZ7kMD6EDVYU0X6j4qCVzq3pu971CvdxuN7pWvh7FzoO+WzT0/55MMTjN3jy+9/zIN3H1NVNUfHJVVRAsEi5xScSijKIAYfdYchGpeoS3iPuqirJPlQzEBYNw2uJAoR+c+APw18pKp/IG67D/w08D7wLeDHVfXRy5vmFXPkmmpW0mGT/T6+FBM/g6FeoXc95+s1vetpTYMvaygLTGGxNuoHGlJRP/rwnF/8xd/j5OQRrf+QTj/l6KjhD/6hr/ClLx+yWMDRkVCVsLdssGYRstWMDnK6GJ0q+ghgEY31L9REbgFRSYh6S7pvHzmggAsKrmCwSUHWNnA+7+jWZ2zW57heWZ0LfQtnJ/Dph8p6rXz7n37G7/zWJxTVHj/6L30M1SP29vbZ2z+ibmI8WCIGLzg10fcQRLpUCCEFYKpX1IX9XkMoYSKISa74pZAMCBeJVy8OrsMp/p/A/xX4z7Ntfxn4e6r6V2IN2b8M/KUXP73rw67HMxdOdoNOvycZ34fKgN6DWoMpSqQos5ihIHp49bSd4+y84+ysY9V1rPse5zs+/niFsSWLBWzWUJbC3lI5Xwaxqig8xmgsnRNWfWMMtigioYb9OVGkUIm5g0MHhxixeh/RfxD8C951eLfBe8f52YrVeoXrGYniFD79RNmslU8+2/Dp45ayLnnydMXp6Rkilt4FogtKe4yGFTNYvgb/xeyRTqwBg06QiCJsm9vHshu79O29DLiSKFT1vxeR92ebfwz4l+P3nwL+AV8wUcxhbrgbZOz4TmLYWTg2pkYmZQ4VXO9oXTBCmXqf5vAtysU+UlS4WBSg75S+dzw9XfPRozWPH615/LTj6UlPWTm+873vsdj7kEUj3DuwlIWwbEqWTUVhhcVCaBqoKuHo2FI3wt5eyb17NVVp2VtW7C2r0WsN+N6zWXc4p3in9H3wCne9oeuFvvecna3ZbHq63nO+9vROWa97zlY9fe95erLm7Lyld7BeW7peaFs4PRO6Xnn06IyPPzmhqR3l29+hs0e8/fbbPHj4LoeH90AstqiCxGdrvKnx6uic0HYei8e4qBdlnj4l5DFKDIo0MYZsmse9ewnbsiu+RLHr8+oU76jq9+P3D4B3XtB8XhgEothtqAUGwkgxSerHektBxw7I5FSwZUO5d0TRLBFb4mOxsK539F3P+brl6WnHo6cdH3/c89kjh+L59nfPQByL2nC8X1AVhqa0LGtLUQiHB5a9PcNiYXj73YK9Pcu9exXdl5c0jcUdNxR+ESLQozLbtY6z0w195+k7pd2EGq3r1rJuLV3n+OzxKWfna9Ybx5OTjrbznJw5Hj/tA9I/7Tk57emdhHOcofewcYLzsFpvOD1fs2jgne99wv7B94btYeE2WFOCVcSUqJR4BKdC13vUKNWgW4yfwMgCZ0Hy3IucKK6ALEHpZcFzK9qqqiJyIY+7MWUzsyepg+lWI+sYrSI+JvKHuDWLQ7CmwVZ72HKBmBDw1nVrnj59yma95snTR5ycP+Fs/RRHR1GH5BqVWHTACL0TEhl6FNuDE8+qU+qVp1OlWTgeP1XOVoa6Mhwdeo4OfTDHxlDrrvOcn0Wi6D1dG4miL9h0NnCC0zXrdUvbec7Oe/peOV87Ts48zimnK1h3gvNC5wh1rTSIRUagsIamKqhKw3q95tPPHrG3v8/5asV602IlpI+IMTETsQrVQBWc8xjnYqbdyCWCRS3lqttwrjHTsJBdL+uKrbukq+dlIp+XKD4UkS+p6vdF5EvARxcd+DLLZm5zgJ08YXiaKilsPL0sgBjnE/UH13vUQ9cqm77CiaUu71Hvfwlb1dgilHpZnT3h29/+pzx+/Jjf+fZv8p0Pf5fT0zPKasHyuEHx9F6i80xZ9R7pCPWQTPQ9PHaxxioUNphx69qyvzzHWsNyUUTxCZBQRME5z3rT0juHc56uCybjVktaX+JV6btQaCwpthqRte9juc7kkGN8FMmXZhGKRcWiKbDG8tmjT1mvV7R9z4cffcKDh2+zqCsODxYYKbB1Q7HYh36D0xXrzQZFWDgHRYwFU4f4WPjMVkhRBmKKZUeff+W/tqnlWvB5ieLvAP8m8Ffi3//PC5nN54CtxyBT+8RuN4ZOH6EyKKkhklZjLr/BE6pz2HIR5GhTIgh97zg5OeHJk0ecnD7hfHXCanOOrUqKOirFzgdO1Pug/KrifHATKoTKgo5Y0j5wqMJ66spjjNA0BU3Tx8iP4Mhz3rNp2xB35B193+OBVitaylGxjfebfCp5AGRhQ42mYPgJYplhtMIN9a0E2s2Gtu14+vSE1WrNZtNSFqnaYSzYUJSgHu9WIYw8D6MZNO84ITGZ6JTFmj03vDih6jom2f+CoFQ/FJHvAv8hgRj+loj8BeDbwI+/sBm9Epg/wKhwx6wz58LyXTQNxpRUTUXVFNjCYm0woTrXcX5+xsnJKc7BYnmESkVR1sFenzsHY6abxBCSVKJAC0K1bxVMjCQ1BsSECoGd79GNH51gEuboNKbHWouYMuS7DVYeCSVllIh4oXhxiPkKofLGCtam+45EYYWyKjCpN4eGpaPvHd73nK9XPH76hM8efQZ6yPHhHtYajC0p6yVqDKxPcb0LXvSUB6/hfO88SCi4bIoyJmqZF0QQLxauY336cxfs+uMveC6vAGT2ySBm5PXO4XoPpqbaW6BFTbPfsNyLL1JAcPRuw9OnT3n06BF9r+wfPqCoNjjnQviGxAodJphTU8mYoGJEh1vBEDBoTbDfq442/dY7Vl3Se2LusxVMEcJTjBGsDVGpxreUGiqoqw+lZUQsRmoEi/cO50JpTbE+El4stoZiC6FalBhjcU7puxBi37qWdr3h5PSUTz75hA8+/AAjnnfffkBpK2xZUy4P0bZAN5/S9T2F6/E+Vi9xPb7vQgSuFJiyDp9odbquFUlSkOErgFvv0VYuflAK8yLV2wcwShsjt9ewstvgmwimw+jgS1f1nr7r6boOVaWwBUXho/zeDebfYJqfuNTHlT+W3BcYaiDgo97hw735TBSC5PiOIRaJ8ISBA5FdNzkm0xQmAcAipCoi6dhB+Y0OuGQx8qo452i7DZvNZrhnCCKUKQq8i+m5kwSikQOr+sgxYxLS5/Bm7zr+ZRDKrSaKnCCii2vnUcnUyoD8Ov4edoWIT+eV3ita1RR7D5Bygan2CZUwTHSKKV3nODvfcHKypuuUsqjx3tB3Pc5FrcEEjiE+mCAherFNmn3IRksEGVE6mClNEGtszN8eysYIiA0+APWezgfuYAWslCQPMtqDeNREBXfuNZfgkfe+QNXibQlaIRRY46HwGHEYWcf77Xj86BEff/Qhh/sLnO9BamxZUS72cQK92CHPIuRiE8WnLli4rKBFFbIWTcG0Oc3NEaNuNVEkiOvRRHlO5JHwf1IgLDs+ZY6FFTEEtzlVsDXF4j5SL7HlEontRnx03HWdY3Xecn6+oe8DUagaVnKOc2MFQJEwvkgZrS+K2OgfcdFIG68tceW2UboT0eDxzkMistl739P3PSih35wtAcX7FjSmgEo3eI0lldyRMo4iqNpoMS0RKgQbw088Dhcrlih93/H06RM+/fQT3n7rfiyjD6YsKRdLBEcnlt4FY0B62Bq5jFcfIoFtGau176hockPgtSCKXZAb6bY0iCTbDEcSq27ISDdGIAYAptAO9Urf97je07Yd603LetPSdSGB3/s+RInKGBE6ZhzHCnmDFShjXWlOzErQS9gxkntS3pMlJ674EnWYNGZS8IXYOyJbD7JTc/Dq6V0/jhGJTmMKXuIWbduGYMI0RTGhE9FQ3SR90n0l4og3lNoNXCI6fdFBgq8tUcA2MSSZfXxjsW6RCdllahQ1IeJTqgK73yB1g1QWNIgFp6fnrFYbPvnsMR9+8hkffPwZq/U5q/PTuCKuMbaPK2ToqiqmCD0gJSQZmehDCBxKhrmMEx15XxIRfao/S0paiqVqyrGJYp9qRJkgaiEaW2DqxFKbGEao6hE+vVPOztyon5iQlNX3PSA45zg9PeXRo0ecnZ0HaxIGYyuKehmKPNgq1IMK1Ag6OvS8ghoben6UNdutD24OvLZEsUUMMF2VJ5xChlU3eKJBrEHqAtOUiA3mUnWOdtOyWq05P19xcnbO09Mz+m5N161jg5QOY5Kt3o36i1iQ5MjqY6JfSDzSnFsl5T+zEKhMMrCRbL4SA/7UO1wshWmGSuPRf567C4ii2qz4mvc9Lvb/C0NHpPYOEcF7ZbNpWa1WtG0bdaDQd8MWFT4GS86DAwdl3YcCamKKHfrE54e5VepFcJnXgigGVr5jn8x+BPFaRmtL/GuswXiNdWKjjF42mLJBTMghUA1Ou03bxs+GTbsJEag+WV0kmEOJZSVFI7GlyQp5V6KxGe6I9pObSpW6s7CUUSH3gxiSW9nG7FbJiD8NKtn4OujdqmkWYK2lLAqGpV4DoXnn6LsupN/G+aTausYU8WNDtZN4CZ8IQuOxRRlMyrl+dIFId1140eLWrSeKie7MxOq59Q0YlVZjYkdUE5xyRYFTQYoieHerhmJxiGn2A1oqeN+zXrecna04PTvn5OyEk7OT0ApXUqyPIARFtow9KTyKUxeVaTMiTWRSyWQ5rngR0TEYgiMuteuKPwAf/RRBOQ/VWmc6SRTRxieREVAq0c+of/m4ztdFxXLRAGAwbMRgraXvOtarNV3bxiaaYIzFljVa1hRFhY0N5hXBayhw4H0Qn6wtsPUCWzWhG+z8/ennJ4wXCbeeKLYgo5DdDzj6BSSaOdNfYxDjo61eQzl5WwYlctBUgxjhnAufvqd3PYWJViIYlExJsr9IMPaqJ5XH1IFlxQlHwpg6VWbkrSaOw7C0iiox3SL6Lsbj5+Z7mf43jj4s0+OzMyIUNnA3mxaPKKZ47waCSA85cIsQumFkDAWf+38gcJWx/3g+k5sDt54ohCCVJ8uKhuWYIcQ4r9Mq6aUU2LLCLkKRZKmj8mdqjGswDky9FxGCQAyiUFgWywWHrmd/uaC2NvAEhd7FCNgoQ6OxnvHEDgZTJNDhz+BEG44ZxaREOMOuVExMJFWuytaC7JtOf+fPbDxah+MGZBaDj3MoipqGkqZpaJY1i72aqi7HHIhQUxkKg1Qlpm4wlQ1xVJFoJRZcNrYMz70oGVNuxxu4CVwCbjlRzM2uYSUL+4ydPeUkPEsR4oWqhmK5F1bBZgn1ApzBSInxBtscYq0ZrJsOQQvDYm+BGOFgb4/GFlQInfe06lJrdkCRKPubwTwpTNA3X8o1E222Vk6fzX9yM/necagLPLz5qfkVXJL3gbGVsIRQcjEUZUlRWJpFzXJvwXK/oVqUgyIeTNeC9IKpa0yzxFgZiEJSC2RCnFRRNZiywqQOT2SEcUPgVhMFMBVId+6cFlNML90klp918Bx0DWJcDlsaybCdbMwkFyflMzdzhRi9OcJfhLhzzIgeisk1t7FnJAS9YPs4h0EL37r4VPH1GtKB0hVFwFhLUYSQ8qGPRkx+IlqsAndN1rH0BJPFLCuxI1uXvTFwq4lCh0U3IaJgbERAyREzxhAJlNZCkSJeU6kaBR9yG4pYTMBKNJZGJEoWoL7raduWrmvp+o6u7/H4wYQ6YwCj1JOQJA+jfkWgmqw/GWnF78aYSYwSECqYdF04RkPtp2ZRsFgsODw4YrFchm6ySOghH6uIL4qCYtFQGSX1yUxtC1SD6Kq2IDht7vwULxEyJJP5ahsJIis6VthQxEusiWVt4hjeIQiFhLgiK6Fs/tTgG5Tsvu9DjFMfFG4vGnodopNVewwfh61+EK8Icm4xBPHtII60Pzyv0WkXevUZnHfUVcNyuUddNUPYt6CYmBVYFRZbVxTiMTYYFsTG3AliATZrUWtvJouI8BoQxQhb4kdmIQn7gxlEY3dUdX1AUtcPsTiCx8QsNxmKqka27z1tu+H8/Jz1Zk3XdyHJxyZxYTabuX0UGS2qGWWk4+Yr9nx7vm0XXBYxOieOOWHM9yfu611w4HnvscZSlTVFUWRIrcNzMnGBynnAIIAlZ6O5oJPTDYJbTRRjsN02MYyWy5B3rHGlVt+hroXNGnd+HgwoKqHMvhTBiScecS1Ge/CxuLAIzvU8fvSIDz/8iI8//oSTkxPOV+eYqsQ0JTkhpPDuYJYlim/Tan1XwYv21O4imovCsZ1zMd6pp+8dfbdPXS/Y3z+mbvYQY/AC4j0m9RDEoaKYUIl6yDAMi4oNJu4Y5rGzv/cNgVtNFJAWLJng2ZYlMlNUvYaCYMSS/IlTiOuj0h0iQyU2Fxl7uQUOs95sOD09ZbU6p+1CcJy1BtFitvrGeWUBfHqBPrELWZ+VIOZjpNV/V3LOLh1jvj956J0LkbjOO6wtqKqaoggRvzHOGBN7fpiJryX56EdlG2OD/8fYu9inLwSSPX8i1XjEp0p6Y2RgMJ0Sm7m0eHFIu0bW5+A8pgpBgapBfFqt1mzaECc0eMWNif0bZJzAlvhEFJ+2ZfvhKNWdSPwshDFwq9k5u8adH5+ulXM8AxgxGGsxQ8V1CfjvHNq34NrYpSk81xAwGK1viRBsEWOf7nSKLwhkFGWTyKIe40Nrr5BvEGo9mWinxzucW4Xy88UJcvoUyo5iX7BVhfqe8/MVT56ecH6+QjXECRWFpbCh7H4oeJA7DBPn2m02vS5clzAmTyBDdu/9llI9Hz8dmxOFlZDdV4jBRuebMcXQFNi5HjYrpF8hvkPEh0y9PuRQeAUpy+AbKiukqBD74gICXwZcOTMR+aqI/JyI/KqI/IqI/MW4/b6I/KyI/Fb8e+/lT3cbdMcn2ch3IpHqkNMg46GD91hj43W8w/ep65ELCrqGFbDvY0NDRh+CyGCNH/7OpLqpvXbXvexQdj9PuuWziltzghjGGf6mBSYjmBD6Gior+j5WWNRhEQpV2+P5Mc11koZ6cxnF1URB6Gz476nqjwB/BPi3RORHGOvJ/jDw9+LvVwo6+TaTqbeO1FhQzA+FxQZhJ/ZrMLakKCuKugwvdn2GX52i3YbUxyFZYrwfG6SPCEMmFsjQESi1upoG/c3uJbM8XdeKNIet7LyZXrJr/26r0+hXyZ2YIYzF47qW/vyU/vQEd36KW5/hNivAI1UxBFWKDwGDRV1SNBW2KLBDU/qbSxXXqebxfeD78fuJiPwa8B43qJ7sRKee7ZPJUYrR0HeBQcSJNY5MaMxui7BNUbr1OWo7iu4oNkcMeRKhYkdC8OQjSUgXxZzB9DqqnFOfRzb/Z+QGu0Sp61S7uOyYnZap9InX8vGf61v6s1N614ZFY3WG0Ae/RFlgcFHnCB5uKQooKmxZxHD6m0wSz6hTxELLPwr8PNesJ/uqymYOUnwyPcmcIACNyT/OjRib2dDDMfEs1aCQiwmiQVZZMJl7cyvNED4xw60xSyIRxTSadXLsNRH2IuSe+zWuq4NcxC0GkVQHw2ogEvWo69BOguUucd6BUwb1PNULydQr5t9uIlybKERkH/gvgX9HVZ/O2PCF9WRfZtnMkamnvs2MyJ45l0I9fY/vOrrVCt2ssH2PSe1wTQW2Bu/Rvgf1oU2VB0wP7RraDXQhj0AJynTfdyEcorQUrsSoiXOJHUKnCkX4iHIdqfUqEWoXp9h1/rNwofwcFxcO9akyh1IYobaCdR3+9Al9WVD4FYXvg9PTSPBHmGCAMF7oUZwPVUVSvvck/uUGwrWIQkRKAkH8DVX9r+Lma9eTfTUwFNYfOUZyEUSl0DuH61q0bRHnQvJQCNABKYCgNKqLXMIRajP1PbgueL6j2BVqIUVRyoUcg9CqLsjL41rLwCueBxGu0iV2HXsVx7jMT5HSaH12XStQGsF4h9+s8M6AdFjxIQcFSLnhRmy0iIcqgXn/jJsO17E+CfDXgV9T1f8425XqycIXWk829w3oIOIwbFE0dvBR74YXhIQsu1TsF2PxGNresW47ui40MMRFy0qyPkWRCdVYpS/vgZdsWPncGL3bn8PykluhfNRpJmJbdsyu87ae1g6F+zIxKyUQGTFYVYx3WDwFsQ+FJguUy64XlfShCopH48IRMgjHdNabCNfhFH8U+PPAPxGRfxy3/Z+4ifVkc/FpiDVSvOtR1+H6Ft+HkARKg5QLxJZQ1lDU9J3n6aqlW60pjKGyBaYoglnWOYhcIYVAFLagLEtsYWO7LhmqfIfLC2jUIUzI19ZY5eI66JCQP33f5UtIBJmfc5XHOoerjklRrqYoKNRTuZ7Kd9TSURuLcR3GtaHbmTVDSX+xNuhnvke7FjUO37e4vkWKkkJurq/iOtan/4GL17ZbUE9Wh9UpKcuqChIqZmNDbFPINjP0ztP2DklJSt5EnSQi27BaMyClGeoYZRwrcYm4WWK4R5LohtldQ+bPOUWuTJtZ/NBcbNpFGBeFj1xomUqKs0Qnpw/cwaBYQnUSBsIV0FDRY6g4AlFHk8ghgkl8rId18+DWe7RnfIHt1uNjgktA4thvGmKtl9CwRAm1YxfLfaqiolBH6V2o0B0x2XvPerPm9OyM9WaF1xFJ8y4Lg66fTVJ9LJKmEXnYtvqkv/PvueiUc4l8/1hyf9s3ceGz0+1o2a1nGz30fdezOjvl7MljNnYfaMBY+m6DOz8LVdqLPUwVqw+GKEyMEUpjUAl53oN4ezMlJ+A1IArInu/Qdncq1ZsYqoCYoSixUQaPq1fBq8UUFfuHR4hz0K6R9Skh2T6JYY7z83OePH3C2fn5gKRmcHAJTvNZ5bpOlKdTITO92sKU6w67iAJGxPfeb3GOtD8fM/+bvqdEoy0CkpCu6rzSdS1njx/ztClZVQ7lEBHo2zXnT55gCsti2VBF65Mag6hijaG2odicB4aIgRtMFa8BUVzj4Q4KbrI2ReSJnGJQAkRCqyqrQXlM/dgiAmt03DkXiygnK/BkJpmymQsJF0zzSu91vMiWt3lwIjDMfZc/Ix/rKj/IEO9kbSQUCJl3wRnnncMn/SoSUVhkTOiXMYiRZnymBGVdoxf7JivYCW45UcSHHl1k4X3MPEViEFuBWNQ29NKgOEy/wfoNYjym3CB2DWWNP34LihqefIKsN+Acrutx5yesz09Zr89Yb87YdBta9bQEMcF6SDVkR+dc/KhH1QTF8xLr0y6dIHAIHQjRmNj0RCSkyka/CRCV+SkheA1Bj8N3De2KQ+h2yEoMNaRi/rU13L93n4cPHwLw6SdPePr0jMNlQ6Eduj6B/gAI/h1z8JCivhcscYsKpEBpMc5B3wYuIxVqQoiHjYQulywGXzTccqJIBDEW9hoVXhl/2xJMgdoaZxq89BSuhbYD4zHtBsoNWjV0B/fRxSHSe8R8DK6l63u61Tmb9Rnt5py2XdH2G1qUTkK0aO8JPa8lXwlHLSfmq46bs9XyQt3ChwBEVR1SXzEKpkAsIfQk6TWxJmy45W2RySdvPiAEa5lqyi8J1ftsLExw//59fvDrP4gqFOb7oJ+y11gK30F7GkLF1YCUmP0FtliGu/QtqiHZCO/AdXi1dKZEYwPNYshTublw64liJruMXyahFAFRJa6ylCX0ltQ5W7xD+g51HeI68F2o+RpFreSLSEXBxqy6WQzPgPRTQYpkph1mtC3mTESbmbI8IZT8mGSJGjjmLpDBiTmPdk3bhiLOUTxs247VagWEbEOT9bYIBiaDtyVqKygbTLkAPNI56LvoK1VwHh+teoNolSJld871ZsCtJgqJlhzjZTSVJO15qBgRiwwTgtWqw0O0KVE2bDYg6rHteQhiE8WefwZ0SLcK/bcwSNVgmyVNvaEsK0pbUJqCQmzIM5jankYkJukFUc5O2HmBWTSd5JOIARPlefQ0byvMMuM2oyKeiMGgNhxjxITIJAE1YW6qnvV6jYjwyScfDwaKtm0xpsPYEi1AC4uvavrlEf3ePWgOKBeHwcn51OG7VTBbdx3atvTW4soSNRVqq9jvrrhLR33p4CVT7LJy9FGU0SRiWUPR1KhRurKkl4hMroNWQxJMd460BnwXiMqEynamKCiKgsJYrLHBwyuxflSmJiTcTFeert86coy5oSfnFpnzbcusOrNmhrik7XV3OH/wl+Sm1/GaKeRd1Q/NZs7OzinsI4wxlIWEntliooNOUFvgywZfLpB6D9Psg2vRsyLGhY3hL04UjyEUlR77Z1/s+vri4bUgCo2lLckEGp3iUbB+2BKp98GWSHOAqQ+G8ATXK37Tok+eIusOv2rx600QMdoO0zu6ztF1PV07Vt4eetDFK4cCCeF70nQUiXZ7QLadVhOdYoyU2vJTOBetQN7jM4/2xPKbri7j+blPA0IhZWSU65NhwEQjhfeOzWaDMUK7CVNfGMHakkW9DBU9xIQ+F95hXBv0jHaFbs5CUYiux3cOLSymWkLZIEVNiDMzN5kmbjdRpO49aqPLTmNV2SRNRa7hJZgXTbXAHFrEddi2Q9cbtGvpz57i2nOkWyGr74NYem/YOIOKpS4XlHXHet2xXrWsVxu6TQs+ijgINjoHk4kyGWODCDV295Gki8NW/BIQEpKy0I70NxEFgHcFjmA6BUZOkcy3Wf3c5NuYEIVq7Nw0dfAlUa3ve87OzlAlrvieCqUqFxwd3md/uU9lCgoM4nrYnKPdGn/2GHPyGb5t6Vcb+rbHNiV2cR9TLzH1Ptgqdo25uVRxq4kiwBiRqoNyPVqAxlZTEqpJFFUwXZY1UjbhfFOgGu3ybQf09FrQU6JGsF6xnrGsfNZAPbNzZcr92HU0arOkr2Pow7ZeEBTp7e3p3BRlmu8Z/C9MdQl0LLl5UbDg3AmYwKuPkcLQtT3OefrOIZggOkY/hHgfUlGdIH0HfQd9G1N4Y7MYMZiiwhR1zM0Oi8bwuG4g3HqiEKLCPWzQrf1m8DdHti2CWRxR3HfhBTb7mPV5UBBd6ANRisUQomfLg2PK5R7l2RoxRaiPRrD1SzQBq2oIicCj4jOyjKq+BiLZJU1PLU/b+0SEuq4obDCllkUZwzqCLhAOtBlBTC8yR/yLnHg7dZfYFbbr4NGnT/iwMFS24vH9e/R7+xRVQVmVaN/SP/kUd3aGR9B6D6kNxcE9ysPjyCkavJFBB5trXDcFbjdRRMwbxOPknY6In2BSTiC24zV7x5hqifoeDu5h2xXeObrNBu8chRhqCfWJ7GIf2+xRnq0QWww6vTGSIWcqxe/wQ/3x0U/hBu/udGWfe5p1ThXxnLquMfUcuT1eXWRGM2uObo8xpJXuENsG/YTxXsIYoZpf1ymffvqYum+pVHnvYEG3t2DRlCyaErxjc/6EfnOOVAvsYXi+9ug+9fF9TNXgrRlaNN9kVft2E0UGSWiZrJAT4SZbRSGsgEUBXoIohSLOISoYF/q82diARIqxKHCwrmSouyuIji2cHFbGnXO/KEI1218WoSQ+QzxULKPfh6uppr7Vubl3/ns3Gl4aajJYzIKWpNF3I9mCMNyZLSCKpaZeIvUSU9ZDJPL4KiYC4IXX/qLgVhNFsPZMTYwqNtsbV9bMZabpizFgquARNja8QPWULnh4jSg2rvVebPgUlk49bd/Suh4Xr++HOehkbhLjfca+FUQ/xGj5mazOmq7HgOCqSlEUPHhwn6PDQ7qu4/TslK7rWK9WtO0G732wBo0tH8Yxo9Ke511cFE6eK+Vhqor6LlQuMSXN8RH7bz/g8Etf4v57X+VgP/TPDq1joJKHwUxdLSmP3gqGjWYfyjK0qBCHhCZkpL7kNxFuNVEMkGm6oxix44HnfrNkd8dCUQ4klPQTi8fSA4r2Hu8UbwxOPZ0LnUiDzB2cYH4Hb0h+gqRWpmC4eQDebIqZjzmAMYaDwwPeevsh6/UaJTja+r4brFK22F7tw+V0yzybX3tXElM+G9Uerx0YpdrfY3F8j+W9++w/fIuD/QV9t6Hv1qgYimYvNM+sFpSHb8UmmgY1wxOAWOnjJgtQt54oJk6zmW9icty8rkJc2WX8MZyULJqphW/Ix3b03o952TFhJvcLDCKSJy31SYYZQzT0YqKIno6ZnyKEWVdVyXK5BIIpNnfs5RanqSlMh83zjLyrRLY0v6QnKT4sCrZAixrqJdRLxBYYWwSxql5CWWOKOiuNOXUtSniws/dws+DWEwUwoYCE5ilif/JCJDRkkTnJRLkqZY6hoEZw0f/ROs+m7VhvNqzbDZvNhrbrgukSH2TsSYkcyWS1QDg+i2YdcThbuXfYY7x6ehfqKR0eHvLOO+/w9OlTHj9+HJ1rJotjUlSDTpTCW9I1wi16hvb0My/5RTkageh7nN/Q09MWFW21pFseoUfvwOFBSEft29B/r6xDP20xYEu8Mam2Sbyr3BhwA6khwpVEISIN8N8DdTz+Z1T1PxSRrwN/E3gA/CLw51W1fZmT3Q3b5kzNXvgYLBewNLSiymT/7HQB8gakiXzGjqjBZDsUEBg9EbN3PHFMjKt0fsR1nFcDp4Cqqlgul7Rtu8Uphmupxgoa28bOpF/syrSbi1UjUSTO5fBoximqyCn2EN+Bq4JfpqxCau/sOUi8l2Gb5HtvHlwnKmsD/DFV/YPAN4A/KSJ/BPiPgP9EVX8IeAT8hZc2y8sgrch+dFakBLzxk3sNLoB0sPVgNEYiRPYvBis2lHxEhuLCoyikg34xGOFl/D201M2ibec+gV3OvDQxYwx1XbO/v0/TNKRMu53xUTMiN8N+k33fTlPNs/vGqiE+3LstEWPpe0/b9nQuFE4O92ggVRLPtIXhkx69Erhoskxs8+sbA1cShQY4jT/L+FHgjwE/E7f/FPCvvYwJXjG7+Ges3pfCKIwGC7tVYpfO8Nn5IhISm3hCEQkjNne3EoIAbUqUEYkN5xU0Ouqiwj0nBIwgNuQqWGsx1kyQMpf1twgi0pgxhr29PY6Pj9nf30dEhooic/9CwsBUsc+YeG2Tel3vztvOQ0lSWVDvFTGWIlYabzvPat3Rtv2o+YiFmC9hSG2CNTzG+EgHWdYL6g2a55bcQLhW/K6IWAnlbT4Cfhb4HeCxarCSA98l1Jfdde5PiMgviMgvfPzxxy9gylPY4gGabc1t9PkhacUaYpLG2CRN4lW22stAHlkexXylzy+UuEUOkrkQt5Ay5xJzs26IxC2KgqqqKIpiK5Zpp8KdtCvJBnsGGOYiOc+dcrJx8R9s0kwKV6sOc5ns0emYNw2upWirqgO+ISLHwN8Gfv91L6AvsWxm8I4qLjZhDCw7IkpynQqDFzmTUnLRNg4G06y58eUWJrSmqmxJbUsqW9A7N4hQDOQkeSMfLuBLAwwWoKgojzvCtZPu0NQ1B/v7HB8fc3JyMhDGMPUd1iwfq4dkD2HU8LPrXwbBTpDENGVRWg6akqa0E60pGJOU1C8whmhF41uswBgPnhjHbig8k/VJVR+LyM8B/yJwLCJF5BZfAb73MiZ45ZyA3gT0swETo8IcG5sDedGt5DYzifvPB5PxS9oXGpYUVJEgKlvQmS6aFxmY0lx3ScM960qQcLWwFrGWJuoTx8fHPHr0aIiOhVH8ysWoJFpNVvvYLH5Y0WXsWnQh6KigG2BRFexHojBJ/CNUFxUJvh1ispbzyQhRRMuczGnyxsJ1yma+FTkEIrIA/gTwa8DPAf96POzf5Isqmxk5Qa7bhu2j5SP/lh2ZrZ7brHyyNYknhqAs28wEO7ncKL7oKIxNB2SqO0yQMtGYpgIIEvKmraUqS6pyKj6F0Zj+vcj3MLnFiyt77AKN5rWknxhjmGJ4LqrOn/PFU7mpNHIdTvEl4KdEYrgk/C1V/bsi8qvA3xSR/wvwS4R6s68Ukrc4aP47CMMmTjH+mRf7mP4Qgno+Axv3lQZT25Aj4LpgzdGon0ficoxFiYO1Z3qJhPQw6gJAyIFQUE09rKFqFuw1C/b39jk+Oub+/ft8/PFHg6LtfOg/qjIqytMHlJf2GecyxDLtMMVOQBX1EuLDMJRVRbVYUFZV9I2EY4z6sMaYJCppLLyupILTN5UAdsF1ymb+T4SeFPPtvwv84ZcxqWcFkxSEmcKbv4gktl/0csbTdxwhQCFIIZjShuYjhck4VLRGSSQKMqS/xvyH5CTNCjgTLEdVVVHXNYvFgv29PZq6GYjCex0ENt3FgeIKcZmIlGfmbVnFSIOHO7RFCBM3RVg4kvo8KBEyNniU7IjbRBDwGni0J4/8iqd/lUy7M9eZrO2ujCKEiBlWZzMzpw4iVz6nS6SViTlWR05iraWua+q6DoWcrY3FnEdfx3Bu0qfJwzeyO5oRRzrkoiDB8WbGtNdkVraDaTeXmrJVaIc2Ldn9XXTMTYFbTxSvEoxIZhq1aIyJMjGyFgkGLDOIK9sWImBre/5J9ZkEqKuKw6MjDg8PWS6X1HU9mGWTZSoRptqxgMGw8ocfYVt2HxcRwK5YrJwYyrIK1y/LWFCNUS+bccXEeW8eyl8Nd0SxA3YpoRJ1hnGVNlsILZNjdw3MYBiYJ/kM3xOnEMEkBTsjhFRJZFypM04xn69k38c9cdv4ffs+03Rjt1ORIX/Cxuy/cexhsO1rzG/+hRrkXx7c3OI7NwDmXmZjDHVVD6t2CrfwPvdI73jzOzbt4hbh0HGsuqo4ipyiaWqsTVX8QiU/M0+geA64kHsQOJ41lqoqgygXy3buOvryizz/PF8F3BHFBbAr/MIYQ7NoODg4YLFoQBhihDRL8dwx2kAYYWHdDvPID01xTc1iwf0HD7h//x6LxYIiEURZDjqGpDFe0jKcc4i6rmmaZrQ+DQddrsxvD/ri5/ki4U58egYY/AZJpk82lgskg8H0ms6/YH8+Poxj2Sg+lWWFNWNkrJHtDka7rj3O73In3UW6T5p1UrRz8Uny/fO7H7X4yTj5n5sMd5ziEtglPi2XexwdHbG3tz840uZ+rO2BrsaFgXtkpuVF03Dv3j2Oj4+p6mo4rigKyiJwitH5sO3ACybV5+MgxoS4q6IoaJqG5XJJVdVbzsvcLHyBEHlr4I5TXAK5MhxWS2HRNOzv77NYLAYL0JaiOTO7KGwRTi7ubOc4hH1VXXN4eMjBwT5VWQ77C2MpiyKYRqcTniHjkBf4uS1BgSsEQszNwyTn3a77TvFctxTuOMUzgIhgC0tZhsoag41/18GjVy0Tn4RtL+PsGsTch7hC13VNVVaDUi0IxmbhFlw6XNg94RbPtoYns2z4JAXfTH0w3Aqp6NpwxykugS27vQjNjFMM+64aa8462E0eifCKomBvb4979+5xdHxMXVWIgjVCVUWFtyjCABcp+TEMY3TnPzvqGmuGLrBNE65bVVVoeTbceFocsmSrWwx3RHEJ5N7eJEKl1bssyx3xQttCyi4Lfi7r5z6LdICJnKIsS5rFgqZpsEURd0u0QgWTrAx+iss5QLrmdUI+8rmmKoiJcw1WL0lcL/25IKRjFrg4fSY3k3juxKcL4CKLTFo1kxgRjx7jhMJP8tirCdnMk3R2WKAS8tV1zaJpaJomhJELSCSWwas+G+OZTKPXhDyJ6aLMvZ1wIffKf9w8uOMUl0DuS0hBelVVslwsqOvgTBusTzMmMUgVWcjP9irKMH7uCynqiqZu2N8Llq6DgwOq2IrXWkPTNOzt7VGV1RjmcVVuxOd7AhMvvbU2Www+x7VuJg1swR1RXAJbog15zrO91qo52n70Suuoqg6ik7EhKrUoC8oiJeqMyJnyvS8dbzKPdI38/i6fTzpxuMc5p5iJfVsDz5yStwXuxKfLQAdJKFhhJIRyLxYLmqYexCib10llW4bON+wsUACTPtlVVdJE7/GiWVA39aDUp8oey+WSqqyuxx0mOsfFXoStsYTBEmZtHnM1dTLuvuH54FdP86bAHae4CnRc7W2MfVpE5bcqK8qiiH3jLjp9oIbp9ixOKheBEKiqmjrqEovFgqZuQnFlwBo7OtHqaxBFtDpdR9HeBQNBWIO1WXme7LYkV6YuG/55HCavEO6I4lKYKrAiuY/ADnWcUh/KUdmeleNMo11lIcqchEM07rxHnJB1ar041GMoiJCNG75fT2wak46IoSUjl8gjcNMtz52IF8LcwXED4U58ugh0JuqIICYXnxqqagzMS9p2CnWYlOvX3QQxcaml8HORwbpUlkUwv6ZwbabZeGVVDiv3TuLQTI/JxKch3+cC5M23W2vjXMoh3MOm1snRmZ/Z6S59pLcFrk0UEmo//ZKI/N34++si8vMi8tsi8tMiUr28aX4RsB3lF7qFBoStqpIixh8NHt7oLLs0jHwY/WK53tqIfLYYx898JqO/oNjJLaZFDcb/dUIYO4h0h74zybizY9bfhYv9a0AXz8Ip/iKhikeCm1E282VC7miIn5CBFgiiLCxFGVbOEaGSWp7J3RmmDLnUMwKamGVTJGwmvgzTELDGUBQhLXREzqzAjmzFrQ4h6znojIDnIe2QFoJiuJ61FjE7nHTZHd52uG6FwK8A/yrw1+Jv4UaUzXyJkAghE+eNMdRNzd7eHnt7S5bLJcvFgqIoQldTF6oz27iahgiMxHFGD7DH43FBxNrK3gse69IWFNZQGIM1EquQKDZapxZ1TVmORdECjeWVDhlaaQ23FAktcIrYSkB9/J552InNIFGqsmSvaVg2DU1dU1dBjBoUE5nc2msB11W0/1Pg3wcO4u8HPEPZTOAnAL72ta997ol+EbAtpkvkFET5OiT9GCMZUofjgCjHx0jVSaBTkuu3V1WBIV8il9vTsSLBgRdW7qnPPA2p46bsXnaXZcjPz+837bFGKApLae2Q+bcVurFz7JfjXX8VcJ1iaH8a+EhVf/HzXEBVf1JVv6mq33zrrbc+zxA3FnKRZ7TMcLHreifMwklEMMZOrE8mE2tCmEfUKYoiJ4edi/bnQ8zEucCmWK+qClaw58zPuA1wHU7xR4E/IyJ/CmiAQ+CvckPKZn5RMFiZfBCZctl/9PrphDtM/8IWQcAkSnZSLTweF6xPwVcy5DXESxnmhTs/571pavsVxKflchmCEs24hmpGiPN7ue1wnVL8/4GqfkVV3wf+LPD3VfXf4KaUzfwCYKy6EX9f4pEaleq04YpxmQbgMRk5Fl7O/RSzMV6MGDNOMrc+TSt9MHCT+T2luc/1pYu8+TcNnsdP8ZeAf1dEfpugY7zyspkvG7Zf4sgdQi+Hnr7vQ82n7ZN3I8VgDp1iU+oNgSrW2FkU7siZRAxVHXwlVVVNSt7sKmDwucSnaBkToKkbDg8P2VsuQ/ptOiAR+2tgbZrDs1Yd/wfAP4jfb0zZzFcJqZyN9x4XETm1AM4D5SJehZ+5SXamh6RtecuwpNDmVcLTECJCVVaD83BQyC+Z82WBi6P/AyY8KbYqSh2UlsslxazaeRzhNRKcAtx5tK8BowcYVMfuqCleSYey84x/438T8WmHCDW3Ckn0nA+e6uEUHWKj5mEeE4TPfCDjpstX8wsJhuiXiZ5smRHoyC1eL7iLfboWhFVT1dN1HZtN6JDatS1t29LHfhBDWEX8jLg46hwBjwIypRgnH1d7Qyo6Vg1VAQd7kjKYfIfSnWUx8XZ77xld6zH+TqcFlC+G0WOe/hpjWDQNhwdRfMo4xUAQrxub4I5TXBuS2OOco++DLtHH/nDq/dRUOcj2M3v+BRA4QqrpZLJ0021FGgnlNFNQYm6uHdJSP6e8P6ZNjIRRlmV02tUTHWfCLV4zuCOKS2F0joXVFvq+H/pouz42TEzNIInEA1MRKf13RbxQUpS9D62Jk1iWzhcRrDFD+cq6rihjsJ6Jq/gzSzRZmEeO30mMS1wpeM/NZP9w3C110l0Ed+LThTAlCAj6xHq95vT0lPPzc9p2Q9d1+Nk5WzL8xCOtW6pp3ltCvcf1Dtf3oe91PMaIAVGKsmRvbw9rC/b3D9jb2+P8/Jyu62m7Pl5j+24mljTNxKv0d0YQyRTb1CH1Na9eIoBGfed1Iwi4I4pLYTuAjkF8GriE97GZvYDs4BKTIbbX8DkBqY4IvK0g51U1fAgtj0pw7xJpBjS/iDDGkabbUyhJOiTFSY1lQvPmk7FU5mW5HEyf3zwoMo1yE+GOKC6B+cv1fuQUq9U5XdfhvE+hqwSFWBn6Tw7/7xZoBrElq1o+j1zdCuUWM5StrOtm6FnRdVlQH+n6o7XowgU9hpYkR1w6LnGKlD9S1/XEo70rf/1ZYaxfeLPgjiiuAcly473j/HzFyckJZ2dngSj6HiksSSXORSHJ0jNzQ81AJkmHUD8liMSBBqTLPMzWDPnhy5jsVDcN6007XD+JfEPa6HjBCXWMxrEpAY7iU0Hd1EOYx7T0/zSs/drPMnsGNxXuFO0rYJD140rqnKPrOvp+9FMwQd4dYs9grr16Zc3l+XkR41EvMaM/I+VdpLkql8r6u+Y2J74h9dbI4F0fyvmQ4rw+P0HcdLjjFJfAHIG896xW55ycnHB+fkbf96ECh05NlQMhiYy93rIxh3Gjj2EIFTeGug4tvY6OjmiaJiJe8JGEDxOteGr9GRSCrTil/G86L3ne53sTURZFGWpMLZcsmgX2ipI6V8HFguTNgjtOcQHsTtf0bDYbzs7OWK3Wg9Nu8FozKpG70lEv4hRh1ZdYKrNib2+Pg/39oVtSOjd8/FQEyWLVJ0xr501tX3ec23R76phUVyGkpK5nforPCVPL9M0Uou44xTUh+SlS4J6fO+xm73dQsuOKvGuNTA67wAkC4pvoMCurKluZd8j8YjAxNbYsU/DgBRQRw0MmM5spyluhJol7ZbnZc3FpYsHafeULjr7ZcEcUzwRK23asVivatg2pphkkz3Qy42znPOfWrGm5GI3EZgvL/v4+h4eH1HU9nDvG3wlWLGqUpg6txs5XK54+PY3HTNnFqBMx4GXueBuINnoIEzEkU29VVUOl8Z0e7eHmn+e53iy4I4pngMApQmiHc25ie99aaZktzhHxBj/ezMM9pLPKWMLG2iKL7ctkfgm52EURTKZ1xlV2iU+XreKy43vOKSbtzF5DR90uuCOKC2CXHV5R+t7Rdl3Mo/BbIogkhiC5DhEs8lvXiPsGkxGaqwgjEQmoH3tjJ7NrWZQc7O+xOt+nqqpR70jn5HZgMjNwFhw4JjIJXhWJpTsTpwipr0VsUsMwh8nArxmt3BHFJTCsjNHc772n7VrW6zWbth10C/WK6Lj4J24wybNIZWE04xY6+YEYAlVFkSqJN3nDSdLhKHVd8eD+Pbx3fOe7TfBvoHhGwhiV8N0m4cQRVIMIp5FLpFZedV1TN/UQIZvq3ZpLSoXedrizPl0Cg8xNdMrFYL2BGC6yJs1+DxapbMvAJaLJaEsyGUNWt9zR6brWmiHM3Fp77cDYXYlOifATF0mVz40xQ/rr9o3KzvldcNHdnxsId5ziCsht66pKH513ru8n4Rjj8amcTU4IMv4eDlfQMUZpHrA3iEFp3ExxT5+qqjg4OGCzaaPJVHCRa81zHXbpA1OiHsdNBd+qsoolfMzweR2jYudwLaIQkW8BJ4ADelX9pojcB34aeB/4FvDjqvro5Uzzi4Xghohcou+jR7vH71rpIhVtBRNOB4uHBsKYXuuC1TMjhrRqJ6Jo246mDvFQk9qAFxBGnnQ00Yki0qeynHnHJpNxi9edKJ5FfPpfqeo3VPWb8fdfBv6eqv4w8Pfi79cCtqpp7LASzQsEXAm7XRU7lNTtkpdbQ0VENmIGZdjYWROZCae6eJyLdI2cK0yfhVzr/NsMz6NT/BihXCa8jmUzJ5DiiwTv/RDekUKrJ+VcZhGv4wjpI1ue6PBzLKa2ax2ej6kaciv29/c5ODgIdZm2nGw6iGL5NXaBZv6NuSn2TRGbElyXKBT4/4rIL0oogwnwjqp+P37/AHjnhc/uhsCAzBH5B4tTFGW2QzG2dYI0Toqoi3/C9skqfAHi6TaxFTY0cGmaUBgtiDi7456uhFGqG/SK1L1olzf7dYbrKtr/kqp+T0TeBn5WRH4936mqKiI7n7/c4lqywER2SgQxlrXJwzSijM4Oyw4T10M2XnIo5wjvx+QlTSv91DKUwNjUKTXK/iJRr2CYy2iSysy8kzkMCs4W4eVi4tQzf9Uju90EdC1Ooarfi38/Av42od7ThyLyJYD496MLzn1tasmqhhzttm3pe4dIkr0hRcfOrUhwmW9rioCJ4FJhBOf8aLkchhxL5pdlyf7ePvv7+zR1jU2e56yVsWhOGCNM4puMQSJha8wPH7z2r5m+cB24ToHlPRE5SN+B/zXwy8DfIZTLhNe5bOawWgfMDEXQ3NBCeOAUiUNkyAu7wyhg9N1trcwauiANNaXSeDsEIWMMRVlQZA3fx5wH2cmdxjmP34djE/HlxJ3dz/MQyC6T8k3lKNcRn94B/na8gQL4f6nqfyMi/xD4WyLyF4BvAz/+8qb5xYJq8BI7F0I8NpuWru9iC69ZSqUw/N5pltXoR4jH7la2M6U7faJVah7urd7Pwk1mdtjJxTNi3eW3iOKWjxwjNxrcVAR+GXAlUWgoj/kHd2z/FPjjL2NSNw28Ks47etezaTes1qsQ5uFTbkMkjIwgEsxX11SIXLJ9uUgUwsIls/gkwtixumZcxWf6x4XyWlRiJlax2XgKgwjl/ZspPt15tDO4GAFyuV9xURG+zLYzBtrljrt8xIvP20kAOxf/XMzS6XUuYBaSEcbEmTeEGU5PeGZ/zGsAd0RxDUhI57JK4867Cbro/K+OYtWWTD8cOyLlqPjawZscetrNnHEZ4Trn2Gw2rNfrEHoSlWMxFrtLhMv1A6bcKZmJ53ATq228bLgjimvA6JvwuGiZ8bG/XfK27VhnkxYw25a+BNEo9w0Ep9mY4GOsvRQlnfO0mzbUtY1EETjHJaVoVAeucBW8SXpEDm88UVxHZh6tRAxWJoVk+g/HpAMjBLEpqccXI9jUbxGIaGomZSoazY736mdVRbY5yi5n3lx82iXQ7brqdQjlohD12wJvPFFcBVNT6WjpuZCYkhkzST3MPdbj9l3XSTnadVVhrImKtO7CWdR7+q6n62LFwnjsdZXjCWGQaDo3D19rmNcO7vIprgXbvoQBY3JWkdn1kzMvwVUr5TCuhJ4QtggFjdOldq3aGq1i3rtJQTUuI9o016vuOLvHXUT9OsMdp7gG5DrFVpKREqK/o88hOMFmWJcIZi7fJwU38ywXRUFVh7zrsigw0SSLyhZhOB/6ZbRth3cujjkVkcJlZHrN6I+YIHvcvtOhCLH/9vXEzdsOd0RxDfA+9aXIe1P00ZbvQx6CSQgGolME0ohskukkCBhGL3Reu3Uvhm7UdR2z3wSvia5Gwur7ntVqFera9rHi+BXWotzbPTf9JpEpxV5dll34OsMdUcxgt8c3D9Ybo2TnK/cY47pDcVXQvIXwhGFME4iGpi02C9neERahMYw9hbI/C+Re9zTvubHgeX0Ut1XcuiOKa0DfOzbrNoR3pKw77zFisCaUocHn5lWiTJVjfvgveacTpCC8ULRGsUYoY+KQzToZzclMUTZdy5OnJzx+8pT1en3lyj6IS5FjSfweJ4IObcpCXwzzRnop7ojiQhhDwaHvetbrDev1hrYNhOG9YsSCNWO7XwgrutlhXWIargEhfMRr6ISEKgawYqjKgqosKaKfYo6YKYN1vWl5/PgJnz16zPn5KhLF1RwjjZlXIEEV9WN1EkPowcctXe2fB954opiv2lswhHdE0Sm3QF0Ak12X4FTu1RhEJJP5KWbnishkvMvEp6tEl3k4x/TcafSsTPa9/kTyxhPFZTAN7wgtt1xEwmRQuuTsC8cDxhVYgmXHlCWoDiVrQtma8fVME5aCJap3jtV6zWq1ioq2piG3PdkzSEg/zwFJWXdVFfrpTXtSvBlwRxRXQJL5nXOROFITeCDzOM/PuWxBHXwAJqzDYgRD6HRaFmXIkYhVNNJACYEHbqGhhGcb2xeHkjthTvNzpvPKiOuCSQ4txIritS56dhHcEcUOmCN50hnypvI58qW/uaI9fL/0Qum/QBijaTYrGJAdviuVNEXs7iy3M7uXnEimZthtcXAU1V4MUVzEtW6iOHZHFFeAqtL1fSiVuV7TtsH6JGKwdiQMTc6v3E92xbgkx5gRjC2x1lBWoVHKYrGgLDPxafZNAe+S864dnXeza+RJQgkx82p/g2My/h2uMpiJr/+sXhd444niKudUcmYNDrvIKcSEGJkx6nX4bxIbe1nW2nBtzbhE8lPYYoK8YfRpTShVxTsXRLorOMWlGXcZp5hwlowULzv/dYO72KcrQem60JNivVrTdf3ovLvMAsXlIUaDWTauznnI+NiIZbeSO40UyXKst47bRvRd+/PfiWP0Li0Cz5ebfRvhWkQhIsci8jMi8usi8msi8i+KyH0R+VkR+a34997LnuzLhote/maz4eTkKSenp7TtZkwyGo4fkW9X0OBcZp9bfoChQUpd1SzqmqauKQqbDx/OjRuGldvIUOtpl56wS2fY9T0/zvWOru3o2g7vt8Wy1x2uyyn+KvDfqOrvJ+Rr/xqvcdnMBMNq7hxdl/kD5gg/Oy8g2vYqfbmJ1EwqfZvY+TSeGQ9iCPhL8VNbqaty9bWumldKcx0ib2fnXeWn2X1/t6eax3VK3BwB/0vgrwOoaquqj3lNy2buetlt23J2dsZ57J0dPNF5facdoopeL3ZIVTHGsFg07O3tBQU7FjcLoeMxryGTx6Ihl8IWNHWoEGjLApXxMCV6vo2EOlBGSCG3SizGoEEXST0t0jFelc71dH0fijN8DiK4zXAdTvF14GPg/yEivyQif01C/ac3omymqtK2Laenp5yfn9N1XYYkOvxLxw7nMQb/XaTIJjBGQmvevT2apsmqfQ/C0oS8Uj3aoWzmoqEoigHhJ8eLjIQhggoDESSCGOhNQoVBp56uT0ThBq/+m0IY1yGKAvhDwP9dVX8UOGMmKmkuL8xARH5CRH5BRH7h448/ft75fgEw1o8NpsvZ7mfEk12yP4w9IQpbRKRPslIirpEDpXGmTjYzEh6RKC8RU+a6RBozF78SYb8pxJDgOkTxXeC7qvrz8ffPEIjktS+bmez0uUn2OgF3cHleQ2556roOUJbLPY6OjljuLbE2U7BjPIl6nTjpRISqrodG9HVdRwTfjoHa5fTL8yVyopgUU86I8U0ijCuJQlU/AL4jIv+zuOmPA7/Ka142cx4o6H2o4PGikCNZnxL3qeuK5XJJXYXmKzD1dUwccfH8oihYLBYsFosgPpGFkFxwP/k4u0LNtwnomZnhrYfrOu/+j8DfEJEK+F3gf08gqDeibKaIDCXpkzi1yxGxLabsKnyT7Y3IaYxQ1w3L5ZKqrjKr0/bxs9ynDMkZPORza9E88O8iR9zOXOxrGoh2LRY31bp0FVyLKFT1HwPf3LHrjSibGQoZl9hi7CU9qZ60kzhiGk8ynWb7ciT13mOM5ejoiLfeeoujwyOM3XbaTa1aMSZQo0c75lEM0s5Mqb/IXzHOdSpmTcymb2Ca0Rsf5nEdGFJFJctxyCxL82PjNxLpXIZYaSWvqmqwPF13hdVoQdousDwdP/0dxK+ZnrErUHCeu/G8cJs4yR1RXAAJeQAkWnlM1kvaGI1K76zqxaCoTrdrNm6OgCl34eBgn+PjY5bLZSiXSSSpwXNtJkGr3nvazYanT5/y5MkT1utN5BBZ9C67rUxk80yOwsn9Mv37siC/5k2CO6K4BBIS2UgUeYN1Fz8YM0G4UYYniDSqeB3xNIlMMOYt1HXN0dExDx484ODwYKf4ZCT6Ghgdh6v1ms8++5RPP/2U1Wq1lfI6F5vSHFMvu9zapKpjSi0ZXd08nH3pcBcQeB0QtkSOy2wyg9tmxyFzJ15CzDIGAgY/xa45TLFUiabiIfzETfZftMpfGF4hu78Lbx5d3HGKSyDgjmZoOPMr5yty3OpVRySKfa0lnakavc6pActYAG3RLDhY7tNU9VSuN2YLM1NUrHN+TEft+nF/nKqoYMWE0jqZtj9k9EUPd7o7BvEpfmRWmfwNgTuiuAAGghgIY0oUSUxJPyaKbsRBM0QpAQSRSRV8wMAhrKKwlqZuWDZL6rKemGRlng4aFQshpKOGUvybTPQRUhCUINHnoWP80wwmzExijzxhIObkE3kZRHFTCe2OKK4BaTUviliL6ZLQCRGZWJwusTulxToo3MYOKahETrAlfUmusIfKhaGaRxfNsoxpSDqeMxGpZDaugkrm5xgDoeLxiXgvR+Dn3X+T4I4orgARoa5rDg4OODk9pW7qQVFNMFW0J8afZJklR/NRnzBYO+ZSNE1DWZWTuq3p2DwLL1Qp1CF69/TsjHazCceKDPslo6yBs2UK/zBhBfWK62OIiEYiNfbOT3EHu1e0ZCGq63piuQm+uSuCIHas2AxEAQx6hY2cyE7msHP86Ml2ztG2Le0mE58yM/H0nqZ+h9w3MVinfH6OiZ83jyzuiOIKSJxif3+fvb096jo42JI+gMgYaxTl8UgtsZhB3JTFeyjJRxFyskOD+FDrKeRRXIKGWTBf3zv6rqMbiqGF85zzoH1mctUQPj7nYgP3ml4v+U8G4n/D4I4orgARYW9vj7fffotN23JwcEDTNHgFpzpYmyRZoLwOcruYhIWTaCQSEpZlRV2VLBZLlsu9EBBYV1veZhj1Fe9DEbS+72m7ltV6zXq9Ds3uCT6TruvoCSJXWYZkJYzBeKIctfNOh1BxEwm1KMut4glvArx5d/yMkKJR67qhrmvKWKjMWrOFXOnnJNkoM9cmA9aoJ4QVuSgsNopP8wJo26ATcSf10c7pbmf+x8SJN7GjTeaW3/fEwvYGwR2nuACytZq6bjg4UA4PDjg8POTw8DD00T49w2vsC7EjNHvcnvsDQlFlUSjLksViQdMsqKs6lMrcUQBNMn0giDaCErhAVVVUZYW1LQmrjbFDL+5k0VJCP4s4yGjajcgfyoKGLLt203J+dsZquaTP/B9vCtwRxYUwmjGbqEccHR1xfHTM8fExp2dnnJ2twAfxyYgMXuYUxjFRmGOORGjc6DAm9Lbb20tiUyCKqeFquyCBiGCsoRAoy4KmCpU/Ts9WA1ewNliOkgIvIvTO0fX9YKFKXnprg0KdauWqVzbrNWdnZyyXS7q+u9qY8JrBHVFkMM04G/+KjOUsk+m0bbtowfFBoc4V7nT6rsjTTEwZaz3ZWZVxjTr7thilyctM1hZMxiDEcfWPLYlzC9OORjPqFYxuiU6pffGbKD7d6RQ7YChHEOV1kbD6VnXFW2+9xde+9jXeeush1gp936HeD+KKyZx7u8K1gyUq9KJYNAuOj4442D+ICnG4+mWttYSY8GSDJ3wonBZ1HBEN5TfLUPig73vatqXvOrzr8b2LOkggEuc8LlqvktVpb2+Phw8f8uD+A5qmeeMI445TXAQTvSAgelmWHB2HZKD1eo0YwXkXwroj0ifkuihkO4RgBI93XdXsLfdYLJqguA+XjueyO7Q6OPcMJnrB7VD5I4hGaZV3ToemlRMiEwAzcBKFCWE3dc3h4SEHhwdUVfXin+0NhzeeKHY5yuZBfjF8LyJMw/7+PovFAmtMlsIQ45yykIhdmW6h0FjYXhQFTdNQVdXU9DkTxXTwf+yCII8lkScdn4hgSJDKqn2ExpXTpjCBd4XU2OVyycMHD3hw/wGLpnmm5/k6wJVEIaFgwU9nm34Q+D8D/3nc/j7wLeDHVfXRi5/iFwWCpiC+KN0XRcHDBw84P19xdnIanHhxrxlEphD+kTzOc2uUd46+7/A+WJ7u37/P0eFRHIsxIHbQbxLXyKcWiS5aslT9YDoOBOFo2zFnw1o74RTGGsTaKVF4j7ogJn75S+/yjT/4z3P/3j0ePnzwxolP16nm8Ruq+g1V/QbwLwDnwN/mNSybOQn024EHRgxN03Cwv89i0UxW91SrKYkgFyGS6pgklMJH5pximIck/YYd0YFpY0x5lTGb7iJOMf1Mt9ns797ekrcePuThwwcsmsWFz+uivO/bDs8qPv1x4HdU9dsi8mPAvxy3/xTwD4C/9OKm9sXDxGq0tc9Q2ILChsjZoc2oRGU40zPmOkbSKZbLBffu3ePg4ABr7WAZyn0kg0Mwj1uKf8uiYLlYhhTWpyc4F0yuRdQ10txzppMccrkJWfE0Vc3h3jFNU/Puu+/wzjtvc3h4yGJxMVG8rvCsRPFngf8ifn+ty2bmMvo87CLZ98sidDEN6aM6xDSZqHBPqvbFFTVxCTHC4cEhb7/9Nvfv36csy7CdjBTy67KteFdlxcHhAav1io8+/ZTetYBQlyVVHG/KMSJRxDKaqh7ne7xzNIcHfPVrX+Ho4JAf/MEf5P2vv8/+3t6QgvsmwbVNshJqPv0Z4P8936dD/uXO825t2cxdeRN5qETKnJszEmGbywym2XiEiAxdi6p6rmgnT/Yl+jVjjFIZY5TCNYJ+ITPxKBB5IIZQw0rGEHV0sDotlgsWTUOTxDpr51GErz08C6f43wD/SFU/jL8/FJEvqer35YqymcBPAnzzm9+8dcJnLi875zg/P+fp06esViskeqVTP+xgEjUpPWerjVbSIwpr2N/f5/79B7z7pXc52NunrkMaqlz4hHRMAoqm1KquuHfvHl3fj1YiTTxLKcqCptkLirYbY6EktiXrXY93PepcSIldLtnb26OqyuF6A7xBhPEsRPHnGEUnGMtm/hVew7KZu8B5x2q14uTkhPV6NVh8+hQ3xGi+TU42zGgiBWJB5OAgu3//Pu++8y5NzNUIXmp28tyRF487q6rm+OiYru9pmmYkmnhMWRYcRl+D60OouaqGTDsJSUqb1YqubSltwXKxYLlchtpTQeOAK/nV6wfXIgoJpff/BPB/yDb/FW5E2cxdS+v2S/x8FpKkD8TG8snESljxDw4OAmK1LevNZijE7PqxwH3SBYjnWVNSx05FdVUNZfcvNHteQCQAVVkOc1guAjJ758Mp6hExVFVN09SgMi78BjBC17YYhPViwdHRUQxObIYGlHmmxVU1mq56vrfJrHvdsplnwIPZtk/5wstmzoKUrljRVBn0gRChve24S9/HtAOl61ratmW9WeNVMbbg3oMHfONHv8Fqtebxkyd88umnbDYbfu/73+fjj4MkaWO0KurxrkNR9pZHvPv22zx8+BYP7t/j6GB/UlMqKOuzic8IIyniDx884J//A3+Ax4+f8N3v/h6/8eu/GaJ3nafvHIUteHD/IYeHhyyXSw4ODodwlbIqcX3P06dP2azXHOzv8c7bD1k0DYeHh4gdC7IFR/mzIfU87us2wWvm0d7OIpvv9z6t/rmpUid/J0NlaZ99FxDbWMPe3h5fbb6GV89HH32MKUrOz8/59LPP6F0ItzYiIDYQhXeAUpUlR4eHHB8dsr+3ZLHIYosu4QpzEGB/b5+vvPcVDg8OefjgIfvLPaxYzlbn9L3HSNBdjo6OuXfvHm+99RZlWbJcLmmaBud6Tk9PaTcbytKyqCvKMlQynwZHJsPC9RB8e4G5XYTxmhHFaOm5aH8Kupu/p/k53jucD+14Hz1+zKNHjzlfrTg/X4UgOu9ouw7nHb0bm0I2i4ajo2Occ/RtG30HRAuRsFgshnDx5IGeR+fqZUg4EFAI3qvrmmaxYH9/yeHRIUVZ4NSFnA2BzWbN+fkZNiYylWVJ71z0hDNYwQTC/TjPpu1oux7EUBg7ROFeF8Fzv8xFzzc/9qbBLSeKMVx6DvPoVCBGu24rjuOxo7Nr3fWcn6/YtC3/9Fv/P/7pt75F1/dsNh1d7+j6jvPVir7vWa03g1/i3vF9yrJivVrxe9/7HudnZ1RVxXK5R1mW3Lt3n7ffeYf79+7RNM1knnEyF8xdtuSqsqo4tCFj75133+arP/AeJycnYBTFI6I8efqYttvw9PQpn372GWVZ8s4773D/wQPKsmR/b0nTLGMH2FNQz9OTM05XaxoPy7qmqcbeFyngcZjTRW/mAtH0JhLBHG45UcBV3OFZ2HdYOcMq7ZwLCvR6zdPTUz797FFEiGC373tHF4sGpK6pqkpVVezpHqLEgMEwr9Qfu6ormrqmaZrgxY4EYMQMJH5d8SP5IMqqolk0LPcWON9TVSVlaREj9F3Hxhh65+l6Fwjh4IDl3l4wICyXwZEHoQCCc2y6jrbtMMbSxJis/Bk/j0h0G8Sp14AoLoZdD3+6ao2hDwB93/H06Sld1/G973/Ab/7O73J+fs6HH33Mxx9/EhNyggLadh2nZ2eBU6xWnJ2doaohNuog5Ee89fZb1NExl0rjlEXwgI9tgcMnn9SulThZvSbZeDDkRtRVxeHhAdYazs/PQy55WVI3NdbawNmersK1RFitV5RlycFnn9HUdeCCqzWqHoxwcnrKcrnk9//QP8PX3ntvcBTm4SMv6p3cNHjtiOIyq0fuL8h3JzGl6zo++eRjTs9O+R9/+Vf4+//d/8DTkxNUM5FLwmreti1np2cDp2jblsJavvKVr3B8fEzXLUGVe8fHdF3Her1GVSmrcgi+myQlMQw/TC6/h9TXOoSRBAdhHsbRLBru3wuxS6AcHu7jvdL1IZ/ifLWK3E45PT/jo48/ooixU2VVTeK0fu+DD/gff/mXOTw4wKjn+OAgtgs4GNqIfR64DQQBryFRzCEPs7gI0grsXM/56pzT01NOTk548vQpT09OsLbA2iRXhzParmPTtrFsZT+0Eg5EF8ImlosFhTV0XUcZler9qGQvFguKOOaWBXangs2FlikRoa4q9g/2sYWl61qsMWzajtPTsyHJyMW+2G3bImLo+1Abquv7IcQ8zxgE5fx8Rdu2kziu54WbThyvPVHkkAf5JQtUyF32OPU8PXnKb/zGb/DBBx/wu9/+Dk+ePmG1Wg91Xr0qXdfHPIngIFMF13V0mw2+6zk7OeHJomHRNLz//g9wsL8XAgijqPSlL73LV997j8Viydtvvx2delcjSpr7QOQpotYaiqrk61//Onv7CzZty2effcbZ2Tnf/+AD/tEv/RInp6e0m3WwhnlPKyZk2hlD33Uhv0KmqbQS/SuffvoJH3zwwZBYNc/Eu45D76YTwRzeKKKA7Rckojgf/Ainp6d8+9vf5lvf+hYffvIZZ6enbNpuqJbnnWe13tDFbVUUO1zf029avDGszs84PampyoL3vvQuX/nKV6jKkr1ogn3r4Vu88847Qz+K61pm5hl9YWNw5BVFwXtf/jJf/vK7dF3Ho0ePOD0/49d//Tf4J//kl2k3G7quw8VCzJ2Jhc9EaLs2I4TQ6HLgGsDjx4/59NNP6fued955Zye3uEp3u21w64niqod/+f5YGMxPxR5rLU0TSmXWWd2jQDyKIUShWhOQqSpLzBIKa3lw/z5fevcd7h3f4/79+xwfH1EVJYtFQ2ELFouGMqtHe1E67HVBhmCpEG9lTSDWhfPsLUPLsLOzc1BYna/onYtNLcuxwWWMhE3+iBBla1guA2ewef3cC+Z6kQl251O/4SbaW00Uz7pq7Q4D9/QulLMH2Nvb4/j4mHqxZP/wmN451us1m82Gvnecn52xaduQVtoFU2y9twxh14sF/8If+lF+5J/9Z9k/2Of9H3ife/fuYYxQ2FT8oKaKjVnyTLm5GTbNd3eC03YciPpo7sSwtzxk0Rzw1fc2fON//g3ee/c9vvXtb2PEstlsgvXLWoqy4ODgkKquh2qFg0ipymKx4OGD+xzEOrqpguF1su12Oft2+V9uItxqorgOXGXzD5xiLGBWliHHwZYVVbPEec/p6Sln0fwqKEVh6buOtSrOwaKpOTw4YH9/n/e+/GV+6J/5QZbLJe+++y4HB/uzGUn2iVt2eICvgvk5SjICCFVZIWI4PDji3XfepSorzs7O+b3f+z3KohgKF5RlydHhAYvlckIUSTFfNA3L5SKWCx3NsfPne9H8L0L8m+6reO2JYgt5hu/jamysxaqyXC758pe/TNM0bNqO800IrluvV6zXa5xzrFZrurYdImLxysHBAfeOj1ksFnzta1/l6OhoKNufXyub1ZXzvApp5kgYSmua8TtQ1zVvv/U2y8VyQPRNG/pYeNUgJi4byqKciIPGGIy1NHXNu2+/w/HxMU3TTMyxF3m1t+a1pcPdXGJI8NoTBWxnv6mGUi6JjZdFhTWWe8f3+ZEf+efYbNacnZ3z+MnTkCsRayep94PX15iQo22M4fj4mIcPHlJVJYeHh+zt7cX9ZkiEGJXjlOK6PcfPgzAjEgpGLDnB7e/v80M/9EM47/jqV7/K7/t9v4+u62i7lr4Llqiu7+idi/6RsIIvFgv2lsF/8e7bX+LB/QcYYweiyOc596XMRb+cK9wGgoA3hChyGEMV8tg6wRhLWVYc7O/TNDXWFrjYPks1hE8rGqrpqWKNiYWNbSCKhw+H1NByCI3wSabJJsAWo7jKl3IdJXbSWiV+LWxBsSzGUpkCzvVsNhvaNjR6Wa1XdH0fJxaO29vbY39/n7Ks2I89OS6b07PCTSeON44o8uDAhD2pWWJRFDSLJZVzFEVFXTdD4bK02gfPsmLFDMlBi0WQu5PlKsGIv3kwn/Csodhz2EUYKe8hv8ecTqqq4vDwEO9DzJZzwUjQ9SEyNj0TkdA3o6qqaHauhzGS3pWucZFodNOR/iq41URxVYjyRS8tHDMqu8msaa1hYQtA2dtT9PjeJAgu/U0iQe5M222aZLjGWL1v9IrvmmN+rcsU1XwOEPrgaWwYY4YQ+RFSfamQUxIKqF00Vg7GjMXd8sogu5Tui0TA2+bEu9VEsQ3Xs+CEl7OLYObEMh03Ze1BblYcV/5stHBOsG9myLA7VuOqCN9rWaYmtxPuY15hPM034L5MiDNsT4vGuP16JuHd+2+rA+9WE8W4eie2fv3WtuOKfdmxw7ds/Hzfblk/7ZuKMVOudJX3ete4+f5txFMkVaOR8Hvcr9nc5/cyhr0EUZEJ18gJJIW6z6+fcixuuv/hunCriSLAqCAGuOilbK/Sg0HoglNyZTwcd5E4M73ORF/YccrzIs7cmha2za41OOHSgrFNGHNRb9psRrLzxuvudoCGzy6Ran6d2wCvAVFsw7ZcP+xJW59ltEuvcZmIEPDvgvOz7y9SzLgOAj4Pjj5LOMdtBXmVNyYiHwNnwCev7KKvFh7yet7b63hfP6Cqb+3a8UqJAkBEfkFVv/lKL/qK4HW9t9f1vi6Cu/Zed3AHM7gjiju4gxl8EUTxk1/ANV8VvK739rre10545TrFHdzBTYc78ekO7mAGr5QoRORPishviMhvi8it7ZEnIl8VkZ8TkV8VkV8Rkb8Yt98XkZ8Vkd+Kf+990XP9PCAiVkR+SUT+bvz9dRH5+fjeflpCA5/XFl4ZUYiIBf5vhOYvPwL8ORH5kVd1/RcMPfDvqeqPAH8E+LfivbwuzTH/IvBr2e//CPhPVPWHgEfAX/hCZvWK4FVyij8M/Laq/q6qtsDfBH7sFV7/hYGqfl9V/1H8fkJAoPcI9/NT8bCfAv61L2SCzwEi8hXgXwX+WvwtwB8DfiYecivv61ngVRLFe8B3st/fjdtuNYjI+8CPAj/P69Ec8z8F/n0gBU09AB6raipr8lq8t8vgTtF+DhCRfeC/BP4dVX2a79MxPPXWgIj8aeAjVf3FL3ouXyS8yoDA7wFfzX5/JW67lSAiJYEg/oaq/ldx87WaY95g+KPAnxGRPwU0wCHwV4FjESkit7jV7+068Co5xT8EfjhaMipCT+6/8wqv/8Igytl/Hfg1Vf2Ps12pOSbcwuaYqvofqOpXVPV9wvv5+6r6bwA/B/zr8bBbd1/PCq+MKOIq828D/y1BMf1bqvorr+r6Lxj+KPDngT8mIv84fv4UoTnmnxCR3wL+lfj7dYC/BPy7IvLbBB3jr3/B83mpcOfRvoM7mMGdon0HdzCDO6K4gzuYwR1R3MEdzOCOKO7gDmZwRxR3cAczuCOKO7iDGdwRxR3cwQzuiOIO7mAG/39FK8HiogVDTAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Show an image of the dataset\n",
        "image = cv2.imread(\"fashion-dataset-small/images/18000.jpg\")\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "#image = cv2.cvtColor(image, cv2.COLOR_BGR5652GRAY)\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Structure du dataset\n",
        "\n",
        "Le dataset est composé de 10 caractéristiques (id, gender, masterCategory, subCategory, articleType, baseColour, season, year, usage, productDisplayName).\n",
        "\n",
        "Afin d'effectuer la reconnaissance de vêtements, nous avons dû dans un premier temps sélectionner la caractéristique qu'on allait utiliser plus tard pour entrainer le modèle. Dans notre cas, nous avons choisi d'utiliser la caractéristique \"subCategory\" car elle disposait d'assez de classes avec au moins 500+ données labélisées par classe.\n",
        "\n",
        "Ensuite, nous avons conservé que les classes qui correspondait à des vêtements / accessoires qui disposait de 500+ données labélisées, pour arriver à un total de 30848 données labélisées."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Dataset size'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "30848"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "array(['Shirts', 'Jeans', 'Track Pants', 'Tshirts', 'Casual Shoes',\n",
              "       'Flip Flops', 'Handbags', 'Tops', 'Sandals', 'Sweatshirts',\n",
              "       'Formal Shoes', 'Flats', 'Kurtas', 'Waistcoat', 'Sports Shoes',\n",
              "       'Shorts', 'Heels', 'Laptop Bag', 'Rain Jacket', 'Dresses',\n",
              "       'Skirts', 'Blazers', 'Clutches', 'Shrug', 'Backpacks', 'Trousers',\n",
              "       'Dupatta', 'Capris', 'Tunics', 'Jackets', 'Duffel Bag',\n",
              "       'Sports Sandals', 'Sweaters', 'Trolley Bag', 'Tracksuits',\n",
              "       'Swimwear', 'Leggings', 'Kurtis', 'Mobile Pouch', 'Messenger Bag',\n",
              "       'Jumpsuit', 'Suspenders', 'Salwar and Dupatta', 'Patiala',\n",
              "       'Stockings', 'Tights', 'Churidar', 'Tablet Sleeve',\n",
              "       'Nehru Jackets', 'Salwar', 'Jeggings', 'Wallets', 'Rompers',\n",
              "       'Waist Pouch', 'Rucksacks', 'Lehenga Choli', 'Belts',\n",
              "       'Rain Trousers', 'Suits', 'Travel Accessory'], dtype=object)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Tshirts               7066\n",
              "Shirts                3217\n",
              "Casual Shoes          2845\n",
              "Sports Shoes          2036\n",
              "Kurtas                1844\n",
              "Tops                  1762\n",
              "Handbags              1757\n",
              "Heels                 1323\n",
              "Flip Flops             914\n",
              "Sandals                897\n",
              "Backpacks              722\n",
              "Formal Shoes           637\n",
              "Jeans                  609\n",
              "Shorts                 545\n",
              "Trousers               530\n",
              "Flats                  500\n",
              "Dresses                464\n",
              "Track Pants            304\n",
              "Clutches               289\n",
              "Sweatshirts            285\n",
              "Sweaters               277\n",
              "Jackets                258\n",
              "Kurtis                 234\n",
              "Tunics                 229\n",
              "Leggings               177\n",
              "Capris                 175\n",
              "Skirts                 128\n",
              "Dupatta                116\n",
              "Duffel Bag              88\n",
              "Laptop Bag              81\n",
              "Sports Sandals          67\n",
              "Mobile Pouch            47\n",
              "Suspenders              40\n",
              "Patiala                 38\n",
              "Messenger Bag           34\n",
              "Jeggings                34\n",
              "Stockings               32\n",
              "Salwar                  32\n",
              "Churidar                30\n",
              "Tracksuits              29\n",
              "Rain Jacket             18\n",
              "Waist Pouch             17\n",
              "Jumpsuit                16\n",
              "Waistcoat               15\n",
              "Swimwear                13\n",
              "Rompers                 12\n",
              "Rucksacks               11\n",
              "Tights                   9\n",
              "Blazers                  8\n",
              "Salwar and Dupatta       7\n",
              "Shrug                    6\n",
              "Nehru Jackets            5\n",
              "Lehenga Choli            4\n",
              "Tablet Sleeve            3\n",
              "Trolley Bag              3\n",
              "Belts                    3\n",
              "Wallets                  2\n",
              "Rain Trousers            2\n",
              "Suits                    1\n",
              "Travel Accessory         1\n",
              "Name: articleType, dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>masterCategory</th>\n",
              "      <th>subCategory</th>\n",
              "      <th>articleType</th>\n",
              "      <th>baseColour</th>\n",
              "      <th>season</th>\n",
              "      <th>year</th>\n",
              "      <th>usage</th>\n",
              "      <th>productDisplayName</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>44031</th>\n",
              "      <td>7124</td>\n",
              "      <td>Unisex</td>\n",
              "      <td>Accessories</td>\n",
              "      <td>Bags</td>\n",
              "      <td>Travel Accessory</td>\n",
              "      <td>Black</td>\n",
              "      <td>Winter</td>\n",
              "      <td>2015.0</td>\n",
              "      <td>Casual</td>\n",
              "      <td>Wildcraft Unisex Black Waist Pouch</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  gender masterCategory subCategory       articleType baseColour  \\\n",
              "44031  7124  Unisex    Accessories        Bags  Travel Accessory      Black   \n",
              "\n",
              "       season    year   usage                  productDisplayName  \n",
              "44031  Winter  2015.0  Casual  Wildcraft Unisex Black Waist Pouch  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Drop subCategory that is not Topwear, Bottomwear, Shoes, Sandal, Bags, Flip Flops, Dress\n",
        "df = df[df.subCategory.isin([\"Topwear\", \"Bottomwear\", \"Shoes\", \"Sandal\", \"Bags\", \"Flip Flops\", \"Dress\"])]\n",
        "\n",
        "# Show dataset size with Topwear, Bottomwear, Shoes, Sandal, Bags, Flip Flops, Dress\n",
        "display(\"Dataset size\", df.shape[0])\n",
        "\n",
        "# Show number of articleType\n",
        "display(df['articleType'].unique())\n",
        "\n",
        "df_2 = df['articleType'].value_counts()\n",
        "display(df['articleType'].value_counts())\n",
        "display(df_2[df_2 >= 500].shape[0])\n",
        "\n",
        "\n",
        "display(df.loc[df['articleType'] == \"Travel Accessory\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'articleTypeCode and the associated articleType'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>articleTypeCode</th>\n",
              "      <th>articleType</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>0</td>\n",
              "      <td>Backpacks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13713</th>\n",
              "      <td>1</td>\n",
              "      <td>Belts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>2</td>\n",
              "      <td>Blazers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214</th>\n",
              "      <td>3</td>\n",
              "      <td>Capris</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4</td>\n",
              "      <td>Casual Shoes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2897</th>\n",
              "      <td>5</td>\n",
              "      <td>Churidar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>6</td>\n",
              "      <td>Clutches</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>7</td>\n",
              "      <td>Dresses</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>318</th>\n",
              "      <td>8</td>\n",
              "      <td>Duffel Bag</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211</th>\n",
              "      <td>9</td>\n",
              "      <td>Dupatta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>10</td>\n",
              "      <td>Flats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>11</td>\n",
              "      <td>Flip Flops</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>12</td>\n",
              "      <td>Formal Shoes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>Handbags</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>14</td>\n",
              "      <td>Heels</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>257</th>\n",
              "      <td>15</td>\n",
              "      <td>Jackets</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16</td>\n",
              "      <td>Jeans</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5950</th>\n",
              "      <td>17</td>\n",
              "      <td>Jeggings</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1169</th>\n",
              "      <td>18</td>\n",
              "      <td>Jumpsuit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>19</td>\n",
              "      <td>Kurtas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>20</td>\n",
              "      <td>Kurtis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>21</td>\n",
              "      <td>Laptop Bag</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>631</th>\n",
              "      <td>22</td>\n",
              "      <td>Leggings</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10399</th>\n",
              "      <td>23</td>\n",
              "      <td>Lehenga Choli</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>884</th>\n",
              "      <td>24</td>\n",
              "      <td>Messenger Bag</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>809</th>\n",
              "      <td>25</td>\n",
              "      <td>Mobile Pouch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5720</th>\n",
              "      <td>26</td>\n",
              "      <td>Nehru Jackets</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1707</th>\n",
              "      <td>27</td>\n",
              "      <td>Patiala</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>28</td>\n",
              "      <td>Rain Jacket</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33825</th>\n",
              "      <td>29</td>\n",
              "      <td>Rain Trousers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6205</th>\n",
              "      <td>30</td>\n",
              "      <td>Rompers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9148</th>\n",
              "      <td>31</td>\n",
              "      <td>Rucksacks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5769</th>\n",
              "      <td>32</td>\n",
              "      <td>Salwar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1683</th>\n",
              "      <td>33</td>\n",
              "      <td>Salwar and Dupatta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>34</td>\n",
              "      <td>Sandals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>35</td>\n",
              "      <td>Shirts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>36</td>\n",
              "      <td>Shorts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>37</td>\n",
              "      <td>Shrug</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>38</td>\n",
              "      <td>Skirts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>324</th>\n",
              "      <td>39</td>\n",
              "      <td>Sports Sandals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>40</td>\n",
              "      <td>Sports Shoes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1739</th>\n",
              "      <td>41</td>\n",
              "      <td>Stockings</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40000</th>\n",
              "      <td>42</td>\n",
              "      <td>Suits</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1421</th>\n",
              "      <td>43</td>\n",
              "      <td>Suspenders</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>44</td>\n",
              "      <td>Sweaters</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>45</td>\n",
              "      <td>Sweatshirts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>46</td>\n",
              "      <td>Swimwear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000</th>\n",
              "      <td>47</td>\n",
              "      <td>Tablet Sleeve</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2731</th>\n",
              "      <td>48</td>\n",
              "      <td>Tights</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>49</td>\n",
              "      <td>Tops</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>50</td>\n",
              "      <td>Track Pants</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>51</td>\n",
              "      <td>Tracksuits</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44031</th>\n",
              "      <td>52</td>\n",
              "      <td>Travel Accessory</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400</th>\n",
              "      <td>53</td>\n",
              "      <td>Trolley Bag</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>54</td>\n",
              "      <td>Trousers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>55</td>\n",
              "      <td>Tshirts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>56</td>\n",
              "      <td>Tunics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7460</th>\n",
              "      <td>57</td>\n",
              "      <td>Waist Pouch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>58</td>\n",
              "      <td>Waistcoat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6191</th>\n",
              "      <td>59</td>\n",
              "      <td>Wallets</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       articleTypeCode         articleType\n",
              "145                  0           Backpacks\n",
              "13713                1               Belts\n",
              "117                  2             Blazers\n",
              "214                  3              Capris\n",
              "10                   4        Casual Shoes\n",
              "2897                 5            Churidar\n",
              "135                  6            Clutches\n",
              "94                   7             Dresses\n",
              "318                  8          Duffel Bag\n",
              "211                  9             Dupatta\n",
              "34                  10               Flats\n",
              "12                  11          Flip Flops\n",
              "25                  12        Formal Shoes\n",
              "13                  13            Handbags\n",
              "76                  14               Heels\n",
              "257                 15             Jackets\n",
              "1                   16               Jeans\n",
              "5950                17            Jeggings\n",
              "1169                18            Jumpsuit\n",
              "35                  19              Kurtas\n",
              "765                 20              Kurtis\n",
              "88                  21          Laptop Bag\n",
              "631                 22            Leggings\n",
              "10399               23       Lehenga Choli\n",
              "884                 24       Messenger Bag\n",
              "809                 25        Mobile Pouch\n",
              "5720                26       Nehru Jackets\n",
              "1707                27             Patiala\n",
              "92                  28         Rain Jacket\n",
              "33825               29       Rain Trousers\n",
              "6205                30             Rompers\n",
              "9148                31           Rucksacks\n",
              "5769                32              Salwar\n",
              "1683                33  Salwar and Dupatta\n",
              "21                  34             Sandals\n",
              "0                   35              Shirts\n",
              "46                  36              Shorts\n",
              "140                 37               Shrug\n",
              "100                 38              Skirts\n",
              "324                 39      Sports Sandals\n",
              "45                  40        Sports Shoes\n",
              "1739                41           Stockings\n",
              "40000               42               Suits\n",
              "1421                43          Suspenders\n",
              "341                 44            Sweaters\n",
              "23                  45         Sweatshirts\n",
              "438                 46            Swimwear\n",
              "5000                47       Tablet Sleeve\n",
              "2731                48              Tights\n",
              "17                  49                Tops\n",
              "3                   50         Track Pants\n",
              "423                 51          Tracksuits\n",
              "44031               52    Travel Accessory\n",
              "400                 53         Trolley Bag\n",
              "166                 54            Trousers\n",
              "4                   55             Tshirts\n",
              "254                 56              Tunics\n",
              "7460                57         Waist Pouch\n",
              "44                  58           Waistcoat\n",
              "6191                59             Wallets"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## Data preprocessing\n",
        "\n",
        "# For each articleType assign a number\n",
        "df['articleTypeCode'] = df['articleType'].astype('category')\n",
        "df['articleTypeCode'] = df['articleTypeCode'].cat.codes\n",
        "\n",
        "# Get the articleTypeCode and the associated articleType ordered by articleTypeCode\n",
        "article_type_dict = df[['articleTypeCode', 'articleType']].drop_duplicates().sort_values('articleTypeCode')\n",
        "display(\"articleTypeCode and the associated articleType\", article_type_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Instanciation du device\n",
        "\n",
        "Afin d'obtenir de meilleures performances lors de l'entrainement et de l'évaluation des différents modèles, nous allons utiliser un GPU à l'aide de cuda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # Select the device\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Classe illustrant le dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# Create a class for the dataset with colors pictures\n",
        "class FashionDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df.copy()\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # Get the image if the image exists\n",
        "        try:\n",
        "            image = cv2.imread(\"fashion-dataset-small/images/\" + str(self.df.iloc[idx, 0]) + \".jpg\")\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        except:\n",
        "            image = np.zeros((224, 224, 3), np.uint8)\n",
        "\n",
        "        # Get the label with the articleType\n",
        "        label = self.df['articleTypeCode'].iloc[idx]\n",
        "        \n",
        "        # Transform the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Instantiation des set d'entrainements et de tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 3, 224, 224])\n",
            "tensor([10, 55, 35, 14, 15, 16, 22, 10, 35, 45, 13, 44, 49, 55, 35, 13],\n",
            "       dtype=torch.int8)\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    # Resize\n",
        "    transforms.Resize((224, 224)),\n",
        "    # To tensor\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "dataset = FashionDataset(df, transform=transform)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "for batch_id, batch in enumerate(train_loader):\n",
        "    images, labels  = batch\n",
        "    if batch_id == 0:\n",
        "        print(images.shape)\n",
        "        print(labels)\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fonction permettant l'enregistrement du model entrainé / le chargement du model entrainé"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def save_model_min(model, modelName, path):\n",
        "    create_directory(modelName)\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "def save_model(model, modelName, epoch, optimizer, path):\n",
        "    create_directory(modelName)\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()\n",
        "    }, path)\n",
        "\n",
        "def create_directory(modelName):\n",
        "    Path(\"models\").mkdir(parents=True, exist_ok=True)\n",
        "    Path(\"models/{}\".format(modelName)).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def load_model(model, path):\n",
        "    model.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fonction d'entrainement générique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Training function\n",
        "def train_optim(model, _train_loader, _test_loader, epochs, log_frequency, device, modelName, batch_size, learning_rate=1e-4):\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  print(\"TRAIN&TEST on {} model | Number of epochs : {} | Batch size : {} | Log frequency : {} | Device : {}\".format(modelName, epochs, batch_size, log_frequency, device))\n",
        "\n",
        "  model.to(device) # we make sure the model is on the proper device\n",
        "\n",
        "  # Multiclass classification setting, we use cross-entropy\n",
        "  # note that this implementation requires the logits as input \n",
        "  # logits: values prior softmax transformation \n",
        "  loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  \n",
        "  numberOfBatch = round(len(_train_loader.dataset) / batch_size)\n",
        "\n",
        "  rates = []\n",
        "  lossesValidation = []\n",
        "  lossesTrain = []\n",
        "  for t in range(epochs):\n",
        "\n",
        "      lossesTrainByBatch = []\n",
        "\n",
        "      model.train() # we specify that we are training the model\n",
        "\n",
        "      # At each epoch, the training set will be processed as a set of batches\n",
        "      for batch_id,  batch in enumerate(_train_loader) : \n",
        "\n",
        "        images, labels  = batch\n",
        "        labels = labels.type(torch.LongTensor)\n",
        "\n",
        "        # we put the data on the same device\n",
        "        images , labels = images.to(device), labels.to(device)  \n",
        "        \n",
        "        y_pred = model(images) # forward pass output=logits\n",
        "\n",
        "        loss = loss_fn(y_pred, labels)\n",
        "\n",
        "        lossesTrainByBatch.append(loss.item())\n",
        "\n",
        "        if batch_id % log_frequency == 0:\n",
        "          print(\"Epoch: {:03d} / {} | Batch: {:03d} / {} | Loss: {:.3f} | Time elapsed: {:.3f} s \".format(t+1, epochs, batch_id+1, numberOfBatch, loss.item(), (time.time() - start_time)))\n",
        "\n",
        "        optimizer.zero_grad() # clear the gradient before backward\n",
        "        loss.backward()       # update the gradient\n",
        "\n",
        "        optimizer.step() # update the model parameters using the gradient\n",
        "\n",
        "      lossesTrain.append((sum(lossesTrainByBatch))/len(lossesTrainByBatch))\n",
        "\n",
        "      # Model evaluation after each step computing the accuracy\n",
        "      with torch.no_grad():\n",
        "        model.eval()\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        lossesValidationByBatch = []\n",
        "\n",
        "        for batch_id, batch in enumerate(_test_loader):\n",
        "          images , labels = batch\n",
        "          labels = labels.type(torch.LongTensor)\n",
        "          images , labels = images.to(device), labels.to(device)\n",
        "          y_pred = model(images) # forward computes the logits\n",
        "\n",
        "          lossesValidationByBatch.append(loss_fn(y_pred, labels).item())\n",
        "\n",
        "          sf_y_pred = torch.nn.Softmax(dim=1)(y_pred) # softmax to obtain the probability distribution\n",
        "          _, predicted = torch.max(sf_y_pred , 1)     # decision rule, we select the max\n",
        "          \n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        print(\"[Validation] Accuracy: {:.3f}%\\n\".format(100 * correct / total))\n",
        "        rates.append(correct / total)\n",
        "\n",
        "        print(\"[Validation] Loss (avg): {:.3f}\\n\".format((sum(lossesValidationByBatch))/len(lossesValidationByBatch)))\n",
        "        lossesValidation.append((sum(lossesValidationByBatch))/len(lossesValidationByBatch))\n",
        "        \n",
        "        # Save the model after each epoch\n",
        "        save_model_min(model, modelName, \"models/{}/{}_{}.pt\".format(modelName, modelName, t))\n",
        "  \n",
        "  # Show a summary chart of rates %\n",
        "  plt.plot(np.arange(0, len(rates)), rates)\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(np.arange(0, len(lossesValidation)), lossesValidation, color='red', label='Loss validation')\n",
        "  plt.ylabel(\"Loss value (avg)\")\n",
        "  plt.plot(np.arange(0, len(lossesTrain)), lossesTrain, color='green', label = 'Loss Train')\n",
        "  plt.legend(loc = 'upper right')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pour chaque modèle :\n",
        "\n",
        "- Tests non pré-entrainé\n",
        "- Tests pré-entrainé\n",
        "- Tests couleur\n",
        "- Tests sans-couleur\n",
        "- Tests différents MLP à la sortie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on package torchvision.models in torchvision:\n",
            "\n",
            "NAME\n",
            "    torchvision.models\n",
            "\n",
            "PACKAGE CONTENTS\n",
            "    _api\n",
            "    _meta\n",
            "    _utils\n",
            "    alexnet\n",
            "    convnext\n",
            "    densenet\n",
            "    detection (package)\n",
            "    efficientnet\n",
            "    feature_extraction\n",
            "    googlenet\n",
            "    inception\n",
            "    mnasnet\n",
            "    mobilenet\n",
            "    mobilenetv2\n",
            "    mobilenetv3\n",
            "    optical_flow (package)\n",
            "    quantization (package)\n",
            "    regnet\n",
            "    resnet\n",
            "    segmentation (package)\n",
            "    shufflenetv2\n",
            "    squeezenet\n",
            "    swin_transformer\n",
            "    vgg\n",
            "    video (package)\n",
            "    vision_transformer\n",
            "\n",
            "FILE\n",
            "    c:\\users\\quent\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\torchvision\\models\\__init__.py\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torchvision import models\n",
        "\n",
        "help(models)\n",
        "\n",
        "def freeze_model(model):\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "def unfreeze_model(model):\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Modèle : ResNet18**\n",
        "\n",
        "##### Architecture :\n",
        "\n",
        "![resnet-18-architecture](https://penseeartificielle.fr/wp-content/uploads/2019/01/Proposed-Modified-ResNet-18-architecture-for-Bangla-HCR-In-the-diagram-conv-stands-for.jpg)\n",
        "\n",
        "*(source : https://penseeartificielle.fr/focus-reseau-neurones-convolutifs/proposed-modified-resnet-18-architecture-for-bangla-hcr-in-the-diagram-conv-stands-for/)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Used device : cuda:0'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 001, batch: 001, loss: 4.044 \n",
            "epoch: 001, batch: 021, loss: 3.454 \n",
            "epoch: 001, batch: 041, loss: 3.122 \n",
            "epoch: 001, batch: 061, loss: 2.587 \n",
            "epoch: 001, batch: 081, loss: 3.367 \n",
            "epoch: 001, batch: 101, loss: 2.254 \n",
            "epoch: 001, batch: 121, loss: 1.952 \n",
            "epoch: 001, batch: 141, loss: 2.236 \n",
            "epoch: 001, batch: 161, loss: 2.236 \n",
            "epoch: 001, batch: 181, loss: 2.856 \n",
            "epoch: 001, batch: 201, loss: 2.992 \n",
            "epoch: 001, batch: 221, loss: 2.382 \n",
            "epoch: 001, batch: 241, loss: 1.352 \n",
            "epoch: 001, batch: 261, loss: 1.307 \n",
            "epoch: 001, batch: 281, loss: 1.311 \n",
            "epoch: 001, batch: 301, loss: 1.632 \n",
            "epoch: 001, batch: 321, loss: 1.564 \n",
            "epoch: 001, batch: 341, loss: 1.885 \n",
            "epoch: 001, batch: 361, loss: 1.680 \n",
            "epoch: 001, batch: 381, loss: 2.169 \n",
            "epoch: 001, batch: 401, loss: 1.427 \n",
            "epoch: 001, batch: 421, loss: 2.087 \n",
            "epoch: 001, batch: 441, loss: 1.486 \n",
            "epoch: 001, batch: 461, loss: 1.332 \n",
            "epoch: 001, batch: 481, loss: 1.389 \n",
            "epoch: 001, batch: 501, loss: 1.078 \n",
            "epoch: 001, batch: 521, loss: 1.571 \n",
            "epoch: 001, batch: 541, loss: 1.792 \n",
            "epoch: 001, batch: 561, loss: 0.901 \n",
            "epoch: 001, batch: 581, loss: 0.944 \n",
            "epoch: 001, batch: 601, loss: 1.698 \n",
            "epoch: 001, batch: 621, loss: 1.764 \n",
            "epoch: 001, batch: 641, loss: 1.031 \n",
            "epoch: 001, batch: 661, loss: 1.214 \n",
            "epoch: 001, batch: 681, loss: 1.910 \n",
            "epoch: 001, batch: 701, loss: 1.233 \n",
            "epoch: 001, batch: 721, loss: 1.487 \n",
            "epoch: 001, batch: 741, loss: 1.450 \n",
            "epoch: 001, batch: 761, loss: 1.210 \n",
            "epoch: 001, batch: 781, loss: 1.003 \n",
            "epoch: 001, batch: 801, loss: 1.308 \n",
            "epoch: 001, batch: 821, loss: 1.112 \n",
            "epoch: 001, batch: 841, loss: 0.956 \n",
            "epoch: 001, batch: 861, loss: 1.059 \n",
            "epoch: 001, batch: 881, loss: 1.088 \n",
            "epoch: 001, batch: 901, loss: 1.109 \n",
            "epoch: 001, batch: 921, loss: 1.222 \n",
            "epoch: 001, batch: 941, loss: 1.081 \n",
            "epoch: 001, batch: 961, loss: 0.705 \n",
            "epoch: 001, batch: 981, loss: 1.205 \n",
            "epoch: 001, batch: 1001, loss: 1.523 \n",
            "epoch: 001, batch: 1021, loss: 1.412 \n",
            "epoch: 001, batch: 1041, loss: 0.767 \n",
            "epoch: 001, batch: 1061, loss: 1.056 \n",
            "epoch: 001, batch: 1081, loss: 1.693 \n",
            "epoch: 001, batch: 1101, loss: 0.726 \n",
            "epoch: 001, batch: 1121, loss: 0.820 \n",
            "epoch: 001, batch: 1141, loss: 1.946 \n",
            "epoch: 001, batch: 1161, loss: 1.329 \n",
            "epoch: 001, batch: 1181, loss: 1.198 \n",
            "epoch: 001, batch: 1201, loss: 1.110 \n",
            "epoch: 001, batch: 1221, loss: 0.909 \n",
            "epoch: 001, batch: 1241, loss: 0.750 \n",
            "epoch: 001, batch: 1261, loss: 0.982 \n",
            "epoch: 001, batch: 1281, loss: 1.079 \n",
            "epoch: 001, batch: 1301, loss: 0.794 \n",
            "epoch: 001, batch: 1321, loss: 1.291 \n",
            "epoch: 001, batch: 1341, loss: 1.207 \n",
            "epoch: 001, batch: 1361, loss: 0.803 \n",
            "epoch: 001, batch: 1381, loss: 1.596 \n",
            "epoch: 001, batch: 1401, loss: 1.015 \n",
            "epoch: 001, batch: 1421, loss: 1.394 \n",
            "epoch: 001, batch: 1441, loss: 1.505 \n",
            "epoch: 001, batch: 1461, loss: 0.902 \n",
            "epoch: 001, batch: 1481, loss: 1.912 \n",
            "epoch: 001, batch: 1501, loss: 1.263 \n",
            "epoch: 001, batch: 1521, loss: 1.825 \n",
            "epoch: 001, batch: 1541, loss: 1.406 \n",
            "[validation] accuracy: 69.303%\n",
            "\n",
            "[validation] loss: 1.023\n",
            "\n",
            "epoch: 002, batch: 001, loss: 0.741 \n",
            "epoch: 002, batch: 021, loss: 1.225 \n",
            "epoch: 002, batch: 041, loss: 1.294 \n",
            "epoch: 002, batch: 061, loss: 1.283 \n",
            "epoch: 002, batch: 081, loss: 0.980 \n",
            "epoch: 002, batch: 101, loss: 0.985 \n",
            "epoch: 002, batch: 121, loss: 1.421 \n",
            "epoch: 002, batch: 141, loss: 1.120 \n",
            "epoch: 002, batch: 161, loss: 0.962 \n",
            "epoch: 002, batch: 181, loss: 1.746 \n",
            "epoch: 002, batch: 201, loss: 1.016 \n",
            "epoch: 002, batch: 221, loss: 0.812 \n",
            "epoch: 002, batch: 241, loss: 1.203 \n",
            "epoch: 002, batch: 261, loss: 0.674 \n",
            "epoch: 002, batch: 281, loss: 1.786 \n",
            "epoch: 002, batch: 301, loss: 0.539 \n",
            "epoch: 002, batch: 321, loss: 1.126 \n",
            "epoch: 002, batch: 341, loss: 0.817 \n",
            "epoch: 002, batch: 361, loss: 0.842 \n",
            "epoch: 002, batch: 381, loss: 0.965 \n",
            "epoch: 002, batch: 401, loss: 1.258 \n",
            "epoch: 002, batch: 421, loss: 1.131 \n",
            "epoch: 002, batch: 441, loss: 1.294 \n",
            "epoch: 002, batch: 461, loss: 1.868 \n",
            "epoch: 002, batch: 481, loss: 1.213 \n",
            "epoch: 002, batch: 501, loss: 0.912 \n",
            "epoch: 002, batch: 521, loss: 1.246 \n",
            "epoch: 002, batch: 541, loss: 1.504 \n",
            "epoch: 002, batch: 561, loss: 0.899 \n",
            "epoch: 002, batch: 581, loss: 1.316 \n",
            "epoch: 002, batch: 601, loss: 0.906 \n",
            "epoch: 002, batch: 621, loss: 0.787 \n",
            "epoch: 002, batch: 641, loss: 1.016 \n",
            "epoch: 002, batch: 661, loss: 1.311 \n",
            "epoch: 002, batch: 681, loss: 0.816 \n",
            "epoch: 002, batch: 701, loss: 0.711 \n",
            "epoch: 002, batch: 721, loss: 1.289 \n",
            "epoch: 002, batch: 741, loss: 1.060 \n",
            "epoch: 002, batch: 761, loss: 0.679 \n",
            "epoch: 002, batch: 781, loss: 1.364 \n",
            "epoch: 002, batch: 801, loss: 1.096 \n",
            "epoch: 002, batch: 821, loss: 0.897 \n",
            "epoch: 002, batch: 841, loss: 0.802 \n",
            "epoch: 002, batch: 861, loss: 0.922 \n",
            "epoch: 002, batch: 881, loss: 1.284 \n",
            "epoch: 002, batch: 901, loss: 1.129 \n",
            "epoch: 002, batch: 921, loss: 1.498 \n",
            "epoch: 002, batch: 941, loss: 1.253 \n",
            "epoch: 002, batch: 961, loss: 0.573 \n",
            "epoch: 002, batch: 981, loss: 1.069 \n",
            "epoch: 002, batch: 1001, loss: 0.358 \n",
            "epoch: 002, batch: 1021, loss: 0.556 \n",
            "epoch: 002, batch: 1041, loss: 0.969 \n",
            "epoch: 002, batch: 1061, loss: 0.801 \n",
            "epoch: 002, batch: 1081, loss: 1.165 \n",
            "epoch: 002, batch: 1101, loss: 0.999 \n",
            "epoch: 002, batch: 1121, loss: 1.038 \n",
            "epoch: 002, batch: 1141, loss: 1.291 \n",
            "epoch: 002, batch: 1161, loss: 1.123 \n",
            "epoch: 002, batch: 1181, loss: 0.687 \n",
            "epoch: 002, batch: 1201, loss: 0.896 \n",
            "epoch: 002, batch: 1221, loss: 1.049 \n",
            "epoch: 002, batch: 1241, loss: 0.942 \n",
            "epoch: 002, batch: 1261, loss: 1.073 \n",
            "epoch: 002, batch: 1281, loss: 0.762 \n",
            "epoch: 002, batch: 1301, loss: 1.140 \n",
            "epoch: 002, batch: 1321, loss: 0.539 \n",
            "epoch: 002, batch: 1341, loss: 0.680 \n",
            "epoch: 002, batch: 1361, loss: 0.886 \n",
            "epoch: 002, batch: 1381, loss: 1.163 \n",
            "epoch: 002, batch: 1401, loss: 1.072 \n",
            "epoch: 002, batch: 1421, loss: 0.934 \n",
            "epoch: 002, batch: 1441, loss: 0.762 \n",
            "epoch: 002, batch: 1461, loss: 1.225 \n",
            "epoch: 002, batch: 1481, loss: 0.728 \n",
            "epoch: 002, batch: 1501, loss: 0.952 \n",
            "epoch: 002, batch: 1521, loss: 0.911 \n",
            "epoch: 002, batch: 1541, loss: 0.489 \n",
            "[validation] accuracy: 73.501%\n",
            "\n",
            "[validation] loss: 0.843\n",
            "\n",
            "epoch: 003, batch: 001, loss: 0.880 \n",
            "epoch: 003, batch: 021, loss: 0.849 \n",
            "epoch: 003, batch: 041, loss: 0.746 \n",
            "epoch: 003, batch: 061, loss: 0.513 \n",
            "epoch: 003, batch: 081, loss: 1.175 \n",
            "epoch: 003, batch: 101, loss: 0.525 \n",
            "epoch: 003, batch: 121, loss: 1.162 \n",
            "epoch: 003, batch: 141, loss: 1.320 \n",
            "epoch: 003, batch: 161, loss: 1.391 \n",
            "epoch: 003, batch: 181, loss: 1.015 \n",
            "epoch: 003, batch: 201, loss: 0.840 \n",
            "epoch: 003, batch: 221, loss: 0.640 \n",
            "epoch: 003, batch: 241, loss: 0.742 \n",
            "epoch: 003, batch: 261, loss: 0.799 \n",
            "epoch: 003, batch: 281, loss: 1.278 \n",
            "epoch: 003, batch: 301, loss: 1.682 \n",
            "epoch: 003, batch: 321, loss: 1.243 \n",
            "epoch: 003, batch: 341, loss: 1.209 \n",
            "epoch: 003, batch: 361, loss: 1.212 \n",
            "epoch: 003, batch: 381, loss: 0.748 \n",
            "epoch: 003, batch: 401, loss: 0.698 \n",
            "epoch: 003, batch: 421, loss: 0.918 \n",
            "epoch: 003, batch: 441, loss: 0.691 \n",
            "epoch: 003, batch: 461, loss: 0.712 \n",
            "epoch: 003, batch: 481, loss: 1.088 \n",
            "epoch: 003, batch: 501, loss: 0.653 \n",
            "epoch: 003, batch: 521, loss: 1.171 \n",
            "epoch: 003, batch: 541, loss: 0.707 \n",
            "epoch: 003, batch: 561, loss: 0.845 \n",
            "epoch: 003, batch: 581, loss: 0.387 \n",
            "epoch: 003, batch: 601, loss: 1.085 \n",
            "epoch: 003, batch: 621, loss: 1.479 \n",
            "epoch: 003, batch: 641, loss: 0.810 \n",
            "epoch: 003, batch: 661, loss: 0.835 \n",
            "epoch: 003, batch: 681, loss: 0.734 \n",
            "epoch: 003, batch: 701, loss: 0.926 \n",
            "epoch: 003, batch: 721, loss: 0.555 \n",
            "epoch: 003, batch: 741, loss: 0.646 \n",
            "epoch: 003, batch: 761, loss: 1.413 \n",
            "epoch: 003, batch: 781, loss: 0.343 \n",
            "epoch: 003, batch: 801, loss: 1.210 \n",
            "epoch: 003, batch: 821, loss: 0.765 \n",
            "epoch: 003, batch: 841, loss: 0.760 \n",
            "epoch: 003, batch: 861, loss: 0.439 \n",
            "epoch: 003, batch: 881, loss: 1.585 \n",
            "epoch: 003, batch: 901, loss: 0.907 \n",
            "epoch: 003, batch: 921, loss: 0.585 \n",
            "epoch: 003, batch: 941, loss: 0.516 \n",
            "epoch: 003, batch: 961, loss: 0.766 \n",
            "epoch: 003, batch: 981, loss: 0.624 \n",
            "epoch: 003, batch: 1001, loss: 1.062 \n",
            "epoch: 003, batch: 1021, loss: 1.016 \n",
            "epoch: 003, batch: 1041, loss: 0.510 \n",
            "epoch: 003, batch: 1061, loss: 0.614 \n",
            "epoch: 003, batch: 1081, loss: 0.608 \n",
            "epoch: 003, batch: 1101, loss: 0.906 \n",
            "epoch: 003, batch: 1121, loss: 0.463 \n",
            "epoch: 003, batch: 1141, loss: 0.681 \n",
            "epoch: 003, batch: 1161, loss: 1.831 \n",
            "epoch: 003, batch: 1181, loss: 1.190 \n",
            "epoch: 003, batch: 1201, loss: 0.590 \n",
            "epoch: 003, batch: 1221, loss: 1.792 \n",
            "epoch: 003, batch: 1241, loss: 0.487 \n",
            "epoch: 003, batch: 1261, loss: 0.557 \n",
            "epoch: 003, batch: 1281, loss: 0.935 \n",
            "epoch: 003, batch: 1301, loss: 0.378 \n",
            "epoch: 003, batch: 1321, loss: 0.663 \n",
            "epoch: 003, batch: 1341, loss: 1.133 \n",
            "epoch: 003, batch: 1361, loss: 1.446 \n",
            "epoch: 003, batch: 1381, loss: 0.491 \n",
            "epoch: 003, batch: 1401, loss: 0.784 \n",
            "epoch: 003, batch: 1421, loss: 0.974 \n",
            "epoch: 003, batch: 1441, loss: 0.867 \n",
            "epoch: 003, batch: 1461, loss: 0.704 \n",
            "epoch: 003, batch: 1481, loss: 0.871 \n",
            "epoch: 003, batch: 1501, loss: 1.248 \n",
            "epoch: 003, batch: 1521, loss: 1.290 \n",
            "epoch: 003, batch: 1541, loss: 0.766 \n",
            "[validation] accuracy: 75.316%\n",
            "\n",
            "[validation] loss: 0.772\n",
            "\n",
            "epoch: 004, batch: 001, loss: 0.368 \n",
            "epoch: 004, batch: 021, loss: 0.330 \n",
            "epoch: 004, batch: 041, loss: 0.839 \n",
            "epoch: 004, batch: 061, loss: 0.883 \n",
            "epoch: 004, batch: 081, loss: 0.678 \n",
            "epoch: 004, batch: 101, loss: 0.841 \n",
            "epoch: 004, batch: 121, loss: 0.725 \n",
            "epoch: 004, batch: 141, loss: 0.583 \n",
            "epoch: 004, batch: 161, loss: 0.598 \n",
            "epoch: 004, batch: 181, loss: 0.481 \n",
            "epoch: 004, batch: 201, loss: 1.245 \n",
            "epoch: 004, batch: 221, loss: 0.845 \n",
            "epoch: 004, batch: 241, loss: 1.110 \n",
            "epoch: 004, batch: 261, loss: 0.636 \n",
            "epoch: 004, batch: 281, loss: 1.110 \n",
            "epoch: 004, batch: 301, loss: 0.591 \n",
            "epoch: 004, batch: 321, loss: 0.902 \n",
            "epoch: 004, batch: 341, loss: 0.466 \n",
            "epoch: 004, batch: 361, loss: 0.425 \n",
            "epoch: 004, batch: 381, loss: 1.052 \n",
            "epoch: 004, batch: 401, loss: 0.657 \n",
            "epoch: 004, batch: 421, loss: 0.455 \n",
            "epoch: 004, batch: 441, loss: 0.664 \n",
            "epoch: 004, batch: 461, loss: 0.572 \n",
            "epoch: 004, batch: 481, loss: 0.684 \n",
            "epoch: 004, batch: 501, loss: 0.581 \n",
            "epoch: 004, batch: 521, loss: 0.961 \n",
            "epoch: 004, batch: 541, loss: 1.483 \n",
            "epoch: 004, batch: 561, loss: 1.268 \n",
            "epoch: 004, batch: 581, loss: 0.231 \n",
            "epoch: 004, batch: 601, loss: 0.736 \n",
            "epoch: 004, batch: 621, loss: 0.440 \n",
            "epoch: 004, batch: 641, loss: 0.363 \n",
            "epoch: 004, batch: 661, loss: 0.877 \n",
            "epoch: 004, batch: 681, loss: 0.498 \n",
            "epoch: 004, batch: 701, loss: 0.510 \n",
            "epoch: 004, batch: 721, loss: 0.621 \n",
            "epoch: 004, batch: 741, loss: 0.358 \n",
            "epoch: 004, batch: 761, loss: 0.863 \n",
            "epoch: 004, batch: 781, loss: 0.783 \n",
            "epoch: 004, batch: 801, loss: 0.939 \n",
            "epoch: 004, batch: 821, loss: 0.480 \n",
            "epoch: 004, batch: 841, loss: 1.279 \n",
            "epoch: 004, batch: 861, loss: 0.474 \n",
            "epoch: 004, batch: 881, loss: 1.024 \n",
            "epoch: 004, batch: 901, loss: 0.689 \n",
            "epoch: 004, batch: 921, loss: 0.573 \n",
            "epoch: 004, batch: 941, loss: 0.450 \n",
            "epoch: 004, batch: 961, loss: 0.596 \n",
            "epoch: 004, batch: 981, loss: 0.648 \n",
            "epoch: 004, batch: 1001, loss: 0.787 \n",
            "epoch: 004, batch: 1021, loss: 1.316 \n",
            "epoch: 004, batch: 1041, loss: 0.461 \n",
            "epoch: 004, batch: 1061, loss: 0.432 \n",
            "epoch: 004, batch: 1081, loss: 1.185 \n",
            "epoch: 004, batch: 1101, loss: 0.762 \n",
            "epoch: 004, batch: 1121, loss: 0.942 \n",
            "epoch: 004, batch: 1141, loss: 0.776 \n",
            "epoch: 004, batch: 1161, loss: 0.241 \n",
            "epoch: 004, batch: 1181, loss: 0.694 \n",
            "epoch: 004, batch: 1201, loss: 1.014 \n",
            "epoch: 004, batch: 1221, loss: 0.706 \n",
            "epoch: 004, batch: 1241, loss: 1.056 \n",
            "epoch: 004, batch: 1261, loss: 0.708 \n",
            "epoch: 004, batch: 1281, loss: 0.939 \n",
            "epoch: 004, batch: 1301, loss: 0.765 \n",
            "epoch: 004, batch: 1321, loss: 0.511 \n",
            "epoch: 004, batch: 1341, loss: 1.566 \n",
            "epoch: 004, batch: 1361, loss: 1.069 \n",
            "epoch: 004, batch: 1381, loss: 1.155 \n",
            "epoch: 004, batch: 1401, loss: 0.567 \n",
            "epoch: 004, batch: 1421, loss: 0.341 \n",
            "epoch: 004, batch: 1441, loss: 1.308 \n",
            "epoch: 004, batch: 1461, loss: 0.789 \n",
            "epoch: 004, batch: 1481, loss: 0.757 \n",
            "epoch: 004, batch: 1501, loss: 0.582 \n",
            "epoch: 004, batch: 1521, loss: 0.690 \n",
            "epoch: 004, batch: 1541, loss: 0.265 \n",
            "[validation] accuracy: 75.203%\n",
            "\n",
            "[validation] loss: 0.755\n",
            "\n",
            "epoch: 005, batch: 001, loss: 1.294 \n",
            "epoch: 005, batch: 021, loss: 0.648 \n",
            "epoch: 005, batch: 041, loss: 0.757 \n",
            "epoch: 005, batch: 061, loss: 0.577 \n",
            "epoch: 005, batch: 081, loss: 0.548 \n",
            "epoch: 005, batch: 101, loss: 0.521 \n",
            "epoch: 005, batch: 121, loss: 1.042 \n",
            "epoch: 005, batch: 141, loss: 0.612 \n",
            "epoch: 005, batch: 161, loss: 0.201 \n",
            "epoch: 005, batch: 181, loss: 1.633 \n",
            "epoch: 005, batch: 201, loss: 0.634 \n",
            "epoch: 005, batch: 221, loss: 1.056 \n",
            "epoch: 005, batch: 241, loss: 0.643 \n",
            "epoch: 005, batch: 261, loss: 0.525 \n",
            "epoch: 005, batch: 281, loss: 0.298 \n",
            "epoch: 005, batch: 301, loss: 0.642 \n",
            "epoch: 005, batch: 321, loss: 0.437 \n",
            "epoch: 005, batch: 341, loss: 0.990 \n",
            "epoch: 005, batch: 361, loss: 0.835 \n",
            "epoch: 005, batch: 381, loss: 0.872 \n",
            "epoch: 005, batch: 401, loss: 0.686 \n",
            "epoch: 005, batch: 421, loss: 0.670 \n",
            "epoch: 005, batch: 441, loss: 1.148 \n",
            "epoch: 005, batch: 461, loss: 0.294 \n",
            "epoch: 005, batch: 481, loss: 0.482 \n",
            "epoch: 005, batch: 501, loss: 1.228 \n",
            "epoch: 005, batch: 521, loss: 0.491 \n",
            "epoch: 005, batch: 541, loss: 1.069 \n",
            "epoch: 005, batch: 561, loss: 0.252 \n",
            "epoch: 005, batch: 581, loss: 0.632 \n",
            "epoch: 005, batch: 601, loss: 0.370 \n",
            "epoch: 005, batch: 621, loss: 0.481 \n",
            "epoch: 005, batch: 641, loss: 0.816 \n",
            "epoch: 005, batch: 661, loss: 1.228 \n",
            "epoch: 005, batch: 681, loss: 0.611 \n",
            "epoch: 005, batch: 701, loss: 0.693 \n",
            "epoch: 005, batch: 721, loss: 0.887 \n",
            "epoch: 005, batch: 741, loss: 0.992 \n",
            "epoch: 005, batch: 761, loss: 0.857 \n",
            "epoch: 005, batch: 781, loss: 0.733 \n",
            "epoch: 005, batch: 801, loss: 0.659 \n",
            "epoch: 005, batch: 821, loss: 0.888 \n",
            "epoch: 005, batch: 841, loss: 0.608 \n",
            "epoch: 005, batch: 861, loss: 0.538 \n",
            "epoch: 005, batch: 881, loss: 1.177 \n",
            "epoch: 005, batch: 901, loss: 0.444 \n",
            "epoch: 005, batch: 921, loss: 0.763 \n",
            "epoch: 005, batch: 941, loss: 0.960 \n",
            "epoch: 005, batch: 961, loss: 0.343 \n",
            "epoch: 005, batch: 981, loss: 0.415 \n",
            "epoch: 005, batch: 1001, loss: 0.869 \n",
            "epoch: 005, batch: 1021, loss: 0.710 \n",
            "epoch: 005, batch: 1041, loss: 1.183 \n",
            "epoch: 005, batch: 1061, loss: 0.778 \n",
            "epoch: 005, batch: 1081, loss: 0.492 \n",
            "epoch: 005, batch: 1101, loss: 0.538 \n",
            "epoch: 005, batch: 1121, loss: 0.591 \n",
            "epoch: 005, batch: 1141, loss: 0.678 \n",
            "epoch: 005, batch: 1161, loss: 0.442 \n",
            "epoch: 005, batch: 1181, loss: 0.492 \n",
            "epoch: 005, batch: 1201, loss: 0.819 \n",
            "epoch: 005, batch: 1221, loss: 0.362 \n",
            "epoch: 005, batch: 1241, loss: 0.674 \n",
            "epoch: 005, batch: 1261, loss: 0.749 \n",
            "epoch: 005, batch: 1281, loss: 0.541 \n",
            "epoch: 005, batch: 1301, loss: 0.487 \n",
            "epoch: 005, batch: 1321, loss: 0.947 \n",
            "epoch: 005, batch: 1341, loss: 0.486 \n",
            "epoch: 005, batch: 1361, loss: 0.789 \n",
            "epoch: 005, batch: 1381, loss: 0.267 \n",
            "epoch: 005, batch: 1401, loss: 0.305 \n",
            "epoch: 005, batch: 1421, loss: 0.769 \n",
            "epoch: 005, batch: 1441, loss: 1.059 \n",
            "epoch: 005, batch: 1461, loss: 0.453 \n",
            "epoch: 005, batch: 1481, loss: 1.061 \n",
            "epoch: 005, batch: 1501, loss: 1.011 \n",
            "epoch: 005, batch: 1521, loss: 0.966 \n",
            "epoch: 005, batch: 1541, loss: 1.001 \n",
            "[validation] accuracy: 76.499%\n",
            "\n",
            "[validation] loss: 0.729\n",
            "\n",
            "epoch: 006, batch: 001, loss: 0.464 \n",
            "epoch: 006, batch: 021, loss: 0.365 \n",
            "epoch: 006, batch: 041, loss: 0.840 \n",
            "epoch: 006, batch: 061, loss: 0.625 \n",
            "epoch: 006, batch: 081, loss: 1.302 \n",
            "epoch: 006, batch: 101, loss: 0.565 \n",
            "epoch: 006, batch: 121, loss: 0.455 \n",
            "epoch: 006, batch: 141, loss: 0.690 \n",
            "epoch: 006, batch: 161, loss: 0.767 \n",
            "epoch: 006, batch: 181, loss: 1.122 \n",
            "epoch: 006, batch: 201, loss: 0.614 \n",
            "epoch: 006, batch: 221, loss: 1.002 \n",
            "epoch: 006, batch: 241, loss: 0.859 \n",
            "epoch: 006, batch: 261, loss: 0.659 \n",
            "epoch: 006, batch: 281, loss: 0.410 \n",
            "epoch: 006, batch: 301, loss: 0.837 \n",
            "epoch: 006, batch: 321, loss: 1.316 \n",
            "epoch: 006, batch: 341, loss: 1.187 \n",
            "epoch: 006, batch: 361, loss: 0.742 \n",
            "epoch: 006, batch: 381, loss: 0.716 \n",
            "epoch: 006, batch: 401, loss: 0.737 \n",
            "epoch: 006, batch: 421, loss: 0.376 \n",
            "epoch: 006, batch: 441, loss: 0.644 \n",
            "epoch: 006, batch: 461, loss: 0.857 \n",
            "epoch: 006, batch: 481, loss: 0.638 \n",
            "epoch: 006, batch: 501, loss: 0.532 \n",
            "epoch: 006, batch: 521, loss: 0.449 \n",
            "epoch: 006, batch: 541, loss: 0.401 \n",
            "epoch: 006, batch: 561, loss: 0.643 \n",
            "epoch: 006, batch: 581, loss: 0.754 \n",
            "epoch: 006, batch: 601, loss: 0.808 \n",
            "epoch: 006, batch: 621, loss: 0.344 \n",
            "epoch: 006, batch: 641, loss: 1.095 \n",
            "epoch: 006, batch: 661, loss: 0.955 \n",
            "epoch: 006, batch: 681, loss: 0.795 \n",
            "epoch: 006, batch: 701, loss: 0.674 \n",
            "epoch: 006, batch: 721, loss: 0.649 \n",
            "epoch: 006, batch: 741, loss: 0.953 \n",
            "epoch: 006, batch: 761, loss: 0.737 \n",
            "epoch: 006, batch: 781, loss: 0.470 \n",
            "epoch: 006, batch: 801, loss: 0.433 \n",
            "epoch: 006, batch: 821, loss: 0.383 \n",
            "epoch: 006, batch: 841, loss: 0.695 \n",
            "epoch: 006, batch: 861, loss: 0.855 \n",
            "epoch: 006, batch: 881, loss: 1.200 \n",
            "epoch: 006, batch: 901, loss: 1.887 \n",
            "epoch: 006, batch: 921, loss: 1.115 \n",
            "epoch: 006, batch: 941, loss: 0.348 \n",
            "epoch: 006, batch: 961, loss: 0.624 \n",
            "epoch: 006, batch: 981, loss: 0.979 \n",
            "epoch: 006, batch: 1001, loss: 0.752 \n",
            "epoch: 006, batch: 1021, loss: 0.305 \n",
            "epoch: 006, batch: 1041, loss: 0.833 \n",
            "epoch: 006, batch: 1061, loss: 1.033 \n",
            "epoch: 006, batch: 1081, loss: 0.788 \n",
            "epoch: 006, batch: 1101, loss: 1.247 \n",
            "epoch: 006, batch: 1121, loss: 0.963 \n",
            "epoch: 006, batch: 1141, loss: 0.397 \n",
            "epoch: 006, batch: 1161, loss: 0.557 \n",
            "epoch: 006, batch: 1181, loss: 0.832 \n",
            "epoch: 006, batch: 1201, loss: 0.614 \n",
            "epoch: 006, batch: 1221, loss: 1.033 \n",
            "epoch: 006, batch: 1241, loss: 0.520 \n",
            "epoch: 006, batch: 1261, loss: 0.670 \n",
            "epoch: 006, batch: 1281, loss: 0.582 \n",
            "epoch: 006, batch: 1301, loss: 0.359 \n",
            "epoch: 006, batch: 1321, loss: 0.756 \n",
            "epoch: 006, batch: 1341, loss: 0.516 \n",
            "epoch: 006, batch: 1361, loss: 0.699 \n",
            "epoch: 006, batch: 1381, loss: 0.771 \n",
            "epoch: 006, batch: 1401, loss: 1.060 \n",
            "epoch: 006, batch: 1421, loss: 0.395 \n",
            "epoch: 006, batch: 1441, loss: 0.681 \n",
            "epoch: 006, batch: 1461, loss: 0.563 \n",
            "epoch: 006, batch: 1481, loss: 0.538 \n",
            "epoch: 006, batch: 1501, loss: 0.794 \n",
            "epoch: 006, batch: 1521, loss: 0.486 \n",
            "epoch: 006, batch: 1541, loss: 0.635 \n",
            "[validation] accuracy: 77.147%\n",
            "\n",
            "[validation] loss: 0.708\n",
            "\n",
            "epoch: 007, batch: 001, loss: 0.571 \n",
            "epoch: 007, batch: 021, loss: 0.624 \n",
            "epoch: 007, batch: 041, loss: 0.487 \n",
            "epoch: 007, batch: 061, loss: 0.518 \n",
            "epoch: 007, batch: 081, loss: 0.497 \n",
            "epoch: 007, batch: 101, loss: 1.272 \n",
            "epoch: 007, batch: 121, loss: 0.422 \n",
            "epoch: 007, batch: 141, loss: 0.567 \n",
            "epoch: 007, batch: 161, loss: 0.257 \n",
            "epoch: 007, batch: 181, loss: 0.803 \n",
            "epoch: 007, batch: 201, loss: 0.264 \n",
            "epoch: 007, batch: 221, loss: 0.875 \n",
            "epoch: 007, batch: 241, loss: 0.533 \n",
            "epoch: 007, batch: 261, loss: 0.871 \n",
            "epoch: 007, batch: 281, loss: 1.068 \n",
            "epoch: 007, batch: 301, loss: 0.619 \n",
            "epoch: 007, batch: 321, loss: 0.464 \n",
            "epoch: 007, batch: 341, loss: 0.267 \n",
            "epoch: 007, batch: 361, loss: 0.773 \n",
            "epoch: 007, batch: 381, loss: 0.632 \n",
            "epoch: 007, batch: 401, loss: 0.753 \n",
            "epoch: 007, batch: 421, loss: 0.805 \n",
            "epoch: 007, batch: 441, loss: 0.957 \n",
            "epoch: 007, batch: 461, loss: 0.926 \n",
            "epoch: 007, batch: 481, loss: 0.707 \n",
            "epoch: 007, batch: 501, loss: 0.678 \n",
            "epoch: 007, batch: 521, loss: 0.498 \n",
            "epoch: 007, batch: 541, loss: 1.142 \n",
            "epoch: 007, batch: 561, loss: 0.776 \n",
            "epoch: 007, batch: 581, loss: 0.584 \n",
            "epoch: 007, batch: 601, loss: 0.583 \n",
            "epoch: 007, batch: 621, loss: 0.486 \n",
            "epoch: 007, batch: 641, loss: 0.741 \n",
            "epoch: 007, batch: 661, loss: 0.252 \n",
            "epoch: 007, batch: 681, loss: 0.330 \n",
            "epoch: 007, batch: 701, loss: 1.089 \n",
            "epoch: 007, batch: 721, loss: 0.989 \n",
            "epoch: 007, batch: 741, loss: 0.455 \n",
            "epoch: 007, batch: 761, loss: 0.848 \n",
            "epoch: 007, batch: 781, loss: 0.322 \n",
            "epoch: 007, batch: 801, loss: 0.697 \n",
            "epoch: 007, batch: 821, loss: 0.927 \n",
            "epoch: 007, batch: 841, loss: 0.355 \n",
            "epoch: 007, batch: 861, loss: 0.688 \n",
            "epoch: 007, batch: 881, loss: 0.482 \n",
            "epoch: 007, batch: 901, loss: 0.742 \n",
            "epoch: 007, batch: 921, loss: 0.425 \n",
            "epoch: 007, batch: 941, loss: 0.891 \n",
            "epoch: 007, batch: 961, loss: 0.768 \n",
            "epoch: 007, batch: 981, loss: 0.664 \n",
            "epoch: 007, batch: 1001, loss: 0.999 \n",
            "epoch: 007, batch: 1021, loss: 0.541 \n",
            "epoch: 007, batch: 1041, loss: 0.623 \n",
            "epoch: 007, batch: 1061, loss: 0.678 \n",
            "epoch: 007, batch: 1081, loss: 0.694 \n",
            "epoch: 007, batch: 1101, loss: 0.982 \n",
            "epoch: 007, batch: 1121, loss: 1.030 \n",
            "epoch: 007, batch: 1141, loss: 0.352 \n",
            "epoch: 007, batch: 1161, loss: 0.774 \n",
            "epoch: 007, batch: 1181, loss: 0.783 \n",
            "epoch: 007, batch: 1201, loss: 0.293 \n",
            "epoch: 007, batch: 1221, loss: 0.580 \n",
            "epoch: 007, batch: 1241, loss: 1.044 \n",
            "epoch: 007, batch: 1261, loss: 1.195 \n",
            "epoch: 007, batch: 1281, loss: 0.603 \n",
            "epoch: 007, batch: 1301, loss: 0.613 \n",
            "epoch: 007, batch: 1321, loss: 0.554 \n",
            "epoch: 007, batch: 1341, loss: 1.346 \n",
            "epoch: 007, batch: 1361, loss: 0.555 \n",
            "epoch: 007, batch: 1381, loss: 0.480 \n",
            "epoch: 007, batch: 1401, loss: 0.966 \n",
            "epoch: 007, batch: 1421, loss: 0.512 \n",
            "epoch: 007, batch: 1441, loss: 0.691 \n",
            "epoch: 007, batch: 1461, loss: 0.443 \n",
            "epoch: 007, batch: 1481, loss: 0.954 \n",
            "epoch: 007, batch: 1501, loss: 0.760 \n",
            "epoch: 007, batch: 1521, loss: 1.047 \n",
            "epoch: 007, batch: 1541, loss: 0.977 \n",
            "[validation] accuracy: 76.969%\n",
            "\n",
            "[validation] loss: 0.698\n",
            "\n",
            "epoch: 008, batch: 001, loss: 0.660 \n",
            "epoch: 008, batch: 021, loss: 0.801 \n",
            "epoch: 008, batch: 041, loss: 0.327 \n",
            "epoch: 008, batch: 061, loss: 0.868 \n",
            "epoch: 008, batch: 081, loss: 0.691 \n",
            "epoch: 008, batch: 101, loss: 1.245 \n",
            "epoch: 008, batch: 121, loss: 0.838 \n",
            "epoch: 008, batch: 141, loss: 0.871 \n",
            "epoch: 008, batch: 161, loss: 0.657 \n",
            "epoch: 008, batch: 181, loss: 0.503 \n",
            "epoch: 008, batch: 201, loss: 0.789 \n",
            "epoch: 008, batch: 221, loss: 0.707 \n",
            "epoch: 008, batch: 241, loss: 0.590 \n",
            "epoch: 008, batch: 261, loss: 0.811 \n",
            "epoch: 008, batch: 281, loss: 0.301 \n",
            "epoch: 008, batch: 301, loss: 0.474 \n",
            "epoch: 008, batch: 321, loss: 0.828 \n",
            "epoch: 008, batch: 341, loss: 0.538 \n",
            "epoch: 008, batch: 361, loss: 0.340 \n",
            "epoch: 008, batch: 381, loss: 0.802 \n",
            "epoch: 008, batch: 401, loss: 0.558 \n",
            "epoch: 008, batch: 421, loss: 0.477 \n",
            "epoch: 008, batch: 441, loss: 0.320 \n",
            "epoch: 008, batch: 461, loss: 0.835 \n",
            "epoch: 008, batch: 481, loss: 1.348 \n",
            "epoch: 008, batch: 501, loss: 0.510 \n",
            "epoch: 008, batch: 521, loss: 0.585 \n",
            "epoch: 008, batch: 541, loss: 0.316 \n",
            "epoch: 008, batch: 561, loss: 0.892 \n",
            "epoch: 008, batch: 581, loss: 1.213 \n",
            "epoch: 008, batch: 601, loss: 0.948 \n",
            "epoch: 008, batch: 621, loss: 1.088 \n",
            "epoch: 008, batch: 641, loss: 1.272 \n",
            "epoch: 008, batch: 661, loss: 0.401 \n",
            "epoch: 008, batch: 681, loss: 0.389 \n",
            "epoch: 008, batch: 701, loss: 0.568 \n",
            "epoch: 008, batch: 721, loss: 0.493 \n",
            "epoch: 008, batch: 741, loss: 0.241 \n",
            "epoch: 008, batch: 761, loss: 0.698 \n",
            "epoch: 008, batch: 781, loss: 0.712 \n",
            "epoch: 008, batch: 801, loss: 0.698 \n",
            "epoch: 008, batch: 821, loss: 0.648 \n",
            "epoch: 008, batch: 841, loss: 0.690 \n",
            "epoch: 008, batch: 861, loss: 0.698 \n",
            "epoch: 008, batch: 881, loss: 0.304 \n",
            "epoch: 008, batch: 901, loss: 0.943 \n",
            "epoch: 008, batch: 921, loss: 0.532 \n",
            "epoch: 008, batch: 941, loss: 0.536 \n",
            "epoch: 008, batch: 961, loss: 0.757 \n",
            "epoch: 008, batch: 981, loss: 1.039 \n",
            "epoch: 008, batch: 1001, loss: 0.610 \n",
            "epoch: 008, batch: 1021, loss: 0.827 \n",
            "epoch: 008, batch: 1041, loss: 0.404 \n",
            "epoch: 008, batch: 1061, loss: 0.662 \n",
            "epoch: 008, batch: 1081, loss: 1.266 \n",
            "epoch: 008, batch: 1101, loss: 0.543 \n",
            "epoch: 008, batch: 1121, loss: 0.935 \n",
            "epoch: 008, batch: 1141, loss: 1.003 \n",
            "epoch: 008, batch: 1161, loss: 0.978 \n",
            "epoch: 008, batch: 1181, loss: 0.449 \n",
            "epoch: 008, batch: 1201, loss: 0.565 \n",
            "epoch: 008, batch: 1221, loss: 0.538 \n",
            "epoch: 008, batch: 1241, loss: 0.948 \n",
            "epoch: 008, batch: 1261, loss: 0.474 \n",
            "epoch: 008, batch: 1281, loss: 0.922 \n",
            "epoch: 008, batch: 1301, loss: 0.914 \n",
            "epoch: 008, batch: 1321, loss: 1.005 \n",
            "epoch: 008, batch: 1341, loss: 0.773 \n",
            "epoch: 008, batch: 1361, loss: 0.401 \n",
            "epoch: 008, batch: 1381, loss: 1.532 \n",
            "epoch: 008, batch: 1401, loss: 0.789 \n",
            "epoch: 008, batch: 1421, loss: 0.920 \n",
            "epoch: 008, batch: 1441, loss: 0.553 \n",
            "epoch: 008, batch: 1461, loss: 0.678 \n",
            "epoch: 008, batch: 1481, loss: 0.505 \n",
            "epoch: 008, batch: 1501, loss: 0.753 \n",
            "epoch: 008, batch: 1521, loss: 0.349 \n",
            "epoch: 008, batch: 1541, loss: 0.579 \n",
            "[validation] accuracy: 77.504%\n",
            "\n",
            "[validation] loss: 0.691\n",
            "\n",
            "epoch: 009, batch: 001, loss: 0.909 \n",
            "epoch: 009, batch: 021, loss: 0.642 \n",
            "epoch: 009, batch: 041, loss: 0.913 \n",
            "epoch: 009, batch: 061, loss: 0.774 \n",
            "epoch: 009, batch: 081, loss: 0.274 \n",
            "epoch: 009, batch: 101, loss: 0.492 \n",
            "epoch: 009, batch: 121, loss: 0.390 \n",
            "epoch: 009, batch: 141, loss: 0.284 \n",
            "epoch: 009, batch: 161, loss: 0.541 \n",
            "epoch: 009, batch: 181, loss: 0.320 \n",
            "epoch: 009, batch: 201, loss: 0.757 \n",
            "epoch: 009, batch: 221, loss: 0.947 \n",
            "epoch: 009, batch: 241, loss: 1.096 \n",
            "epoch: 009, batch: 261, loss: 0.578 \n",
            "epoch: 009, batch: 281, loss: 0.370 \n",
            "epoch: 009, batch: 301, loss: 0.945 \n",
            "epoch: 009, batch: 321, loss: 0.475 \n",
            "epoch: 009, batch: 341, loss: 0.569 \n",
            "epoch: 009, batch: 361, loss: 0.220 \n",
            "epoch: 009, batch: 381, loss: 0.388 \n",
            "epoch: 009, batch: 401, loss: 0.344 \n",
            "epoch: 009, batch: 421, loss: 0.738 \n",
            "epoch: 009, batch: 441, loss: 0.411 \n",
            "epoch: 009, batch: 461, loss: 0.449 \n",
            "epoch: 009, batch: 481, loss: 0.702 \n",
            "epoch: 009, batch: 501, loss: 0.447 \n",
            "epoch: 009, batch: 521, loss: 0.416 \n",
            "epoch: 009, batch: 541, loss: 0.435 \n",
            "epoch: 009, batch: 561, loss: 0.467 \n",
            "epoch: 009, batch: 581, loss: 0.948 \n",
            "epoch: 009, batch: 601, loss: 0.789 \n",
            "epoch: 009, batch: 621, loss: 0.866 \n",
            "epoch: 009, batch: 641, loss: 0.733 \n",
            "epoch: 009, batch: 661, loss: 0.988 \n",
            "epoch: 009, batch: 681, loss: 0.390 \n",
            "epoch: 009, batch: 701, loss: 0.793 \n",
            "epoch: 009, batch: 721, loss: 1.202 \n",
            "epoch: 009, batch: 741, loss: 0.904 \n",
            "epoch: 009, batch: 761, loss: 0.735 \n",
            "epoch: 009, batch: 781, loss: 0.799 \n",
            "epoch: 009, batch: 801, loss: 0.720 \n",
            "epoch: 009, batch: 821, loss: 0.599 \n",
            "epoch: 009, batch: 841, loss: 1.342 \n",
            "epoch: 009, batch: 861, loss: 0.824 \n",
            "epoch: 009, batch: 881, loss: 0.662 \n",
            "epoch: 009, batch: 901, loss: 1.357 \n",
            "epoch: 009, batch: 921, loss: 0.710 \n",
            "epoch: 009, batch: 941, loss: 1.177 \n",
            "epoch: 009, batch: 961, loss: 0.697 \n",
            "epoch: 009, batch: 981, loss: 0.253 \n",
            "epoch: 009, batch: 1001, loss: 0.808 \n",
            "epoch: 009, batch: 1021, loss: 0.685 \n",
            "epoch: 009, batch: 1041, loss: 0.579 \n",
            "epoch: 009, batch: 1061, loss: 1.005 \n",
            "epoch: 009, batch: 1081, loss: 1.322 \n",
            "epoch: 009, batch: 1101, loss: 0.598 \n",
            "epoch: 009, batch: 1121, loss: 0.702 \n",
            "epoch: 009, batch: 1141, loss: 0.342 \n",
            "epoch: 009, batch: 1161, loss: 0.251 \n",
            "epoch: 009, batch: 1181, loss: 0.855 \n",
            "epoch: 009, batch: 1201, loss: 0.587 \n",
            "epoch: 009, batch: 1221, loss: 0.761 \n",
            "epoch: 009, batch: 1241, loss: 0.368 \n",
            "epoch: 009, batch: 1261, loss: 0.765 \n",
            "epoch: 009, batch: 1281, loss: 0.641 \n",
            "epoch: 009, batch: 1301, loss: 0.542 \n",
            "epoch: 009, batch: 1321, loss: 0.849 \n",
            "epoch: 009, batch: 1341, loss: 0.630 \n",
            "epoch: 009, batch: 1361, loss: 0.348 \n",
            "epoch: 009, batch: 1381, loss: 0.372 \n",
            "epoch: 009, batch: 1401, loss: 0.783 \n",
            "epoch: 009, batch: 1421, loss: 0.404 \n",
            "epoch: 009, batch: 1441, loss: 1.021 \n",
            "epoch: 009, batch: 1461, loss: 1.036 \n",
            "epoch: 009, batch: 1481, loss: 0.255 \n",
            "epoch: 009, batch: 1501, loss: 0.463 \n",
            "epoch: 009, batch: 1521, loss: 0.862 \n",
            "epoch: 009, batch: 1541, loss: 0.412 \n",
            "[validation] accuracy: 77.423%\n",
            "\n",
            "[validation] loss: 0.695\n",
            "\n",
            "epoch: 010, batch: 001, loss: 0.341 \n",
            "epoch: 010, batch: 021, loss: 0.491 \n",
            "epoch: 010, batch: 041, loss: 0.279 \n",
            "epoch: 010, batch: 061, loss: 0.596 \n",
            "epoch: 010, batch: 081, loss: 0.915 \n",
            "epoch: 010, batch: 101, loss: 0.533 \n",
            "epoch: 010, batch: 121, loss: 0.352 \n",
            "epoch: 010, batch: 141, loss: 0.623 \n",
            "epoch: 010, batch: 161, loss: 0.939 \n",
            "epoch: 010, batch: 181, loss: 1.126 \n",
            "epoch: 010, batch: 201, loss: 0.610 \n",
            "epoch: 010, batch: 221, loss: 0.629 \n",
            "epoch: 010, batch: 241, loss: 0.683 \n",
            "epoch: 010, batch: 261, loss: 0.419 \n",
            "epoch: 010, batch: 281, loss: 0.579 \n",
            "epoch: 010, batch: 301, loss: 0.325 \n",
            "epoch: 010, batch: 321, loss: 1.090 \n",
            "epoch: 010, batch: 341, loss: 0.833 \n",
            "epoch: 010, batch: 361, loss: 0.579 \n",
            "epoch: 010, batch: 381, loss: 0.757 \n",
            "epoch: 010, batch: 401, loss: 0.503 \n",
            "epoch: 010, batch: 421, loss: 1.022 \n",
            "epoch: 010, batch: 441, loss: 0.403 \n",
            "epoch: 010, batch: 461, loss: 0.925 \n",
            "epoch: 010, batch: 481, loss: 0.857 \n",
            "epoch: 010, batch: 501, loss: 0.185 \n",
            "epoch: 010, batch: 521, loss: 0.528 \n",
            "epoch: 010, batch: 541, loss: 0.288 \n",
            "epoch: 010, batch: 561, loss: 0.635 \n",
            "epoch: 010, batch: 581, loss: 0.927 \n",
            "epoch: 010, batch: 601, loss: 0.648 \n",
            "epoch: 010, batch: 621, loss: 0.816 \n",
            "epoch: 010, batch: 641, loss: 0.669 \n",
            "epoch: 010, batch: 661, loss: 0.881 \n",
            "epoch: 010, batch: 681, loss: 0.635 \n",
            "epoch: 010, batch: 701, loss: 0.608 \n",
            "epoch: 010, batch: 721, loss: 0.392 \n",
            "epoch: 010, batch: 741, loss: 0.501 \n",
            "epoch: 010, batch: 761, loss: 0.702 \n",
            "epoch: 010, batch: 781, loss: 0.588 \n",
            "epoch: 010, batch: 801, loss: 1.147 \n",
            "epoch: 010, batch: 821, loss: 0.711 \n",
            "epoch: 010, batch: 841, loss: 0.799 \n",
            "epoch: 010, batch: 861, loss: 0.535 \n",
            "epoch: 010, batch: 881, loss: 0.608 \n",
            "epoch: 010, batch: 901, loss: 0.513 \n",
            "epoch: 010, batch: 921, loss: 0.791 \n",
            "epoch: 010, batch: 941, loss: 0.248 \n",
            "epoch: 010, batch: 961, loss: 0.221 \n",
            "epoch: 010, batch: 981, loss: 0.319 \n",
            "epoch: 010, batch: 1001, loss: 0.327 \n",
            "epoch: 010, batch: 1021, loss: 0.538 \n",
            "epoch: 010, batch: 1041, loss: 0.966 \n",
            "epoch: 010, batch: 1061, loss: 0.352 \n",
            "epoch: 010, batch: 1081, loss: 0.703 \n",
            "epoch: 010, batch: 1101, loss: 0.674 \n",
            "epoch: 010, batch: 1121, loss: 0.507 \n",
            "epoch: 010, batch: 1141, loss: 0.667 \n",
            "epoch: 010, batch: 1161, loss: 0.540 \n",
            "epoch: 010, batch: 1181, loss: 0.409 \n",
            "epoch: 010, batch: 1201, loss: 0.647 \n",
            "epoch: 010, batch: 1221, loss: 0.577 \n",
            "epoch: 010, batch: 1241, loss: 0.224 \n",
            "epoch: 010, batch: 1261, loss: 0.855 \n",
            "epoch: 010, batch: 1281, loss: 0.653 \n",
            "epoch: 010, batch: 1301, loss: 0.642 \n",
            "epoch: 010, batch: 1321, loss: 0.453 \n",
            "epoch: 010, batch: 1341, loss: 0.852 \n",
            "epoch: 010, batch: 1361, loss: 0.370 \n",
            "epoch: 010, batch: 1381, loss: 0.271 \n",
            "epoch: 010, batch: 1401, loss: 0.527 \n",
            "epoch: 010, batch: 1421, loss: 0.669 \n",
            "epoch: 010, batch: 1441, loss: 0.776 \n",
            "epoch: 010, batch: 1461, loss: 0.396 \n",
            "epoch: 010, batch: 1481, loss: 0.221 \n",
            "epoch: 010, batch: 1501, loss: 0.437 \n",
            "epoch: 010, batch: 1521, loss: 0.343 \n",
            "epoch: 010, batch: 1541, loss: 0.493 \n",
            "[validation] accuracy: 78.493%\n",
            "\n",
            "[validation] loss: 0.673\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "### A QUENTIN PAS TOUCHE\n",
        "\n",
        "##########################################\n",
        "MODEL_NAME = \"ResNet18\"\n",
        "ETA = 1e-4 # Learning rate\n",
        "EPOCHS = 10 # Epochs / Number of iteration\n",
        "LOGS = 20 # Log Frequency\n",
        "BATCH_SIZE = 16 # Batch size\n",
        "##########################################\n",
        "\n",
        "###########################################################################################\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # Select the device\n",
        "###########################################################################################\n",
        "\n",
        "### RESNET 18\n",
        "\n",
        "resnet18 = models.resnet18()\n",
        "resnet18_pretrained = models.resnet18(weights=\"ResNet18_Weights.DEFAULT\")\n",
        "\n",
        "freeze_model(resnet18_pretrained)\n",
        "\n",
        "dataset_resnet18 = FashionDataset(df, transform=transform)\n",
        "\n",
        "train_size_resnet18 = int(0.8 * len(dataset_resnet18))\n",
        "test_size_resnet18 = len(dataset_resnet18) - train_size_resnet18\n",
        "train_dataset_resnet18, test_dataset_resnet18 = random_split(dataset_resnet18, [train_size_resnet18, test_size_resnet18])\n",
        "train_loader_resnet18 = DataLoader(train_dataset_resnet18, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader_resnet18 = DataLoader(test_dataset_resnet18, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "resnet18_pretrained.fc = torch.nn.Sequential(\n",
        "    torch.nn.Linear(512, 256),\n",
        "    torch.nn.Linear(256, 128),\n",
        "    torch.nn.Linear(128, 60)\n",
        ")\n",
        "\n",
        "# Train\n",
        "train_optim(resnet18_pretrained, train_loader_resnet18, test_loader_resnet18, EPOCHS, LOGS, device, MODEL_NAME, BATCH_SIZE, ETA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Model ResNet50**\n",
        "\n",
        "##### Architecture :\n",
        "\n",
        "![resnet-50-architecture](https://miro.medium.com/max/828/0*9LqUp7XyEx1QNc6A.png)\n",
        "\n",
        "*(source : https://blog.devgenius.io/resnet50-6b42934db431)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN&TEST on ResNet50 model | Number of epochs : 7 | Batch size : 16 | Log frequency : 200 | Device : cuda:0\n",
            "Epoch: 001 / 7 | Batch: 001 / 1542 | Loss: 4.139 | Time elapsed: 0.414 s \n",
            "Epoch: 001 / 7 | Batch: 201 / 1542 | Loss: 2.701 | Time elapsed: 16.834 s \n",
            "Epoch: 001 / 7 | Batch: 401 / 1542 | Loss: 1.266 | Time elapsed: 33.689 s \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_56384/2830708931.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m# Train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0mtrain_optim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresnet50_pretained\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader_resnet50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader_resnet50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLOGS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMODEL_NAME\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mETA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_56384/3706522348.py\u001b[0m in \u001b[0;36mtrain_optim\u001b[1;34m(model, _train_loader, _test_loader, epochs, log_frequency, device, modelName, batch_size, learning_rate)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m       \u001b[1;31m# At each epoch, the training set will be processed as a set of batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_train_loader\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    679\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_56384/2067647899.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# Transform the image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    347\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \"\"\"\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size)\u001b[0m\n\u001b[0;32m    280\u001b[0m                 \u001b[1;34m\"i.e. size should be an int or a sequence of length 1 in torchscript mode.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m             )\n\u001b[1;32m--> 282\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   1978\u001b[0m                 )\n\u001b[0;32m   1979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1980\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1981\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1982\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "### A TOM PAS TOUCHE\n",
        "\n",
        "##########################################\n",
        "MODEL_NAME = \"ResNet50\"\n",
        "ETA = 1e-4 # Learning rate\n",
        "EPOCHS = 7 # Epochs / Number of iteration\n",
        "LOGS = 200 # Log Frequency\n",
        "BATCH_SIZE = 16 # Batch size\n",
        "##########################################\n",
        "\n",
        "###########################################################################################\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # Select the device\n",
        "###########################################################################################\n",
        "\n",
        "### RESNET 50\n",
        "resnet50 = models.resnet50()\n",
        "resnet50_pretained = models.resnet50(weights=\"ResNet50_Weights.DEFAULT\")\n",
        "\n",
        "freeze_model(resnet50_pretained)\n",
        "\n",
        "dataset_resnet50 = FashionDataset(df, transform=transform)\n",
        "\n",
        "train_size_resnet50 = int(0.8 * len(dataset_resnet50))\n",
        "test_size_resnet50 = len(dataset_resnet50) - train_size_resnet50\n",
        "train_dataset_resnet50, test_dataset_resnet50 = random_split(dataset_resnet50, [train_size_resnet50, test_size_resnet50])\n",
        "train_loader_resnet50 = DataLoader(train_dataset_resnet50, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader_resnet50 = DataLoader(test_dataset_resnet50, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Change on resnet50 model to fit our dataset\n",
        "#resnet50.fc = torch.nn.Linear(2048, 60)\n",
        "\n",
        "resnet50_pretained.fc = torch.nn.Sequential(\n",
        "    torch.nn.Linear(2048, 1024),\n",
        "    torch.nn.Linear(1024, 512),\n",
        "    torch.nn.Linear(512, 256),\n",
        "    torch.nn.Linear(256, 128),\n",
        "    torch.nn.Linear(128, 60)\n",
        ")\n",
        "\n",
        "# Train\n",
        "train_optim(resnet50_pretained, train_loader_resnet50, test_loader_resnet50, EPOCHS, LOGS, device, MODEL_NAME, BATCH_SIZE, ETA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Model AlexNet**\n",
        "\n",
        "##### Architecture :\n",
        "\n",
        "![alexnet-architecture](https://raw.githubusercontent.com/blurred-machine/Data-Science/master/Deep%20Learning%20SOTA/img/alexnet2.png)\n",
        "\n",
        "*(source : https://www.kaggle.com/code/blurredmachine/alexnet-architecture-a-complete-guide/notebook)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 001, batch: 001, loss: 4.083 \n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "output with shape [1, 80, 60] doesn't match the broadcast shape [3, 80, 60]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8968/1782453623.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# Train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mtrain_optim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malexnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader_alexnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader_alexnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8968/2054289393.py\u001b[0m in \u001b[0;36mtrain_optim\u001b[1;34m(model, _train_loader, _test_loader, epochs, log_frequency, device, learning_rate)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m       \u001b[1;31m# At each epoch, the training set will be processed as a set of batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_train_loader\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    679\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8968/180657747.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# Transform the image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    267\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \"\"\"\n\u001b[1;32m--> 269\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"img should be Tensor Image. Got {type(tensor)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    957\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mstd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 959\u001b[1;33m     \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    960\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: output with shape [1, 80, 60] doesn't match the broadcast shape [3, 80, 60]"
          ]
        }
      ],
      "source": [
        "### A ALEXANDRE PAS TOUCHE\n",
        "\n",
        "### AlexNet\n",
        "alexnet = models.alexnet(weights=\"AlexNet_Weights.DEFAULT\")\n",
        "\n",
        "# Resize the images to 256x256\n",
        "transform_alexnet = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "dataset_alexnet = FashionDataset(df, transform=transform)\n",
        "\n",
        "train_size_alexnet = int(0.8 * len(dataset_alexnet))\n",
        "test_size_alexnet = len(dataset_alexnet) - train_size_alexnet\n",
        "train_dataset_alexnet, test_dataset_alexnet = random_split(dataset_alexnet, [train_size_alexnet, test_size_alexnet])\n",
        "train_loader_alexnet = DataLoader(train_dataset_alexnet, batch_size=16, shuffle=True)\n",
        "test_loader_alexnet = DataLoader(test_dataset_alexnet, batch_size=16, shuffle=True)\n",
        "\n",
        "# Change on alexnet model to fit our dataset\n",
        "alexnet.features = torch.nn.Sequential(*list(alexnet.features.children())[:-1])\n",
        "alexnet.classifier[6] = torch.nn.Linear(4096, 60)\n",
        "\n",
        "# Train\n",
        "train_optim(alexnet, train_loader_alexnet, test_loader_alexnet, 10, 100, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Model LeNet5**\n",
        "\n",
        "##### Architecture :\n",
        "\n",
        "![letnet-5-architecture](https://www.datasciencecentral.com/wp-content/uploads/2021/10/1lvvWF48t7cyRWqct13eU0w.jpeg)\n",
        "\n",
        "*(source : https://www.datasciencecentral.com/lenet-5-a-classic-cnn-architecture/)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [6, 1, 5, 5], expected input[64, 3, 80, 60] to have 1 channels, but got 3 channels instead",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8968/1247201746.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[0mmodel_lenet5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLeNet5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m \u001b[0mtrain_optim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_lenet5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader_lenet5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader_lenet5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8968/2054289393.py\u001b[0m in \u001b[0;36mtrain_optim\u001b[1;34m(model, _train_loader, _test_loader, epochs, log_frequency, device, learning_rate)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mimages\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# forward pass output=logits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8968/1247201746.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     28\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavgPool1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m#x = self.maxPool1(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    451\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 453\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    454\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [6, 1, 5, 5], expected input[64, 3, 80, 60] to have 1 channels, but got 3 channels instead"
          ]
        }
      ],
      "source": [
        "### A JUILA LA TERREUR PAS TOUCHE\n",
        "\n",
        "##A titre d'exemple\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "##########################################\n",
        "MODEL_NAME = \"LeNet5\"\n",
        "ETA = 1e-4 # Learning rate\n",
        "EPOCHS = 2 # Epochs / Number of iteration\n",
        "LOGS = 20 # Log Frequency\n",
        "BATCH_SIZE = 16 # Batch size\n",
        "##########################################\n",
        "\n",
        "###########################################################################################\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # Select the device\n",
        "###########################################################################################\n",
        "\n",
        "### LE NET 5\n",
        "class LeNet5(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, D_out):\n",
        "\n",
        "    super(LeNet5, self).__init__()\n",
        "    self.name = \"LeNet5\"\n",
        "\n",
        "    self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(224,224), stride=1, padding=2)\n",
        "    self.avgPool1 = torch.nn.AvgPool2d(kernel_size=(2,2), stride=2)\n",
        "    self.conv2 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(2,2), stride=1)\n",
        "    self.avgPool2 = torch.nn.AvgPool2d(kernel_size=(1,1), stride=2)\n",
        "    self.conv3 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(1,1), stride=1) \n",
        "    self.flatten = torch.nn.Flatten()\n",
        "\n",
        "    self.linear1 = torch.nn.Linear(64, 84)\n",
        "    self.linear2 = torch.nn.Linear(84, 42)\n",
        "    self.linear3 = torch.nn.Linear(42, 21 )\n",
        "    self.linear4 = torch.nn.Linear( 21, D_out) \n",
        "  \n",
        "  def forward(self, x):\n",
        "      \n",
        "    x = F.relu(self.conv1(x)) \n",
        "    x = self.avgPool1(x)\n",
        "    x = F.relu( self.conv2(x) )\n",
        "    x = self.avgPool2(x)\n",
        "    x = F.relu( self.conv3(x) )\n",
        "    x = self.flatten(x)\n",
        "    x = F.relu( self.linear1(x) )\n",
        "    x = F.relu( self.linear2(x) )\n",
        "    x = F.relu( self.linear3(x) )\n",
        "    x = self.linear4(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "transform_lenet5 = transforms.Compose(\n",
        "    [\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ]\n",
        ")\n",
        "\n",
        "dataset_lenet5 = FashionDataset(df, transform=transform_lenet5)\n",
        "\n",
        "train_size_lenet5 = int(0.8 * len(dataset_lenet5))\n",
        "test_size_lenet5 = len(dataset_lenet5) - train_size_lenet5\n",
        "train_dataset_lenet5, test_dataset_lenet5 = random_split(dataset_lenet5, [train_size_lenet5, test_size_lenet5])\n",
        "train_loader_lenet5 = DataLoader(train_dataset_lenet5, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader_lenet5 = DataLoader(test_dataset_lenet5, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "D_out = 60\n",
        "model_lenet5 = LeNet5(D_out)\n",
        "\n",
        "train_optim(model_lenet5, train_loader_lenet5, test_loader_lenet5, EPOCHS, LOGS, device,MODEL_NAME,BATCH_SIZE,ETA)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "2823d83d0138f4e88bebb1ac6893599dadbc35aa0bdabe1fb606adea8faab8b6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
