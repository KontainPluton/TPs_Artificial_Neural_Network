{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KontainPluton/TPs_Artificial_Neural_Network/blob/main/%5BTP_0%5D_My_first_Neural_Network_in_Numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUqc1_wsWsWi"
      },
      "source": [
        "# My First Artificial Neural Network\n",
        "\n",
        "**Objective** The goal of this practical session is to implement a simple but fully functional artificial neural network (ANN). It will probably be a first for you so be excited! This session will also be the opportunity to recall some important notions related to Machine Learning. \n",
        "\n",
        "**Prerequisites**: introductory level in machine learning, basics of calculus and linear algebra, a first quick introduction to ANNs, basic Python skills, and well, as usual, motivation!\n",
        "\n",
        "**Estimated time**: 3 hours (generous)\n",
        "\n",
        "****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4rVbB4mTpNa"
      },
      "source": [
        "\n",
        "\n",
        "## Introduction to the session\n",
        "\n",
        "You'll be asked to implement several *tricky* details by hand. \n",
        "It will (i) force you to deeply understand the basics of neural neworks, and (ii) help you to fully understand what's happening under the hood when you'll be using high-level frameworks such as TensorFlow or PyTorch. \n",
        "\n",
        "*Important*: please, don't give up if you find it hard at first, we can ensure you that you've the skills and adapted background to achieve this (easily). Take your time, find your pace, try to fully understand what we are doing at each step.\n",
        "\n",
        "As we will see later introducing PyTorch (a framework commonly used to implement ANN you're not supposed to know yet), the development process of our ANNs will be highly simplified. But for now, we will only use the Numpy library to implement the neural network.\n",
        "\n",
        "*NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.* https://numpy.org/. It is not a prerequesite to master Numpy to understand the content of this session. The duration of this session has been defined to allow you to spend extra time learning about tools or concepts that we will use, so do not hesite to find extra resources to learn about Numpy for instance. \n",
        "\n",
        "This tutorial is based on https://pytorch.org/tutorials/beginner/pytorch_with_examples.html. Several modifications have been made, and extra content added to match, as much as possible, with the content given in this course and your background. \n",
        "\n",
        "Recall the terminology:\n",
        "* supervised machine learning: the setting we will consider in which we do have a set of pairs of the form (input, label), e.g. in a binary classification task related to cancerous tumor detection in image we would have a set of pairs (image, boolean value).\n",
        "* training set: labeled inputs we will use to train our network. \n",
        "* batch: group of input values processed in a iteration. \n",
        "\n",
        "Our aim is to obtain a predictive model, here a neural network, able to perform good predictions while used in production. It is important to stress that we don't only want our model to be good on the training set (that would be memorizing); we want our model to provide good predictions on unseen input values too. Our model should therefore generalize well.  \n",
        "\n",
        "The flow of this session will be the following: \n",
        "1. We will define a neural network architecture in accordance with the input data we will have to deal with. Several design choices will be defined arbitrarily at this step.\n",
        "\n",
        "2. We will define the training procedure that will be used to train the parameters of the networks using training data. You should already be familiar with the training objective classically considered in a supervised setting (i.e. minimizing a loss function; if it's not the case do not hesitate to go back to our introductory course about linear models).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axdQtlZBZSd9"
      },
      "source": [
        "## Context : Supervised setting, **Regression**\n",
        "\n",
        "First we will define the context we will consider.\n",
        "\n",
        "We will place ourself in a regression setting in which we want a predictor $\\hat{f} :\\mathbb{R}^{3} \\rightarrow \\mathbb{R}^{3}$.\n",
        "\n",
        "For this illustration we will consider that we have the (x,y,z) coordinates of an object at time $t$ and that we want to predict its coordinates at time $t+1$, not knowing the dynamics of the underlying system (i.e. how the objects move in the corresponding 3D space). \n",
        "\n",
        "Our model should therefore be able to take inputs of the form $x_i \\in \\mathbb{R}^{3}$ and to produce outputs $\\hat{y}_i \\in \\mathbb{R}^{3}$ such that $\\hat{y}_i \\approx y_i$. Note that we distinguish the expected output (i.e. label) for $x_i$, denoted $y_i$, from the prediction of our network denoted $\\hat{y}_i$.\n",
        "\n",
        "For pedagological reasons, we will arbitrarily and explicitly define the function ($f :\\mathbb{R}^{3} \\rightarrow \\mathbb{R}^{3}$) we would like to approximate or find using our network. This function will be used to generate the datasets we will use. It is however important for you to understand that you'll never have access to that function in practice; you'll be only given the training set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAmC2iond5f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fec1f64-686b-4de5-d0cd-7ca5a5b02634"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "\n",
        "theta = 45\n",
        "\n",
        "# our labeling function\n",
        "def x_axis_theta_rotation(p):\n",
        "  \"\"\" return new (x,y,z) coordinates of the point after a theta-based rotation along the x-axis\n",
        "      Note: understanding this function is not important. \n",
        "      We could also add a random noise\n",
        "  \"\"\"\n",
        "  return np.array([\n",
        "    p[0],\n",
        "    p[1]*np.cos(p[0] * theta) - p[2]*np.sin(p[0] * theta),\n",
        "    p[1]*np.sin(p[0] * theta) + p[2]*np.cos(p[0] * theta)\n",
        "  ])\n",
        "\n",
        "# Generate a label example\n",
        "x_1 = np.random.randn(3)\n",
        "y_1 = x_axis_theta_rotation(x_1)\n",
        "\n",
        "print(\"Input data example\")\n",
        "print(\"x_1\", x_1)\n",
        "print(\"y_1\", y_1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data example\n",
            "x_1 [-0.18169342  0.00183919 -1.07256749]\n",
            "y_1 [-0.18169342 -1.01794913  0.33791121]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbWH835ZcjvE"
      },
      "source": [
        "We will have several observations of this form. \n",
        "Let's say we have *N* observations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZGuS3Gfc1c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "180e4661-307e-49d6-8224-f8be483dcc1b"
      },
      "source": [
        "def get_dataset(N, D_in, D_out):\n",
        "\n",
        "  \"\"\"Create N random input and associated label\"\"\"\n",
        "  x = np.random.randn(N, D_in)\n",
        "  y = np.array([x_axis_theta_rotation(xi) for xi in x]) \n",
        "\n",
        "  return (x,y)\n",
        "\n",
        "N = 1000 # size of the dataset\n",
        "D_in, D_out = 3, 3 # input and output dimensions, we name them to distinguish them hereafter\n",
        "\n",
        "x, y = get_dataset(N, D_in, D_out)\n",
        "\n",
        "print(x.shape)\n",
        "\n",
        "print(\"Data (first 10 elements)\")\n",
        "print(\"Inputs\\n\", x[:10,:])\n",
        "print(\"Labels\\n\", y[:10,:])\n",
        "\n",
        "print(\"\\nExample\")\n",
        "print(\"input 1: \", x[0])\n",
        "print(\"label 1: \", y[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 3)\n",
            "Data (first 10 elements)\n",
            "Inputs\n",
            " [[-1.10822882  0.06684909  0.73735374]\n",
            " [ 0.72364601  0.25826799 -0.66527554]\n",
            " [ 0.59830268 -0.943151   -0.37862946]\n",
            " [ 0.32039354 -1.30071696 -0.36546543]\n",
            " [-0.310481    0.21726022 -0.33060915]\n",
            " [-1.23305997 -0.71991447  2.05594996]\n",
            " [ 0.34478398 -0.08408758 -2.43803293]\n",
            " [ 1.3478922  -0.1177895  -0.46855784]\n",
            " [-1.33540887 -0.38552227 -1.53757129]\n",
            " [-1.73216597 -0.65330972  1.49689706]]\n",
            "Labels\n",
            " [[-1.10822882 -0.2221694   0.70625781]\n",
            " [ 0.72364601  0.71267067 -0.03734195]\n",
            " [ 0.59830268  0.57539667 -0.83774265]\n",
            " [ 0.32039354  0.71131628 -1.14867695]\n",
            " [-0.310481   -0.29029322 -0.26876432]\n",
            " [-1.23305997 -2.14582972  0.37499585]\n",
            " [ 0.34478398  0.54939986  2.37681196]\n",
            " [ 1.3478922  -0.31806561  0.36366892]\n",
            " [-1.33540887  0.95781737  1.26306721]\n",
            " [-1.73216597  1.37773974 -0.87712464]]\n",
            "\n",
            "Example\n",
            "input 1:  [-1.10822882  0.06684909  0.73735374]\n",
            "label 1:  [-1.10822882 -0.2221694   0.70625781]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VBSRKTPZ9xo"
      },
      "source": [
        "## ANN: Feedforward Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XhogCm7ZNYI"
      },
      "source": [
        "### The Model\n",
        "\n",
        "We define the architecture of our network, which can be based on several hyperparameters (number of neurons, layers, activation functions...).\n",
        "\n",
        "We will consider a very simple network composed of a single hidden layer with $H$ neurons using the Relu activation function.\n",
        "\n",
        "***\n",
        "\n",
        "#### Rectified Linear Unit (ReLU)\n",
        "\n",
        "ReLU is a popular [activation function](https://en.wikipedia.org/wiki/Activation_function) that can be used while defining an artificial neuron. \n",
        "\n",
        "$\\phi(z) = max(0,z)$\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Rectifier_and_softplus_functions.svg/500px-Rectifier_and_softplus_functions.svg.png\" alt=\"ReLU\" width=\"300\"/>\n",
        "\n",
        "***\n",
        "By convention, we consider inputs to be row vectors (cf. $x \\in \\mathbb{R}^{N \\times D_{in}}$). \n",
        "\n",
        "Input $i$: $x_i \\in \\mathbb{R}^{1 \\times 3}$ \n",
        "\n",
        "The predictive function $\\hat{f} :\\mathbb{R}^{3} \\rightarrow \\mathbb{R}^{3}$  (our neural network) will be:\n",
        "\n",
        "$\\hat{f}(x_i) = \\hat{y}_i = relu(x_i W_{1}) W_{2}$ \n",
        "\n",
        "with: \n",
        "* $W_{1} \\in \\mathbb{R}^{3 \\times H}$\n",
        "* $relu(z) = [max(0,z_0),max(0,z_1), ...]$  \n",
        "* $W_{2} \\in \\mathbb{R}^{H \\times 3}$\n",
        "\n",
        "**In this simple model we do not consider bias.** \n",
        "\n",
        "Note also that setting the number of neurons in this single-layer network, the predictions are deterministically determined by the weights $W_{1}$ and $W_{2}$. It means that the outputs of our model are fully determined by the weights of the network. Those are the sole parameters of our simple MLP (Multilayer Perceptron) model. \n",
        "\n",
        "Note also that we can use this network on a batch of inputs.\n",
        "with $x = [x_0, x_1, ..., x_N]^T \\in \\mathbb{R}^{N \\times 3}$.\n",
        "\n",
        "In the following we will therefore consider: \n",
        "\n",
        "$\\hat{f} :\\mathbb{R}^{N \\times 3} \\rightarrow \\mathbb{R}^{N \\times 3}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49J5jK5T32HG"
      },
      "source": [
        "### First implementation\n",
        "\n",
        "\n",
        "> **Exercice \"Forward pass\"**:\n",
        "> \n",
        "> The forward pass of a network is used to compute the outputs for a batch of inputs (see $\\hat{f}$ definition above).\n",
        "> \n",
        "> Considering thes simple MLP model we defined, we ask you to implement the forward pass using Numpy.\n",
        "> \n",
        "> Instructions:\n",
        "> * According to our task, both the input and the output must be in $\\mathbb{R}^{N \\times 3}$. You'll denote $D_{in}$ and $D_{out}$, respectively `D_in` and `D_out` the input and output dimensions. As we already said, we here have $D_{in}=D_{out}=3$, but we want our network to be generic so reuse existing `D_in` and `D_out` variables. \n",
        "> * Define a variable `H` to set the number of neurons; we want to be able to modify it.\n",
        "> * Initialize the weights randomly considering a standard normal distribution (mean=0, stdev=1). \n",
        "> * Decompose each step of the forward pass, i.e., do not use a fancy one-line expression. Only use Numpy functions to implement each step.\n",
        "> * Do not define a class now, just code it into a cell.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HDjDdAMjuIO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4283405-a5fb-455f-8637-0c6449531357"
      },
      "source": [
        "# YOUR TURN!\n",
        "\n",
        "def relu(z):\n",
        "  return np.maximum(0, z);\n",
        "\n",
        "N = 1000 # size of the dataset\n",
        "D_in, D_out = 3, 3 # input and output dimensions, we name them to distinguish them hereafter\n",
        "H = 10 # number of neurons\n",
        "\n",
        "x, y = get_dataset(N, D_in, D_out) # x <> R N*D_in (N lines = elements in dataset / D_in columns = entries)\n",
        "\n",
        "W1 = np.random.standard_normal((D_in,H)) # W <> R D_in*H (D_in lines = entries / H columns = neurons)\n",
        "z = np.dot(x,W1)\n",
        "z = relu(z)\n",
        "\n",
        "W2 = np.random.standard_normal((H,D_out)) # W <> R H*D_out (H lines = entries (1 for each neurons from the previous layer) / D_out columns = expected response)\n",
        "y_pred = np.dot(z,W2)\n",
        "# If you want to apply relu function on the result : \n",
        "#y_pred = relu(y_pred)\n",
        "\n",
        "print(y_pred.shape)\n",
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 3)\n",
            "[[-2.64599546 -1.70346041 -0.87333542]\n",
            " [ 0.41460598 -0.8166046   0.55247248]\n",
            " [ 2.59149368  1.86580382 -0.51907514]\n",
            " ...\n",
            " [-5.99220294 -5.39286498 -2.29919941]\n",
            " [ 2.01766761  0.77964877  0.18347276]\n",
            " [ 1.88142889 -4.58173685  2.83048545]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr0iFPdRIkwX"
      },
      "source": [
        "Obviously, the weights being randomly defined, do not expect to have good predictions at this stage. \n",
        "\n",
        "Our job will now be to define a clever procedure to train those parameters in order to obtain a good predictor. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibKMhFQuZlkR"
      },
      "source": [
        "## Training procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwMEDR9I3bog"
      },
      "source": [
        "\n",
        "\n",
        "### Generalities \n",
        "\n",
        "Training the network will aim at finding the $W_{1}$ and $W_{2}$ matrices in order to minimize the loss function, i.e. the function we will use to evaluate the error of our model.\n",
        "\n",
        "This error function (loss function) will be defined this way: \n",
        "\n",
        "$l(\\hat{y}, y) = \\sum_{i=0}^{N-1} \\sum_{j=0}^{D_{out}-1} (\\hat{y}_{i,j} - y_{i,j})^2$\n",
        "\n",
        "*Important*: note that loss function only makes sense in a regression setting. \n",
        "\n",
        "with $\\hat{y}, y \\in \\mathbb{R}^{N \\times D_{out}}$\n",
        "\n",
        "Processing the data, the weights will be iteratively modified in order to reduce the loss.\n",
        "\n",
        "To do so, at each iteration, we will apply two passes: \n",
        "1. Forward pass: compute the loss for the processed input (it can be a subset of the whole training set or even one input). We already know how to do that (see above).\n",
        "2. Backward pass: as we know the expected value for each input processed during the training phase, we can compute the loss for the current set of predictions. As we want to minimize that loss, we will:\n",
        "  *  Compute the partial derivative of the loss with regards to the parameters (here $W_{1}$ and $W_{2}$), also named [gradients](https://en.wikipedia.org/wiki/Gradient) and denoted in our case $\\nabla_{W_1}$ and $\\nabla_{W_2}$. Those gradients will denote the direction of greatest increase of the loss, and can therefore be used to change the parameters to influence the loss (following the opposite direction, we will be able to iteratively minimize the loss). To compute this gradient, we will simply apply the chain rule considering that all functions used in our network have a derivative (chain rule: https://en.wikipedia.org/wiki/Chain_rule) - please put aside consideration induced by the relu function for now, that's not important.\n",
        "  * Adjust the parameters ($W_{1}$ and $W_{2}$) applying an update rule based on the gradient, e.g. considering a constant pre-defined learning rate $\\eta$, this rule could be $W_1 = W_1 - \\eta \\times \\nabla_{W_1}$. This is called backpropagation. \n",
        "\n",
        "Computing the gradients is probably the most technical part, but it should not be a problem as it only requires to compute derivative.\n",
        "We provide a sketch to compute $\\nabla_{W_2}$ (using shortcut notations):\n",
        "\n",
        "\n",
        "\n",
        "$\\frac{\\partial l}{\\partial W_2} = \\frac{\\partial l}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial W_2}$\n",
        "\n",
        "$\\frac{\\partial l}{\\partial \\hat{y}} = 2 (\\hat{y} - y)$ $\\in \\mathbb{R}^{N \\times D_{out}}$ \n",
        "\n",
        "$\\frac{\\partial \\hat{y}}{\\partial W_2} : relu(x_i W_{1})$ $\\in \\mathbb{R}^{N \\times H}$ (shortcut)\n",
        "\n",
        "$\\frac{\\partial l}{\\partial W_2} =  relu(x_i W_{1})^T \\cdot (2 (\\hat{y} - y))$ $\\in \\mathbb{R}^{H \\times D_{out}}$, recall that $W_2 \\in \\mathbb{R}^{H \\times D_{out}}$ \n",
        "\n",
        "Details on how to compute the gradient on linear models (considering multivariate calculus) are provided in that resource: https://web.eecs.umich.edu/~justincj/teaching/eecs442/notes/linear-backprop.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKJ1GHh7ZxAz"
      },
      "source": [
        "### First implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-mcKH4xILx2"
      },
      "source": [
        "> **Exercise: Training procedure implementation**\n",
        ">\n",
        "> You can now try to implement the training procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fy1LQlt58yU"
      },
      "source": [
        "We define below our first implementation of the training procedure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXeIy6cRTf_U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "b8a9dd6b-de76-41a3-e381-6c78ed8f7aaa"
      },
      "source": [
        "# YOUR TURN!\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "N = 1000 # size of the dataset\n",
        "D_in, D_out = 3, 3 # input and output dimensions, we name them to distinguish them hereafter\n",
        "H = 10 # number of neurons\n",
        "J = 100 # number of iterations\n",
        "eta = 1e-5 # learning rate\n",
        "\n",
        "x, y = get_dataset(N, D_in, D_out) # x <> R N*D_in (N lines = elements in dataset / D_in columns = entries)\n",
        "\n",
        "W1 = np.random.standard_normal((D_in,H)) # W <> R D_in*H (D_in lines = entries / H columns = neurons)\n",
        "W2 = np.random.standard_normal((H,D_out)) # W <> R H*D_out (H lines = entries (1 for each neurons from the previous layer) / D_out columns = expected response)\n",
        "\n",
        "lossForEachIteration = []\n",
        "\n",
        "# ITERATIONS\n",
        "for j in range(0,J):\n",
        "\n",
        "  z = np.dot(x,W1)\n",
        "  z_relu = relu(z)\n",
        "  y_pred = np.dot(z_relu,W2)\n",
        "\n",
        "  # Compute square loss\n",
        "  squaredMatrix = np.square(y_pred - y) # Square up a matrix\n",
        "  loss = squaredMatrix.sum() # Sum of all the elements of the matrix\n",
        "\n",
        "  if j % 1000 == 0:\n",
        "    print(j, loss)\n",
        "\n",
        "  lossForEachIteration = np.append(lossForEachIteration, loss)\n",
        "\n",
        "  # Now we must modify the weights in order to reduce the loss\n",
        "  # We compute the partial derivative applying the chain rule for this\n",
        "  # Backpropagation to compute gradients of w1 and w2 with respect to loss\n",
        "  # This the most technical part \n",
        "  gradient_y_pred = 2 * (y_pred - y)\n",
        "  gradient_W2 = np.dot(z_relu.T,gradient_y_pred)\n",
        "  gradient_z_relu = np.dot(gradient_y_pred,W2.T)\n",
        "  gradient_z_relu[z < 0] = 0\n",
        "  gradient_W1 = np.dot(x.T,gradient_z_relu)\n",
        "\n",
        "  # Update weights using the gradient and the learning rate.\n",
        "  W1 -= eta * gradient_W1\n",
        "  W2 -= eta * gradient_W2\n",
        "\n",
        "plt.plot(np.arange(0,J),lossForEachIteration,color='green')\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('loss function')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 33268.76710554114\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lygCcZMfag7t"
      },
      "source": [
        "As we can see, it looks like our network is learning something. \n",
        "\n",
        "To be more precise, the training procedure we defined enables to iteratively minimize the loss (on the training set). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsKO0FaVYb2d"
      },
      "source": [
        "## Refactoring / Refinements\n",
        "\n",
        "Let's restructure the code in order to distinguish the different parts.\n",
        "Several approaches could have been chosen (and a lot of choices could be discussed and questioned in term of design, but that's not the aim of the course). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6JaASu8asly"
      },
      "source": [
        "### Model implementation\n",
        "\n",
        "\n",
        "> **Exercise: refactoring**\n",
        ">\n",
        "> Refactor the code. Create a class `SimpleANN` with a constructor enabling to define $D_{in}$, $D_{out}$, $H$.\n",
        ">\n",
        "> Define a `forward` method in which you'll implement the forward pass (the method returns the prediction)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My7a8bXgZsxZ"
      },
      "source": [
        "# YOUR TURN!\n",
        "\n",
        "class SimpleANN:\n",
        "  \"\"\"\n",
        "    Simple single-layer artificial neural network (ANN) \n",
        "  \"\"\"\n",
        "  def __init__(self,D_in: int, D_out: int, H: int):\n",
        "    \"\"\"\n",
        "      H: number of neurons in the hidden Layer\n",
        "    \"\"\"\n",
        "    self.w1 = np.random.standard_normal((D_in,H))\n",
        "    self.w2 = np.random.standard_normal((H,D_out))\n",
        "    \n",
        "  def forward(self, x):\n",
        "    z = x.dot(self.w1)\n",
        "    z_relu = relu(z)\n",
        "    y_pred = z_relu.dot(self.w2)\n",
        "\n",
        "    return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE22BB_sa1on"
      },
      "source": [
        "### Trainer implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcpN30xJa2DA"
      },
      "source": [
        "# YOUR TURN!\n",
        "\n",
        "class Trainer:\n",
        "  \"\"\"\n",
        "    Simple class implementing a Trainer that will be used\n",
        "    to modify our SimpleANN parameter to minimize the loss\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, ann: SimpleANN, training_set):\n",
        "    self.learning_rate = 1e-6 \n",
        "    self.ann = ann\n",
        "    self.x, self.y = training_set\n",
        "\n",
        "  def compute_loss(self, y_pred, y):\n",
        "    return np.square(y_pred - y).sum()\n",
        "    \n",
        "\n",
        "  def step(self):\n",
        "    \n",
        "    y_pred = self.ann.forward(self.x)\n",
        "    loss = self.compute_loss(y_pred, self.y)\n",
        "    \n",
        "    # backward step\n",
        "    # requires some information already computed in the forward step\n",
        "    z = self.x.dot(self.ann.w1)\n",
        "    z_relu = relu(z)\n",
        "\n",
        "    gradient_y_pred = 2.0 * (y_pred - y)\n",
        "    gradient_w2 = z_relu.T.dot(gradient_y_pred)\n",
        "    gradient_z_relu = gradient_y_pred.dot(self.ann.w2.T)\n",
        "    gradient_z_relu[z < 0] = 0\n",
        "    gradient_w1 = self.x.T.dot(gradient_z_relu)\n",
        "\n",
        "    # Update weights using the gradient and the learning rate.\n",
        "    self.ann.w1 -= self.learning_rate * gradient_w1\n",
        "    self.ann.w2 -= self.learning_rate * gradient_w2\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def fit(self, epoch: int, log_frequency: int):\n",
        "\n",
        "    log_loss = [] # we just log the loss\n",
        "\n",
        "    for i in range(epoch+1):\n",
        "      loss = self.step()\n",
        "      if i % log_frequency == 0:\n",
        "        print(i, loss)\n",
        "        log_loss.append((i,loss))\n",
        "    return log_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVNshkK9bIRN"
      },
      "source": [
        "### Training tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yx5dIj1dyah"
      },
      "source": [
        "We can now easily reimplement the training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhGnSij8eh6Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "outputId": "408d5e19-c76e-4396-9255-f9196cb7b7a1"
      },
      "source": [
        "# YOUR TURN!\n",
        "\n",
        "ann = SimpleANN(D_in=3, D_out=3, H=H)\n",
        "\n",
        "trainer = Trainer(ann, (x, y))\n",
        "log_loss = trainer.fit(epoch=20000, log_frequency=1000)\n",
        "\n",
        "# plotting\n",
        "import matplotlib.pyplot as plt\n",
        "del log_loss[0] # we remove the first loss (random weights)\n",
        "res = list(zip(*log_loss)) \n",
        "plt.plot(res[0], res[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 32533.56223436805\n",
            "1000 2466.951251280512\n",
            "2000 2253.9173754935587\n",
            "3000 2173.240456426336\n",
            "4000 2129.0019786058\n",
            "5000 2094.818551332386\n",
            "6000 2058.933953340004\n",
            "7000 2033.8097122412382\n",
            "8000 2011.5115826983576\n",
            "9000 1993.3791647032185\n",
            "10000 1976.9532765655454\n",
            "11000 1965.2296724797152\n",
            "12000 1957.7448418944953\n",
            "13000 1952.009751999581\n",
            "14000 1947.070129730576\n",
            "15000 1943.2394965315984\n",
            "16000 1940.1558937412633\n",
            "17000 1937.3499675518733\n",
            "18000 1934.3476602303076\n",
            "19000 1931.6252054041106\n",
            "20000 1929.410893219401\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f3b904062d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ztMH5bKhFD7"
      },
      "source": [
        "Let's see how our neural network performes on unseen data : the only things that matters for us.\n",
        "\n",
        "To do so we will generate another set of labelled data not used during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M44TaRIAhcFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76b9cd5e-9186-4da4-859e-8184bdf6c96c"
      },
      "source": [
        "# YOUR TURN!\n",
        "\n",
        "# recall the loss we finally obtained after the training phase \n",
        "y_pred = ann.forward(x)\n",
        "training_loss = trainer.compute_loss(y_pred, y)\n",
        "\n",
        "# Let's now generate a test set (data not used during training)\n",
        "x_test, y_test = get_dataset(N, D_in, D_out)\n",
        "\n",
        "y_pred_test = ann.forward(x_test)\n",
        "test_loss = trainer.compute_loss(y_pred_test, y_test)\n",
        "\n",
        "# due to the way the loss has been defined the two sets must have the same size\n",
        "# in order for us to compare the loss\n",
        "assert x.shape[0] == x_test.shape[0]\n",
        "\n",
        "print(\"training loss: \", training_loss)\n",
        "print(\"test loss    : \", test_loss)\n",
        "print(\"diff         : \", abs(training_loss - test_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss:  1929.4086458487739\n",
            "test loss    :  2104.621767102415\n",
            "diff         :  175.21312125364125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEiz3jeXbYbC"
      },
      "source": [
        "Well... even if results may vary from run to run, it looks like our neural network is pretty good on the training set but fails to perform similarly on the test set... we don't like this at all.\n",
        "We are maybe overfitting, i.e. memorizing the training set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQDBBcTIjwil"
      },
      "source": [
        "### Train, Validation, Test \n",
        "\n",
        "\n",
        "\n",
        "To monitor, and eventually overcome this, we will split our initial labelled dataset into 3 parts: \n",
        "* training set: the data we will use to update the weights of our network\n",
        "* validation set: the data we will use to monitor that we are not overfitting the training set during training\n",
        "* test set: the data we will use to evaluate our trained ANN on data not used during the training phase\n",
        "\n",
        "We will monitor our loss on the validation set during training and stop when the difference between our training and validation losses stresses overfitting.\n",
        "\n",
        "We will modify our trainer to accept a validation set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KARuLVwdnolx"
      },
      "source": [
        "# YOUR TURN!\n",
        "\n",
        "class TrainerAdvanced(Trainer):\n",
        "  \"\"\"\n",
        "    Simple Trainer with validation\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, ann: SimpleANN, training_set, validation_set):\n",
        "    Trainer.__init__(self, ann, training_set)\n",
        "    self.x_val, self.y_val = validation_set\n",
        "    \n",
        "  def fit(self, epoch: int, log_frequency: int):\n",
        "\n",
        "    log_loss = [] # we just log the losses\n",
        "\n",
        "    for i in range(epoch+1):\n",
        "      \n",
        "      loss_train = self.step()\n",
        "\n",
        "      if i % log_frequency == 0:\n",
        "\n",
        "        # validation part\n",
        "        y_pred_val = self.ann.forward(self.x_val)\n",
        "        loss_val = self.compute_loss(y_pred_val, self.y_val)\n",
        "\n",
        "        log_loss.append((i, loss_train, loss_val))\n",
        "        loss_train_evol = 0 if len(log_loss) == 1 else loss_train - log_loss[len(log_loss)-2][1]\n",
        "        loss_val_evol = 0 if len(log_loss) == 1 else loss_val - log_loss[len(log_loss)-2][2]\n",
        "        \n",
        "        print(\"{} train: {:.2f}   valid: {:.2f}   diff: {:.2f}   loss train ev.: {:.2f}   loss val ev.: {:.2f}\".format(i, loss_train, loss_val, loss_train - loss_val, loss_train_evol, loss_val_evol))\n",
        "    return log_loss "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmU7f4IZpDS3"
      },
      "source": [
        "Let's train our model using that new Trainer.\n",
        "\n",
        "Note that we already have a training set, and test set. \n",
        "We will only create a validation. \n",
        "Generally you'll have to split an initial dataset to obtain those sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZlHVXcFpIeC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ffb6dcac-6efb-4866-a2f2-dfefa9a52b9d"
      },
      "source": [
        "# YOUR TURN!\n",
        "\n",
        "# we use the network already trained above\n",
        "#ann = SimpleANN(D_in=3, D_out=3, H=H)\n",
        "\n",
        "x_val, y_val = get_dataset(N, D_in, D_out)\n",
        "\n",
        "assert x.shape[0] == x_val.shape[0]\n",
        "\n",
        "trainer = TrainerAdvanced(ann, (x, y), (x_val, y_val))\n",
        "log_loss = trainer.fit(epoch=50000, log_frequency=1000)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_losses(log_losses):\n",
        "  # del log_loss[0] # remove the first loss if you train a new network\n",
        "  res = list(zip(*log_loss)) \n",
        "  plt.plot(res[0], res[1])\n",
        "  plt.show()\n",
        "  plt.plot(res[0], res[2])\n",
        "\n",
        "plot_losses(log_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 train: 1929.41   valid: 2101.63   diff: -172.22   loss train ev.: 0.00   loss val ev.: 0.00\n",
            "1000 train: 1927.29   valid: 2100.31   diff: -173.02   loss train ev.: -2.12   loss val ev.: -1.32\n",
            "2000 train: 1925.49   valid: 2099.08   diff: -173.59   loss train ev.: -1.80   loss val ev.: -1.23\n",
            "3000 train: 1923.88   valid: 2098.08   diff: -174.20   loss train ev.: -1.61   loss val ev.: -1.00\n",
            "4000 train: 1922.43   valid: 2097.40   diff: -174.96   loss train ev.: -1.45   loss val ev.: -0.69\n",
            "5000 train: 1921.44   valid: 2096.77   diff: -175.33   loss train ev.: -1.00   loss val ev.: -0.63\n",
            "6000 train: 1920.58   valid: 2096.51   diff: -175.93   loss train ev.: -0.86   loss val ev.: -0.26\n",
            "7000 train: 1919.86   valid: 2096.43   diff: -176.57   loss train ev.: -0.72   loss val ev.: -0.08\n",
            "8000 train: 1919.30   valid: 2096.50   diff: -177.20   loss train ev.: -0.56   loss val ev.: 0.07\n",
            "9000 train: 1918.81   valid: 2096.60   diff: -177.80   loss train ev.: -0.49   loss val ev.: 0.10\n",
            "10000 train: 1918.39   valid: 2096.69   diff: -178.29   loss train ev.: -0.41   loss val ev.: 0.09\n",
            "11000 train: 1918.06   valid: 2096.71   diff: -178.65   loss train ev.: -0.33   loss val ev.: 0.02\n",
            "12000 train: 1917.74   valid: 2097.03   diff: -179.29   loss train ev.: -0.33   loss val ev.: 0.32\n",
            "13000 train: 1917.43   valid: 2097.38   diff: -179.96   loss train ev.: -0.31   loss val ev.: 0.35\n",
            "14000 train: 1917.18   valid: 2097.46   diff: -180.28   loss train ev.: -0.24   loss val ev.: 0.08\n",
            "15000 train: 1916.96   valid: 2097.58   diff: -180.61   loss train ev.: -0.22   loss val ev.: 0.12\n",
            "16000 train: 1916.74   valid: 2097.86   diff: -181.12   loss train ev.: -0.22   loss val ev.: 0.28\n",
            "17000 train: 1916.53   valid: 2098.08   diff: -181.55   loss train ev.: -0.21   loss val ev.: 0.22\n",
            "18000 train: 1916.23   valid: 2098.23   diff: -182.00   loss train ev.: -0.31   loss val ev.: 0.15\n",
            "19000 train: 1915.93   valid: 2098.31   diff: -182.39   loss train ev.: -0.30   loss val ev.: 0.08\n",
            "20000 train: 1915.68   valid: 2098.33   diff: -182.65   loss train ev.: -0.25   loss val ev.: 0.02\n",
            "21000 train: 1915.47   valid: 2098.59   diff: -183.12   loss train ev.: -0.21   loss val ev.: 0.26\n",
            "22000 train: 1915.30   valid: 2098.96   diff: -183.66   loss train ev.: -0.17   loss val ev.: 0.37\n",
            "23000 train: 1915.16   valid: 2099.11   diff: -183.95   loss train ev.: -0.14   loss val ev.: 0.15\n",
            "24000 train: 1915.00   valid: 2099.43   diff: -184.42   loss train ev.: -0.16   loss val ev.: 0.32\n",
            "25000 train: 1914.79   valid: 2100.22   diff: -185.42   loss train ev.: -0.21   loss val ev.: 0.79\n",
            "26000 train: 1914.61   valid: 2100.85   diff: -186.24   loss train ev.: -0.19   loss val ev.: 0.63\n",
            "27000 train: 1914.39   valid: 2101.62   diff: -187.24   loss train ev.: -0.22   loss val ev.: 0.78\n",
            "28000 train: 1914.25   valid: 2101.70   diff: -187.45   loss train ev.: -0.14   loss val ev.: 0.07\n",
            "29000 train: 1914.11   valid: 2101.80   diff: -187.68   loss train ev.: -0.13   loss val ev.: 0.10\n",
            "30000 train: 1913.96   valid: 2102.06   diff: -188.10   loss train ev.: -0.15   loss val ev.: 0.27\n",
            "31000 train: 1913.82   valid: 2102.22   diff: -188.40   loss train ev.: -0.14   loss val ev.: 0.16\n",
            "32000 train: 1913.68   valid: 2102.29   diff: -188.60   loss train ev.: -0.13   loss val ev.: 0.07\n",
            "33000 train: 1913.55   valid: 2102.32   diff: -188.77   loss train ev.: -0.13   loss val ev.: 0.04\n",
            "34000 train: 1913.42   valid: 2102.34   diff: -188.92   loss train ev.: -0.13   loss val ev.: 0.01\n",
            "35000 train: 1913.29   valid: 2102.34   diff: -189.06   loss train ev.: -0.13   loss val ev.: 0.01\n",
            "36000 train: 1913.16   valid: 2102.22   diff: -189.06   loss train ev.: -0.12   loss val ev.: -0.12\n",
            "37000 train: 1913.04   valid: 2102.02   diff: -188.98   loss train ev.: -0.12   loss val ev.: -0.20\n",
            "38000 train: 1912.93   valid: 2101.73   diff: -188.80   loss train ev.: -0.11   loss val ev.: -0.29\n",
            "39000 train: 1912.82   valid: 2101.48   diff: -188.67   loss train ev.: -0.11   loss val ev.: -0.25\n",
            "40000 train: 1912.71   valid: 2101.31   diff: -188.61   loss train ev.: -0.11   loss val ev.: -0.17\n",
            "41000 train: 1912.60   valid: 2101.17   diff: -188.57   loss train ev.: -0.11   loss val ev.: -0.14\n",
            "42000 train: 1912.49   valid: 2101.05   diff: -188.56   loss train ev.: -0.10   loss val ev.: -0.12\n",
            "43000 train: 1912.39   valid: 2100.91   diff: -188.52   loss train ev.: -0.10   loss val ev.: -0.15\n",
            "44000 train: 1912.29   valid: 2100.79   diff: -188.50   loss train ev.: -0.10   loss val ev.: -0.12\n",
            "45000 train: 1912.20   valid: 2100.69   diff: -188.49   loss train ev.: -0.10   loss val ev.: -0.10\n",
            "46000 train: 1912.10   valid: 2100.60   diff: -188.49   loss train ev.: -0.09   loss val ev.: -0.09\n",
            "47000 train: 1912.01   valid: 2100.52   diff: -188.51   loss train ev.: -0.09   loss val ev.: -0.08\n",
            "48000 train: 1911.92   valid: 2100.41   diff: -188.49   loss train ev.: -0.09   loss val ev.: -0.11\n",
            "49000 train: 1911.84   valid: 2100.32   diff: -188.48   loss train ev.: -0.09   loss val ev.: -0.09\n",
            "50000 train: 1911.75   valid: 2100.24   diff: -188.48   loss train ev.: -0.08   loss val ev.: -0.08\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqjSC9Ndqoot"
      },
      "source": [
        "It looks like we indeed obtain a better loss on the training set compared to the one obtained on the validation set. \n",
        "\n",
        "But we are still reducing the validation loss during training (which means we are not overfitting per se).\n",
        "\n",
        "Let's continue training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOr802U2q8Nl"
      },
      "source": [
        "log_loss = trainer.fit(epoch=100000, log_frequency=5000)\n",
        "plot_losses(log_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFxSjrtOJIxd"
      },
      "source": [
        "[Optional] Execute the code below to plot the full evolution of the losses training a new network from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-1Y8_vUJOhe"
      },
      "source": [
        "ann = SimpleANN(D_in=3, D_out=3, H=H)\n",
        "trainer = TrainerAdvanced(ann, (x, y), (x_val, y_val))\n",
        "log_loss = trainer.fit(epoch=100000, log_frequency=1000)\n",
        "\n",
        "del log_loss[0]\n",
        "plot_losses(log_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdUBr3fJboZL"
      },
      "source": [
        "## Takeaway\n",
        "\n",
        "You've implemented your first ANN. \n",
        "\n",
        "Ok it was a quite simple ANN, but you can still be proud of you!\n",
        "\n",
        "We will now try to extract the main notions you're supposed to grasp having endured that adventure. \n",
        "\n",
        "**A task-dependent ANN model with specific design choices**\n",
        "\n",
        "The architecture of our ANN has been defined based on the task we were adressing. Recall that in our case we were dealing with a specific regression task in which our predictior (ANN) was of the form: $\\hat{f}: \\mathbb{R}^3 \\rightarrow \\mathbb{R}^3$. Always keep in mind what's your goal: given the (x,y,z) 3D coordinates of an object at time $t$, we were asked to predict the location of the object in that same space at time $t+1$.\n",
        "\n",
        "Hence, even if we tried to be as generic as possible, e.g. introducing $D_{in}$ and $D_{out}$ to refer to the input, and output sizes, those values, among others, had to be defined to even think about effectively using the ANN, i.e. to make predictions. This was required to process a given input and produce an output because the shapes of $ W_1 $ and $ W_2 $ were also dependent of these values.\n",
        "\n",
        "In our case we defined $D_{in} = D_{out} = 3$ because of the task we were dealing with ($\\hat{f}: \\mathbb{R}^3 \\rightarrow \\mathbb{R}^3$). Dealing with another task, e.g. classification task, and/or the need to process other types of inputs, or to produce other outputs would require us to make potentially important modifications to our network. This has to be very clear to you.\n",
        "\n",
        "Designing our network, we also arbitrarily defined several things related to its architecture : we chose to only use a single hidden layer, with a specific number of neurons ($H$), that were characterized by a similar activation function (ReLU). The type of ANN we finally defined is called a [feedforward neural network](https://en.wikipedia.org/wiki/Feedforward_neural_network) (no cycles in the flow of information).\n",
        "\n",
        "All these design choices could have been different. We could have changed the number of neurons, the number of layers, the activation function, even the connexions between neurons. Once again, those choices had to be defined in order to obtain a usable network, as they define the way the output will be computed from an input. \n",
        "\n",
        "Expert in machine learning and deep learning knowing the litterature generally have an idea of what types of networks are more susceptible to work dealing with a specific task. They also know how to apply well-defined procedures to train various models and compare their results in a meaningfull way, which is always mandatory in the end to train and obtain a new network trained for a specific task on new data.\n",
        "\n",
        "We therefore distinguish: \n",
        "* Type of networks: feedforward, recurrent...\n",
        "* hyperparameters: number of layers, neurons, types of activation function...\n",
        "* parameters: weights (and bias) of the network that are effectively trainable (values stored in $W_1$ and $W_2$ in our case).  \n",
        "\n",
        "The architecture of the network and the hyperparameters define the way the function mapping the input to the output is constrained (it defines a set of functions). \n",
        "\n",
        "The parameters are implicitly defined by the design of the network, e.g. in our simple network, the hyperparameters $D_{in}$, $D_{out}$ and $H$ are defining the shapes of the matrices $W_1$ and $W_2$ storing the networks' parameters. \n",
        "\n",
        "Once fixed, i.e. assigned to a specific value, the parameters finally fully define the predictor, the function $\\hat{f}$ we will be able to use to map an input to an output.     \n",
        "\n",
        "**The general training procedure is not tight to a specific ANN or Task**\n",
        "\n",
        "The training procedure has been used to find the good values of the parameters with regard to a specific objective we defined. \n",
        "\n",
        "The objective we considered is the minimization of a loss function (it get more complex when we distinguish training, validation and test losses but don't get lost, what we wanted was to minimize a loss considering specific constraints). \n",
        "\n",
        "Recall, the training procedure we used. \n",
        "It worked iteratively as follows: \n",
        "\n",
        "* We iterate several times on the training data (eventually processing splits):\n",
        "  1. Forward pass: The predictor is used to makes predictions on the given inputs.\n",
        "  2. Backward pass: \n",
        "    * We compute the training loss comparing expected values and predictions\n",
        "    * We compute the gradient of the loss function with respect to the parameters of the ANN.\n",
        "    * We modify the parameters using the gradient.  \n",
        "  3. We evaluate a stopping criterion (e.g. based on the loss on a validation set)\n",
        "\n",
        "Obviously, this is a general sketch and each step contains technical aspects. However, the procedure is fully generic considering that: \n",
        "* the ANN is given \n",
        "* the loss is defined\n",
        "* we can compute the gradient of the loss function with respect to the parameters of the ANN (which was in the end the only technical part we had to deal with)\n",
        "\n",
        "\n",
        "Now, read carefully, let's imagine a world where, considering a loss and a set of labeled inputs, computing the gradient of the loss function with respect to the parameters of an ANN would be as easy as calling a function once the forward pass of the ANN is defined. Read again. \n",
        "\n",
        "AS we will see in the nex session, this world exists, it's yours! \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7vgU9gm18rG"
      },
      "source": [
        "> **Exercise \"Explain me what's an ANN.\"**\n",
        ">\n",
        "> Developing ANNs in the following sessions will require you to master the basic concepts. Take a sheet of paper. \n",
        ">  \n",
        "> Considering a supervised setting:\n",
        "> * Introduce what's an ANN; distinguish the main components, illustrate and formalize them.\n",
        "> * Explain how to train an ANN considering a specific dataset.  \n",
        "> * Express an illustration of simple network using Linear Algebra notations we used in the course (it will be important for you to understand this for the next sessions). \n",
        "> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IOKJZ6bwSVt"
      },
      "source": [
        "## Resources \n",
        "* [Machine learning](https://en.wikipedia.org/wiki/Machine_learning)\n",
        "* [Supervised learning](https://en.wikipedia.org/wiki/Supervised_learning)\n",
        "* [Feedforward neural networks](https://en.wikipedia.org/wiki/Feedforward_neural_network)\n",
        "* [Backpropagation](https://en.wikipedia.org/wiki/Backpropagation)\n",
        "* [Chain rule](https://en.wikipedia.org/wiki/Chain_rule)\n",
        "\n",
        "\n",
        "********"
      ]
    }
  ]
}